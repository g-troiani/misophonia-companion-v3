# Concatenated Project Code - Part 2 of 3
# Generated: 2025-05-12 16:05:28
# Root Directory: /Users/gianmariatroiani/Documents/misophonia-companion-v3
================================================================================

################################################################################
# File: scripts/templates/rag_web_interface_gian_v1 copy.py
################################################################################

# File: scripts/templates/rag_web_interface_gian_v1 copy.py

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Interactive RAG demo (Flask) that queries a Supabase vector store
(`research_chunks`) instead of Firebase / Firestore.

Changes in R2 ­­­→  ▸ removes the /stats endpoint + front‑end fetch  
                   ▸ cleaner citation helpers (no "Unknown" placeholders)
"""

from __future__ import annotations

import json
import logging
import os
import sys
from typing import Any, Dict, List, Sequence

from dotenv import load_dotenv
from flask import Flask, jsonify, render_template, request
from openai import OpenAI
from supabase import create_client


# ───────────────────────────── configuration ───────────────────────────── #

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUPABASE_URL   = os.getenv("SUPABASE_URL")
SUPABASE_KEY   = os.getenv("SUPABASE_SERVICE_ROLE_KEY")


# ------------------------------------------------------------------
# Bibliographic fields we want to pull from research_documents
DOC_FIELDS = (
    "title, authors, year, journal, doi"
)

# All columns we need from research_chunks  +  the join
CHUNK_SELECT = (
    "text, page_start, page_end, "
    f"research_documents({DOC_FIELDS})"
)
# ------------------------------------------------------------------

if not OPENAI_API_KEY:
    print("⛔  OPENAI_API_KEY missing — aborting")
    sys.exit(1)

client = OpenAI(api_key=OPENAI_API_KEY)

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s — %(levelname)s — %(message)s"
)
log = logging.getLogger(__name__)

app = Flask(__name__)


# ───────────────────────────── Supabase helpers ─────────────────────────── #

def init_supabase():
    if not SUPABASE_URL or not SUPABASE_KEY:
        log.error("SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY not set.")
        sys.exit(1)
    return create_client(SUPABASE_URL, SUPABASE_KEY)


sb = init_supabase()        # single client reused everywhere


# ──────────────────────── metadata / citation helpers ────────────────────── #

def _first(meta: Dict[str, Any], keys: Sequence[str]):
    """Return first non‑empty, non‑placeholder value in *meta* for any of *keys*."""
    for k in keys:
        v = meta.get(k)
        if v and str(v).strip() and not str(v).lower().startswith("unknown"):
            return v
    return None


def _first_author(meta: Dict[str, Any]) -> str | None:
    if (a := _first(meta, ["primary_author", "author", "creator"])):
        return str(a)
    if (lst := meta.get("authors")):
        if isinstance(lst, list) and lst:
            return str(lst[0])
        # if it's a string "Foo, Bar" take first token
        if isinstance(lst, str) and lst.strip() and not lst.lower().startswith("unknown"):
            return lst.split(",")[0].strip()
    return None


def _page(meta: Dict[str, Any]) -> str | None:
    if (p := _first(meta, ["page", "page_number"])):
        return str(p)
    if (rng := _first(meta, ["pages", "page_range"])):
        # Accept list or "12–15" / "12-15"
        if isinstance(rng, list) and rng:
            return str(rng[0])
        if isinstance(rng, str):
            return rng.split("–")[0].split("-")[0]
    return None


def make_citation(meta: Dict[str, Any]) -> str:
    """
    Compact citation for UI cards – omits any empty element.
    Format:  Author Year – Title. Journal. doi:… p.X
    """
    parts: list[str] = []

    if (a := _first_author(meta)):
        parts.append(a)

    if (y := _first(meta, ["year", "pub_year"])):
        parts.append(str(y))

    if (t := _first(meta, ["title", "document_title", "section"])):
        parts.append(f"– {t.strip()}")

    if (j := _first(meta, ["journal", "source"])):
        parts.append(j)

    if (doi := _first(meta, ["doi"])):
        parts.append(f"doi:{doi}")

    if (pg := _page(meta)):
        parts.append(f"p.{pg}")

    return " ".join(parts) if parts else "No citation metadata"


def long_citation(meta: Dict[str, Any]) -> str:
    """
    Longer citation fed to GPT (always includes Title if available).
    """
    auth  = _first_author(meta) or ""
    year  = _first(meta, ["year", "pub_year"]) or ""
    title = _first(meta, ["title", "document_title", "section"]) or ""
    jour  = _first(meta, ["journal", "source"]) or ""
    doi   = _first(meta, ["doi"]) or ""
    pg    = _page(meta) or ""

    bits = []
    if auth or year:
        bits.append(f"{auth} ({year}).".strip())
    if title:
        bits.append(title)
    if jour:
        bits.append(jour)
    if doi:
        bits.append(f"doi:{doi}")
    if pg:
        bits.append(f"p.{pg}")

    return " ".join(bits)


# keep alias for any legacy code
pretty_source = make_citation


# ────────────────────────────── embedding helpers ────────────────────────── #

def generate_embedding(text: str) -> List[float] | None:
    """Generate an Ada‑002 embedding for *text*."""
    try:
        resp = client.embeddings.create(model="text-embedding-ada-002", input=text)
        return resp.data[0].embedding
    except Exception as exc:                                  # pragma: no cover
        log.error("Embedding failed: %s", exc)
        return None


# ─────────────────────────────── search logic ────────────────────────────── #

def semantic_search(query: str, limit: int = 5) -> List[Dict[str, Any]]:
    """
    Server‑side vector search via RPC.
    """
    vec = generate_embedding(query)
    if vec is None:
        return []

    try:
        resp = (
            sb.rpc(
                "search_research_chunks",
                {
                    "query_embedding": vec,
                    "match_count": limit,
                    "similarity_threshold": 0.6,
                },
            ).execute()
        )
        rows = resp.data or []
        for r in rows:
            r["similarity"] = float(r.get("similarity", 0.0))
            r["match_type"] = f"Semantic ({r['similarity']:.4f})"
            r["chunk_id"]   = r.pop("id", "unknown")
            r["source"]     = make_citation(r.get("metadata", {}))
        return rows
    except Exception as exc:
        log.error("RPC search failed: %s", exc)
        return []


def keyword_search(query: str, limit: int = 5) -> List[Dict[str, Any]]:
    """
    Simple client‑side keyword fallback over a capped set of chunks.
    """
    try:
        resp = (
            sb.table("research_chunks")
            .select(CHUNK_SELECT)
            .limit(2_000)
            .execute()
        )
        all_chunks = resp.data or []
    except Exception as exc:
        log.error("Keyword search fetch failed: %s", exc)
        return []

    stop  = {"the","and","or","in","of","to","a","is","that","for","on","with"}
    terms = {t for t in query.lower().split() if t not in stop}
    if not terms:
        return []

    results: List[Dict[str, Any]] = []
    for ch in all_chunks:
        txt = str(ch.get("text", "")).lower()
        if not txt:
            continue

        matches = sum(1 for t in terms if t in txt)
        if matches == 0:
            continue

        score = matches / len(terms)
        if query.lower() in txt:
            score += 0.3
        if score < 0.3:
            continue

        meta = ch.get("metadata") or {}
        results.append(
            {
                "chunk_id": meta.get("chunk_id", "unknown"),
                "text": ch.get("text", ""),
                "metadata": meta,
                "similarity": float(score),
                "match_type": f"Keyword ({score:.4f})",
                "source": make_citation(meta),
            }
        )

    results.sort(key=lambda r: r["similarity"], reverse=True)
    return results[:limit]


def comprehensive_search(query: str, limit: int = 5) -> List[Dict[str, Any]]:
    sem = semantic_search(query, limit)
    kw  = keyword_search(query, limit)

    combined: List[Dict[str, Any]] = []
    seen = set()

    for r in sem + kw:
        cid = r.get("chunk_id")
        if cid not in seen:
            combined.append(r)
            seen.add(cid)

    combined.sort(key=lambda r: r["similarity"], reverse=True)
    return combined[:limit]


# ──────────────────────────── RAG generation ─────────────────────────────── #

def generate_rag_response(
    query: str,
    docs: List[Dict[str, Any]],
    max_tokens: int = 500,
) -> str:
    """Produce the final GPT‑4o answer with inline citations."""
    try:
        snippets = [
            f"Source {i} — {long_citation(d.get('metadata', {}))}\n{d.get('text','')}\n"
            for i, d in enumerate(docs, 1)
        ]

        prompt = f"""You are a helpful AI assistant specialised in misophonia research.

Answer the question below using *only* the provided research snippets.

Question: "{query}"

Snippets:
{''.join(snippets)}

Give a concise, structured answer with numbered citations like [1], [2] … .
If evidence is insufficient, say so explicitly."""
        comp = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "You answer misophonia questions strictly from the provided sources, citing them.",
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=max_tokens,
        )
        return comp.choices[0].message.content
    except Exception as exc:                                                  # pragma: no cover
        log.error("RAG generation failed: %s", exc)
        return f"Error generating response: {exc}"


# ───────────────────────────── Flask endpoints ───────────────────────────── #

@app.route("/")
def index():
    return render_template("index.html")


@app.route("/search", methods=["POST"])
def search():
    try:
        data = request.get_json(force=True, silent=True)
        if not data or not data.get("query"):
            return jsonify({"error": "No query provided"}), 400

        query = data["query"].strip()
        limit = int(data.get("limit", 5))

        results = comprehensive_search(query, limit)

        if not results:
            return jsonify(
                {"error": "No relevant documents found for your query", "results": []}
            ), 404

        # After you get results from the database:
        sources = []

        for r in results:
            # PostgREST nests the 1‑to‑1 join as an OBJECT, not a list
            doc = r.get("research_documents") or {}

            # authors is a Postgres text[] → Python list; stringify for HTML
            authors = ", ".join(doc["authors"]) if isinstance(doc.get("authors"), list) else doc.get("authors")

            # page range: prefer the 'pages' column; otherwise build it
            page_range = (
                doc.get("pages")
                or f"{r.get('page_start', '?')}-{r.get('page_end', '?')}"
            )

            sources.append(
                {
                    "chunk"      : r.get("text", ""),         # the snippet
                    "title"      : doc.get("title")   or "Unknown Title",
                    "section"    : "Unknown Section",    # add later if you store it
                    "authors"    : authors            or "Unknown Authors",
                    "journal"    : doc.get("journal") or "Unknown Journal",
                    "year"       : doc.get("year")    or "Unknown",
                    "volume"     : doc.get("volume"),
                    "issue"      : doc.get("issue"),
                    "doi"        : doc.get("doi"),
                    "page_range" : page_range,
                }
            )

        answer = generate_rag_response(query, results)
        return jsonify({"results": sources, "response": answer})
    except Exception as e:
        logging.exception("Search failed")
        return jsonify({"error": str(e)}), 500


@app.route("/stats")
def stats():
    try:
        # 1) total rows in the vector table ----------------------------
        total_chunks = (
            sb.table("research_chunks")
              .select("id", count="exact")
              .execute()
              .count
            or 0
        )

        return jsonify({"total_chunks": total_chunks}), 200

    except Exception as e:
        logging.exception("Stats endpoint failed")
        return jsonify({"error": str(e)}), 500


# ────────────────────────────── bootstrap html ───────────────────────────── #

if __name__ == "__main__":
    tpl_dir   = os.path.join(os.path.dirname(__file__), "templates")
    os.makedirs(tpl_dir, exist_ok=True)
    index_path = os.path.join(tpl_dir, "index.html")

    if not os.path.exists(index_path):
        with open(index_path, "w") as fp:
            fp.write(
"""<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Misophonia Research RAG Interface</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
  <style>
    body{padding:20px;background:#f8f9fa}
    .container{max-width:1000px;margin:0 auto}
    .header{text-align:center;margin-bottom:30px}
    .search-box{margin-bottom:20px}
    .results-container{margin-top:20px}
    .result-card{margin-bottom:15px;border-radius:8px;box-shadow:0 2px 5px rgba(0,0,0,.1)}
    .result-card .card-header{font-weight:bold;display:flex;justify-content:space-between}
    .loading{text-align:center;padding:20px;display:none}
    .response-container{margin-top:30px;padding:20px;background:#fff;border-radius:8px;box-shadow:0 2px 5px rgba(0,0,0,.1)}
    pre { background:#f8f9fa;border:0; }
  </style>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>Misophonia Research RAG Interface</h1>
      <p class="text-muted">Search across research documents with semantic and keyword retrieval</p>
    </div>

    <div class="search-box">
      <div class="input-group mb-3">
        <input type="text" id="search-input" class="form-control form-control-lg"
               placeholder="Ask a question about misophonia…" aria-label="Search query">
        <button class="btn btn-primary" type="button" id="search-button">Search</button>
      </div>
      <div class="form-text">Try questions about treatments, neurological basis, symptoms, or coping strategies</div>
    </div>

    <div class="loading" id="loading">
      <div class="spinner-border text-primary" role="status">
        <span class="visually-hidden">Loading…</span>
      </div>
      <p>Searching research documents and generating response…</p>
    </div>

    <div id="response-area" style="display:none">
      <div class="response-container">
        <h3>Research‑Based Answer</h3>
        <div id="response-content"></div>
      </div>

      <div class="results-container">
        <h3>Source Documents</h3>
        <div id="results-list"></div>
      </div>
    </div>
  </div>

<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
<script>
document.addEventListener("DOMContentLoaded",()=>{
  const $q      = document.getElementById("search-input");
  const $btn    = document.getElementById("search-button");
  const $load   = document.getElementById("loading");
  const $resp   = document.getElementById("response-area");
  const $respCt = document.getElementById("response-content");
  const $list   = document.getElementById("results-list");

  async function search(){
    const query=$q.value.trim();
    if(!query) return;

    $load.style.display="block";
    $resp.style.display="none";

    try{
      const res=await fetch("/search",{
        method:"POST",
        headers:{"Content-Type":"application/json"},
        body:JSON.stringify({query,limit:5})
      });
      if(!res.ok){
        let msg=`Server error (${res.status})`;
        try{msg=(await res.json()).error||msg;}catch{}
        throw new Error(msg);
      }
      const data=await res.json();
      const answerHtml=String(data.response||"").replace(/\\n/g,"<br>");
      $respCt.innerHTML=`<p>${answerHtml}</p>`;
      $list.innerHTML="";

      (data.results||[]).forEach((r,i)=>{
        const card=document.createElement("div");
        card.className="card result-card";
        const src = data.results[i].metadata;
        card.innerHTML=`
          <div class="card-header bg-light d-flex justify-content-between align-items-center"
               data-bs-toggle="collapse" data-bs-target="#chunk-${i}">
            <span>${r.source}</span>
            <span class="badge bg-primary opacity-75">${r.match_type}</span>
          </div>
          <div id="chunk-${i}" class="collapse show">
            <div class="card-body">
              <div class="d-flex justify-content-between align-items-baseline">
                <strong>
                  ${src.journal} (${src.year})
                </strong>
                ${src.doi ? `<a href="https://doi.org/${src.doi}" target="_blank" class="badge bg-primary">DOI</a>` : ''}
              </div>

              <h4 class="card-title">${src.title}</h4>
              <small class="text-muted">
                ${src.section}
              </small>

              <p class="mb-0">
                <em>${src.authors}</em><br>
                ${src.volume ? `${src.journal} ${src.volume}${src.issue ? `(${src.issue})` : ''}:` : ''}
                ${src.page_range}
              </p>
              
              <div class="mb-2 small text-muted">Similarity: ${(r.similarity??0).toFixed(4)}</div>
              <pre class="small text-muted mb-0" style="white-space:pre-wrap;">${r.text||""}</pre>
            </div>
          </div>`;
        $list.appendChild(card);
      });

    }catch(err){
      $respCt.innerHTML=`<div class="alert alert-warning">${err.message}</div>`;
      $list.innerHTML="";
    }finally{
      $load.style.display="none";
      $resp.style.display="block";
    }
  }

  $btn.addEventListener("click",search);
  $q.addEventListener("keypress",e=>{if(e.key==="Enter")search();});
});
</script>
</body>
</html>"""
            )

    log.info("Starting Flask on 0.0.0.0:8080 …")
    app.run(host="0.0.0.0", port=8080, debug=True)


================================================================================


################################################################################
# File: scripts/rag_web_app_v10.py
################################################################################

# File: scripts/rag_web_app_v10.py

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Mini Flask app that answers misophonia questions with Retrieval‑Augmented
Generation (gpt-4.1-mini-2025-04-14 + Supabase pgvector).

### Patch 2  (2025‑05‑06)
• **Embeddings** now created with **text‑embedding‑ada‑002** (1536‑D).  
• Similarity is re‑computed client‑side with a **plain cosine function** so the
  ranking no longer depends on pgvector's built‑in distance or any RPC
  threshold quirks.

The rest of the grounded‑answer logic (added in Patch 1) is unchanged.
"""
from __future__ import annotations

import logging
import math
import os
import re
from pathlib import Path        # (unused but left in to mirror original)
from typing import Dict, List
import json

from dotenv import load_dotenv
from flask import Flask, jsonify, request, make_response
from openai import OpenAI
from supabase import create_client
from flask_compress import Compress
from flask_cors import CORS

# ────────────────────────────── configuration ───────────────────────────── #

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUPABASE_URL   = os.getenv("SUPABASE_URL")
SUPABASE_KEY   = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
PORT           = int(os.getenv("PORT", 8080))

if not (OPENAI_API_KEY and SUPABASE_URL and SUPABASE_KEY):
    raise SystemExit(
        "❌  Required env vars: OPENAI_API_KEY, SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY"
    )

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s",
)
log = logging.getLogger("rag_app")

sb = create_client(SUPABASE_URL, SUPABASE_KEY)
oa = OpenAI(api_key=OPENAI_API_KEY)

app = Flask(__name__)
CORS(app)
app.config['COMPRESS_ALGORITHM'] = 'gzip'
Compress(app)

# ────────────────────────────── helper functions ────────────────────────── #


def embed(text: str) -> List[float]:
    """
    Return OpenAI embedding vector for *text* using text‑embedding‑ada‑002.

    ada‑002 has 1536 dimensions and is inexpensive yet solid for similarity.
    """
    resp = oa.embeddings.create(
        model="text-embedding-ada-002",
        input=text[:8192],  # safety slice
    )
    return resp.data[0].embedding


def cosine_similarity(a: List[float], b: List[float]) -> float:
    """Plain cosine similarity between two equal‑length vectors."""
    dot = sum(x * y for x, y in zip(a, b))
    na = math.sqrt(sum(x * x for x in a))
    nb = math.sqrt(sum(y * y for y in b))
    return dot / (na * nb + 1e-9)


# Add in-memory embedding cache
_qcache = {}
def embed_cached(text):
    if text in _qcache: return _qcache[text]
    vec = embed(text)
    _qcache[text] = vec
    return vec


# Add regex patterns for bibliography detection
_DOI_RE   = re.compile(r'\b10\.\d{4,9}/[-._;()/:A-Z0-9]+\b', re.I)
_YEAR_RE  = re.compile(r'\b(19|20)\d{2}\b')

def looks_like_refs(text: str) -> bool:
    """
    Return True if this chunk is likely just a bibliography list:
      • more than 12 DOIs, or
      • more than 15 year mentions.
    """
    doi_count  = len(_DOI_RE.findall(text))
    year_count = len(_YEAR_RE.findall(text))
    return doi_count > 12 or year_count > 15


def semantic_search(
    query: str,
    *,
    limit: int = 8,
    threshold: float = 0.0,
) -> List[Dict]:
    """
    Retrieve candidate chunks via the pgvector RPC, then re-rank with an
    **explicit cosine similarity** so the final score is always in
    **[-100 … +100] percent**.

    Why the extra work?
    -------------------
    •  The SQL function returns a raw inner-product that can be > 1.  
       (embeddings are *not* unit-length.)  
    •  By pulling the real 1 536-D vectors and re-computing a cosine we get a
       true, bounded similarity that front-end code can safely show.

    The -100 … +100 range is produced by:  
        pct = clamp(cosine × 100, -100, 100)
    """
    # 1. Embed the query once and keep it cached
    q_vec = embed_cached(query)

    # 2. Fast ANN search in Postgres (over-fetch 4× so we can re-rank)
    rows = (
        sb.rpc(
            "match_research_chunks",
            {
                "query_embedding": q_vec,
                "match_threshold": threshold,
                "match_count": limit * 4,
            },
        )
        .execute()
        .data
    ) or []

    # 3. Filter out bibliography-only chunks
    rows = [r for r in rows if not looks_like_refs(r["text"])]

    if not rows:
        return []

    # 4. Fetch document metadata (title, authors …) in one round-trip
    doc_ids = {r["document_id"] for r in rows}
    meta = {
        d["id"]: d
        for d in (
            sb.table("research_documents")
              .select("id,title,authors,year,journal,doi,abstract,keywords,research_topics,source_pdf")
              .in_("id", list(doc_ids))
              .execute()
              .data
            or []
        )
    }

    # 5. Pull embeddings once and compute **plain cosine** (no scaling)
    chunk_ids = [r["id"] for r in rows]

    emb_rows = (
        sb.table("research_chunks")
          .select("id, embedding")
          .in_("id", chunk_ids)
          .execute()
          .data
    ) or []

    emb_map: Dict[str, List[float]] = {}
    for e in emb_rows:
        raw = e["embedding"]
        if isinstance(raw, list):                    # list[Decimal]
            emb_map[e["id"]] = [float(x) for x in raw]
        elif isinstance(raw, str) and raw.startswith('['):   # TEXT  "[…]"
            emb_map[e["id"]] = [float(x) for x in raw.strip('[]').split(',')]

    for r in rows:
        vec = emb_map.get(r["id"])
        if vec:                                     # we now have the real vector
            cos = cosine_similarity(q_vec, vec)
            r["similarity"] = round(cos * 100, 1)   # –100…+100 % (or 0…100 %)
        else:                                       # fallback if something failed
            dist = float(r.get("similarity", 1.0))  # 0…2 cosine-distance
            r["similarity"] = round((1.0 - dist) * 100, 1)

        r["doc"] = meta.get(r["document_id"], {})

    # 6. Keep the top *limit* rows after proper re-ranking
    ranked = sorted(rows, key=lambda x: x["similarity"], reverse=True)[:limit]
    return ranked


# ──────────────────────── NEW RAG‑PROMPT HELPERS ───────────────────────── #

MAX_PROMPT_CHARS: int = 24_000  # ~6 k tokens @ 4 chars/token heuristic


def trim_chunks(chunks: List[Dict]) -> List[Dict]:
    """
    Fail‑safe guard: ensure concatenated chunk texts remain under the
    MAX_PROMPT_CHARS budget.  Keeps highest‑similarity chunks first.
    """
    sorted_chunks = sorted(chunks, key=lambda c: c.get("similarity", 0), reverse=True)
    output: List[Dict] = []
    total_chars = 0
    for c in sorted_chunks:
        chunk_len = len(c["text"])
        if total_chars + chunk_len > MAX_PROMPT_CHARS:
            break
        output.append(c)
        total_chars += chunk_len
    return output


def build_prompt(question: str, chunks: List[Dict]) -> str:
    """
    Build a structured prompt that asks GPT to:
      • answer in Markdown with short intro + numbered list of key points
      • cite inline like [1], [2] …
      • finish with a Bibliography that includes the *paper title*
    """
    snippet_lines, biblio_lines = [], []
    for i, c in enumerate(chunks, 1):
        snippet_lines.append(
            f"[{i}] \"{c['text'].strip()}\" "
            f"(pp. {c['page_start']}-{c['page_end']})"
        )

        d = c["doc"]
        title   = d.get("title", "Untitled")
        authors = ", ".join(d.get("authors") or ["Unknown"])
        journal = d.get("journal", "Unknown journal")
        year    = d.get("year", "n.d.")
        pages   = f"pp. {c['page_start']}-{c['page_end']}"
        doi_raw = d.get("doi")
        doi_md  = f"[doi:{doi_raw}](https://doi.org/{doi_raw})" if doi_raw else ""

        # Title now comes first ↓↓↓
        biblio_lines.append(
            f"[{i}] *{title}* · {authors} · {journal} ({year}) · {pages} {doi_md}"
        )

    prompt_parts = [
        "You are Misophonia Companion, a highly knowledgeable and empathetic AI assistant built to support clinicians, researchers, and individuals managing misophonia.",
        "You draw on evidence from peer-reviewed literature, clinical guidelines, and behavioral science.",
        "Your responses are clear, thoughtful, and grounded in the provided context.",
        "====",
        "QUESTION:",
        question,
        "====",
        "CONTEXT:",
        *snippet_lines,
        "====",
        "INSTRUCTIONS:",
        "• Write your answer in **Markdown**.",
        "• Begin with a concise summary (2–3 sentences).",
        "• Then elaborate on key points using well-structured paragraphs.",
        "• Provide relevant insights or suggestions (e.g., clinical, behavioral, emotional, or research-related).",
        "• If helpful, use lists, subheadings, or analogies to enhance understanding.",
        "• Use a professional and empathetic tone.",
        "• Cite sources inline like [1], [2] etc.",
        "• After the answer, include a 'BIBLIOGRAPHY:' section that lists each source exactly as provided below.",
        "• If none of the context answers the question, reply: \"I'm sorry, I don't have sufficient information to answer that.\"",
        "====",
        "BEGIN OUTPUT",
        "ANSWER:",
        "",  # where the model writes the main response
        "BIBLIOGRAPHY:",
        *biblio_lines,
    ]


    return '\n'.join(prompt_parts)


def extract_citations(answer: str) -> List[str]:
    """
    Parse numeric citations (e.g., "[1]", "[2]") from the answer text.
    Returns unique citation numbers in ascending order.
    """
    citations = re.findall(r"\[(\d+)\]", answer)
    return sorted(set(citations), key=int)


# ──────────────────────────────── routes ────────────────────────────────── #

@app.post("/search")
def search():
    payload = request.get_json(force=True, silent=True) or {}
    question = (payload.get("query") or "").strip()
    if not question:
        return jsonify({"error": "Missing 'query'"}), 400

    try:
        # Retrieve semantic matches (client‑side cosine re‑ranked)
        raw_matches = semantic_search(question, limit=int(payload.get("limit", 8)))

        if not raw_matches:
            return jsonify(
                {
                    "answer": "I'm sorry, I don't have sufficient information to answer that.",
                    "citations": [],
                    "results": [],
                }
            )

        # ──────────────────── TRIM CHUNKS TO BUDGET ──────────────────── #
        chunks = trim_chunks(raw_matches)

        # ──────────────────── BUILD PROMPT & CALL LLM ─────────────────── #
        prompt = build_prompt(question, chunks)

        completion = oa.chat.completions.create(
            model="gpt-4.1-mini-2025-04-14",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
        )
        answer_text: str = completion.choices[0].message.content.strip()

        # ──────────────────── EXTRACT CITATIONS ──────────────────────── #
        citations = extract_citations(answer_text)

        # Remove embedding vectors before sending back to the browser
        for m in raw_matches:
            m.pop("embedding", None)

        # ──────────────────── RETURN JSON ─────────────────────────────── #
        response = jsonify(
            {
                "answer": answer_text,
                "citations": citations,
                "results": raw_matches,
            }
        )
        response.headers['Connection'] = 'keep-alive'
        return response
    except Exception as exc:  # noqa: BLE001
        log.exception("search failed")
        return jsonify({"error": str(exc)}), 500


@app.get("/stats")
def stats():
    """Tiny ops endpoint—count total chunks."""
    resp = sb.table("research_chunks").select("id", count="exact").execute()
    return jsonify({"total_chunks": resp.count})


# ──────────────────────────────── main ─────────────────────────────────── #

if __name__ == "__main__":
    log.info("Starting Flask on 0.0.0.0:%s …", PORT)
    app.run(host="0.0.0.0", port=PORT, debug=True)


================================================================================


################################################################################
# File: scripts/templates/rag_web_interface_gian_v1.py
################################################################################

# File: scripts/templates/rag_web_interface_gian_v1.py

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Interactive RAG demo (Flask) that queries a Supabase vector store
(`research_chunks`) instead of Firebase / Firestore.

R5  →  • robust metadata parsing (string → JSON → dict)
        • never throws on missing / malformed metadata
        • identical external API; front-end needs **no** changes
"""

from __future__ import annotations

import json
import logging
import os
import sys
from typing import Any, Dict, List, Sequence

from dotenv import load_dotenv
from flask import Flask, jsonify, render_template, request
from openai import OpenAI
from supabase import create_client


# ───────────────────────────── configuration ───────────────────────────── #

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUPABASE_URL   = os.getenv("SUPABASE_URL")
SUPABASE_KEY   = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

DOC_FIELDS = (
    "title, authors, year, journal, doi, pages, volume, issue"
)

if not OPENAI_API_KEY:
    print("⛔  OPENAI_API_KEY missing — aborting")
    sys.exit(1)

client = OpenAI(api_key=OPENAI_API_KEY)

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s — %(levelname)s — %(message)s"
)
log = logging.getLogger(__name__)
app = Flask(__name__)


# ───────────────────────────── Supabase helpers ─────────────────────────── #

def init_supabase():
    if not SUPABASE_URL or not SUPABASE_KEY:
        log.error("SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY not set.")
        sys.exit(1)
    return create_client(SUPABASE_URL, SUPABASE_KEY)


sb = init_supabase()


# ──────────────────────── metadata / citation helpers ────────────────────── #

def _as_meta(obj: Any) -> Dict[str, Any]:
    """Best‑effort coercion to dict for the *metadata* column."""
    if isinstance(obj, dict):
        return obj
    if isinstance(obj, str):
        try:
            return json.loads(obj) or {}
        except Exception:
            return {}
    return {}


def _first(meta: Dict[str, Any], keys: Sequence[str]):
    for k in keys:
        v = meta.get(k)
        if v and str(v).strip() and not str(v).lower().startswith("unknown"):
            return v
    return None


def _first_author(meta: Dict[str, Any]) -> str | None:
    if (a := _first(meta, ["primary_author", "author", "creator"])):
        return str(a)
    if (lst := meta.get("authors")):
        if isinstance(lst, list) and lst:
            return str(lst[0])
        if isinstance(lst, str) and lst.strip() and not lst.lower().startswith("unknown"):
            return lst.split(",")[0].strip()
    return None


def _page(meta: Dict[str, Any]) -> str | None:
    if (p := _first(meta, ["page", "page_number"])):
        return str(p)
    if (rng := _first(meta, ["pages", "page_range"])):
        if isinstance(rng, list) and rng:
            return str(rng[0])
        if isinstance(rng, str):
            return rng.split("–")[0].split("-")[0]
    if meta.get("page_start") is not None:
        return str(meta["page_start"])
    return None


def make_citation(meta_raw: Any) -> str:
    meta = _as_meta(meta_raw)

    parts = []
    if (a := _first_author(meta)):
        parts.append(a)
    if (y := _first(meta, ["year", "pub_year"])):
        parts.append(str(y))
    if (t := _first(meta, ["title", "document_title", "section"])):
        parts.append(f"– {t.strip()}")
    if (j := _first(meta, ["journal", "source"])):
        parts.append(j)
    if (doi := _first(meta, ["doi"])):
        parts.append(f"doi:{doi}")
    if (pg := _page(meta)):
        parts.append(f"p.{pg}")

    return " ".join(parts) if parts else "No citation metadata"


def long_citation(meta_raw: Any) -> str:
    meta = _as_meta(meta_raw)

    auth  = _first_author(meta) or ""
    year  = _first(meta, ["year", "pub_year"]) or ""
    title = _first(meta, ["title", "document_title", "section"]) or ""
    jour  = _first(meta, ["journal", "source"]) or ""
    doi   = _first(meta, ["doi"]) or ""
    pg    = _page(meta) or ""

    bits = []
    if auth or year:
        bits.append(f"{auth} ({year}).".strip())
    if title:
        bits.append(title)
    if jour:
        bits.append(jour)
    if doi:
        bits.append(f"doi:{doi}")
    if pg:
        bits.append(f"p.{pg}")

    return " ".join(bits)


# ────────────────────────────── embedding helpers ────────────────────────── #

def generate_embedding(text: str) -> List[float] | None:
    try:
        resp = client.embeddings.create(model="text-embedding-ada-002", input=text)
        return resp.data[0].embedding
    except Exception as exc:
        log.error("Embedding failed: %s", exc)
        return None


# ─────────────────────────────── search logic ────────────────────────────── #

def semantic_search(query: str, limit: int = 5) -> List[Dict[str, Any]]:
    vec = generate_embedding(query)
    if vec is None:
        return []

    try:
        resp = (
            sb.rpc(
                "search_research_chunks",
                {
                    "query_embedding": vec,
                    "match_count": limit,
                    "similarity_threshold": 0.6,
                },
            ).execute()
        )
        rows = resp.data or []
        for r in rows:
            r["similarity"] = float(r.get("similarity", 0.0))
            r["match_type"] = f"Semantic ({r['similarity']:.4f})"
            r["chunk_id"]   = r.pop("id", "unknown")
            r["metadata"]   = _as_meta(r.get("metadata", {}))
            r["source"]     = make_citation(r["metadata"])
        return rows
    except Exception as exc:
        log.error("RPC search failed: %s", exc)
        return []


def comprehensive_search(query: str, limit: int = 5) -> List[Dict[str, Any]]:
    return semantic_search(query, limit)


# ──────────────────────────── RAG generation ─────────────────────────────── #

def generate_rag_response(
    query: str,
    docs: List[Dict[str, Any]],
    max_tokens: int = 500,
) -> str:
    try:
        snippets = [
            f"Source {i} — {long_citation(d.get('metadata', {}))}\n{d.get('text','')}\n"
            for i, d in enumerate(docs, 1)
        ]

        prompt = f"""You are a helpful AI assistant specialised in misophonia research.

Answer the question below using *only* the provided research snippets.

Question: "{query}"

Snippets:
{''.join(snippets)}

Give a concise, structured answer with numbered citations like [1], [2] … .
If evidence is insufficient, say so explicitly."""
        comp = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "You answer misophonia questions strictly from the provided sources, citing them.",
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=max_tokens,
        )
        return comp.choices[0].message.content
    except Exception as exc:
        log.error("RAG generation failed: %s", exc)
        return f"Error generating response: {exc}"


# ───────────────────────────── Flask endpoints ───────────────────────────── #

@app.route("/")
def index():
    return render_template("index.html")


@app.route("/search", methods=["POST"])
def search():
    try:
        data = request.get_json(force=True, silent=True)
        if not data or not data.get("query"):
            return jsonify({"error": "No query provided"}), 400

        query = data["query"].strip()
        limit = int(data.get("limit", 5))

        results = comprehensive_search(query, limit)

        if not results:
            return jsonify(
                {"error": "No relevant documents found for your query", "results": []}
            ), 404

        # 1.  Bulk‑fetch document metadata
        doc_map: Dict[str, Dict[str, Any]] = {}
        doc_ids = {r["document_id"] for r in results if r.get("document_id")}
        if doc_ids:
            q = (
                sb.table("research_documents")
                  .select(f"id,{DOC_FIELDS}")
                  .in_("id", list(doc_ids))
                  .execute()
            )
            for d in q.data or []:
                doc_map[d["id"]] = d

        # 2.  Build front‑end payload
        sources = []
        for r in results:
            doc = doc_map.get(r.get("document_id"), {})

            authors = (
                ", ".join(doc["authors"])
                if isinstance(doc.get("authors"), list)
                else doc.get("authors")
            )

            page_range = (
                doc.get("pages")
                or f"{r.get('page_start', '?')}-{r.get('page_end', '?')}"
            )

            sources.append(
                {
                    "chunk":       r.get("text", ""),
                    "source":      r.get("source"),
                    "match_type":  r.get("match_type"),
                    "similarity":  r.get("similarity"),
                    "chunk_id":    r.get("chunk_id"),
                    "metadata": {
                        "title":      doc.get("title")   or "Unknown Title",
                        "section":    "Unknown Section",
                        "authors":    authors            or "Unknown Authors",
                        "journal":    doc.get("journal") or "Unknown Journal",
                        "year":       doc.get("year")    or "Unknown",
                        "volume":     doc.get("volume"),
                        "issue":      doc.get("issue"),
                        "doi":        doc.get("doi"),
                        "page_range": page_range,
                    },
                }
            )

        answer = generate_rag_response(query, results) or ""
        return jsonify({"results": sources, "response": answer})
    except Exception as e:
        logging.exception("Search failed")
        return jsonify({"error": str(e)}), 500


@app.route("/stats")
def stats():
    try:
        total_chunks = (
            sb.table("research_chunks")
              .select("id", count="exact")
              .execute()
              .count
            or 0
        )
        return jsonify({"total_chunks": total_chunks}), 200
    except Exception as e:
        logging.exception("Stats endpoint failed")
        return jsonify({"error": str(e)}), 500


# ────────────────────────────── bootstrap html ───────────────────────────── #

if __name__ == "__main__":
    tpl_dir   = os.path.join(os.path.dirname(__file__), "templates")
    os.makedirs(tpl_dir, exist_ok=True)
    index_path = os.path.join(tpl_dir, "index.html")

    if not os.path.exists(index_path):
        with open(index_path, "w") as fp:
            fp.write("""<!DOCTYPE html>
<!-- identical template as previous version omitted for brevity -->""")

    log.info("Starting Flask on 0.0.0.0:8080 …")
    app.run(host="0.0.0.0", port=8080, debug=True)


================================================================================


################################################################################
# File: scripts/process_research_metadata.py
################################################################################

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Research Metadata Processor

This script:
1. Iterates over text files in documents/research/txt
2. For each file:
   - Takes first 3000 words from the text
   - Makes an OpenAI API call with a specific prompt
   - Updates the corresponding JSON file in documents/research/json
   - Records processed files to avoid reprocessing on subsequent runs

Required environment variables:
------------------------------
OPENAI_API_KEY

Usage:
-----
python process_research_metadata.py [--batch-size N] [--force] [--model MODEL]
"""

import argparse
import json
import logging
import os
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Set

from dotenv import load_dotenv
from openai import OpenAI
from tqdm import tqdm

# ─────────────────────────── Configuration ─────────────────────────────── #

load_dotenv()

OPENAI_API_KEY: Optional[str] = os.environ.get("OPENAI_API_KEY")
DEFAULT_MODEL = "gpt-4.1-mini-2025-04-14"

# Paths
SCRIPT_DIR = Path(__file__).resolve().parent
ROOT_DIR = SCRIPT_DIR.parent
TXT_DIR = ROOT_DIR / "documents" / "research" / "txt"
JSON_DIR = ROOT_DIR / "documents" / "research" / "json"
PROCESSED_FILE = ROOT_DIR / "scripts" / "processed_files.json"

# Initialize logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s",
)
log = logging.getLogger(__name__)

# ─────────────────────────────── Helpers ────────────────────────────────── #

def load_processed_files() -> Set[str]:
    """Load the set of already processed files."""
    if PROCESSED_FILE.exists():
        with open(PROCESSED_FILE, "r") as f:
            try:
                return set(json.load(f))
            except json.JSONDecodeError:
                log.warning("Error decoding processed files list, starting fresh")
    return set()

def save_processed_files(processed: Set[str]) -> None:
    """Save the set of processed files."""
    with open(PROCESSED_FILE, "w") as f:
        json.dump(list(processed), f, indent=2)

def extract_first_n_words(text: str, n: int = 3000) -> str:
    """Extract first n words from text."""
    words = text.split()
    return " ".join(words[:n])

def get_corresponding_json_path(txt_path: Path) -> Path:
    """Get the path to the corresponding JSON file."""
    return JSON_DIR / f"{txt_path.stem}.json"

def generate_metadata(client: OpenAI, text: str, model: str) -> Dict[str, Any]:
    """Call OpenAI API to generate metadata from text."""
    prompt = f"""
Extract the following metadata from this scientific paper and return exactly one JSON object with keys:
  • doc_type (e.g. "scientific paper")
  • title
  • authors (array of strings)
  • year (integer)
  • journal (string or null)
  • DOI (string or null)
  • abstract (string or null)
  • keywords (array of strings)
  • research_topics (array of strings)

If a field is not present, set it to null or an empty array. Here is the paper's full text:

{text}
"""

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are an assistant that extracts structured metadata from scientific papers."},
                {"role": "user", "content": prompt}
            ]
        )
        
        content = response.choices[0].message.content
        
        # Extract JSON from response (in case there's additional text)
        json_match = re.search(r'({[\s\S]*})', content)
        if json_match:
            content = json_match.group(1)
            
        return json.loads(content)
    except Exception as e:
        log.error(f"Error calling OpenAI API: {e}")
        raise

def update_json_file(json_path: Path, metadata: Dict[str, Any]) -> None:
    """Update JSON file with metadata from API response."""
    try:
        # Create if not exists
        if not json_path.exists():
            json_data = {
                "doc_type": "scientific paper",
                "title": "",
                "authors": [],
                "year": None,
                "journal": None,
                "doi": None,
                "abstract": None,
                "keywords": [],
                "research_topics": [],
                "created_at": datetime.utcnow().isoformat() + "Z",
                "source_pdf": "",
                "sections": []
            }
        else:
            # Read existing JSON file
            with open(json_path, "r", encoding="utf-8") as f:
                json_data = json.load(f)

        # Update with new metadata
        for key, value in metadata.items():
            if key in json_data and value not in (None, [], ""):
                json_data[key] = value

        # Write updated JSON file
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
            
        return True
    except Exception as e:
        log.error(f"Error updating JSON file {json_path}: {e}")
        return False

# ────────────────────────────── Main Process ────────────────────────────── #

def process_files(client: OpenAI, batch_size: int, force: bool, model: str) -> Dict[str, Any]:
    """Process text files and update JSON files with metadata."""
    results = {
        "processed": 0,
        "skipped": 0,
        "errors": 0,
        "file_results": []
    }
    
    # Get all text files
    txt_files = list(TXT_DIR.glob("*.txt"))
    log.info(f"Found {len(txt_files)} text files in {TXT_DIR}")
    
    # Load set of processed files
    processed_files = set() if force else load_processed_files()
    
    # Filter unprocessed files or limit to batch size
    if not force:
        txt_files = [f for f in txt_files if f.name not in processed_files]
    
    log.info(f"Found {len(txt_files)} unprocessed files")
    if batch_size > 0:
        txt_files = txt_files[:batch_size]
        log.info(f"Processing batch of {len(txt_files)} files")
    
    # Process each file
    for txt_path in tqdm(txt_files, desc="Processing files"):
        # Log the current file being processed
        log.info(f"Processing file: {txt_path.name}")
        
        file_result = {
            "file": txt_path.name,
            "success": False,
            "error": None
        }
        
        try:
            # Read text file
            with open(txt_path, "r", encoding="utf-8") as f:
                text = f.read()
            
            # Extract first 3000 words
            truncated_text = extract_first_n_words(text)
            
            # Generate metadata - log that we're calling the API
            log.info(f"Calling OpenAI API for: {txt_path.name}")
            metadata = generate_metadata(client, truncated_text, model)
            
            # Get corresponding JSON path
            json_path = get_corresponding_json_path(txt_path)
            
            # Update JSON file
            log.info(f"Updating JSON file for: {txt_path.name}")
            if update_json_file(json_path, metadata):
                file_result["success"] = True
                processed_files.add(txt_path.name)
                results["processed"] += 1
                log.info(f"Successfully processed: {txt_path.name}")
            else:
                file_result["error"] = "Failed to update JSON file"
                results["errors"] += 1
                log.error(f"Failed to update JSON for: {txt_path.name}")
                
        except Exception as e:
            file_result["error"] = str(e)
            results["errors"] += 1
            log.error(f"Error processing {txt_path.name}: {e}")
            
        results["file_results"].append(file_result)
    
    # Save processed files
    save_processed_files(processed_files)
    
    return results

def main() -> None:
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Process research files and extract metadata using OpenAI API")
    parser.add_argument("--batch-size", type=int, default=0, help="Number of files to process (0 = all)")
    parser.add_argument("--force", action="store_true", help="Process all files even if previously processed")
    parser.add_argument("--model", type=str, default=DEFAULT_MODEL, help="OpenAI model to use")
    args = parser.parse_args()
    
    # Check for OpenAI API key
    if not OPENAI_API_KEY:
        log.error("OPENAI_API_KEY not set. Please set it in your environment variables.")
        sys.exit(1)
    
    # Initialize OpenAI client
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    # Create directories if they don't exist
    TXT_DIR.mkdir(parents=True, exist_ok=True)
    JSON_DIR.mkdir(parents=True, exist_ok=True)
    
    # Process files
    log.info(f"Starting metadata extraction with model: {args.model}")
    results = process_files(client, args.batch_size, args.force, args.model)
    
    # Output results
    log.info(f"Processing complete: {results['processed']} processed, {results['skipped']} skipped, {results['errors']} errors")
    
    # Save report
    ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    report_path = ROOT_DIR / f"metadata_extraction_report_{ts}.json"
    with open(report_path, "w") as f:
        json.dump(results, f, indent=2)
    log.info(f"Report saved to {report_path}")

if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: scripts/templates/index.html
################################################################################

<!-- 
File: scripts/templates/index.html
 -->

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Misophonia Research RAG Interface</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
  <style>
    body{padding:20px;background:#f8f9fa}
    .container{max-width:1000px;margin:0 auto}
    .header{text-align:center;margin-bottom:30px}
    .search-box{margin-bottom:20px}
    .results-container{margin-top:20px}
    .result-card{margin-bottom:15px;border-radius:8px;box-shadow:0 2px 5px rgba(0,0,0,.1)}
    .result-card .card-header{font-weight:bold;display:flex;justify-content:space-between}
    .loading{text-align:center;padding:20px;display:none}
    .response-container{margin-top:30px;padding:20px;background:#fff;border-radius:8px;box-shadow:0 2px 5px rgba(0,0,0,.1)}
    pre { background:#f8f9fa;border:0; }
  </style>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>Misophonia Research RAG Interface</h1>
      <p class="text-muted">Search across research documents with semantic retrieval</p>
    </div>

    <div class="search-box">
      <div class="input-group mb-3">
        <input type="text" id="search-input" class="form-control form-control-lg"
               placeholder="Ask a question about misophonia…" aria-label="Search query">
        <button class="btn btn-primary" type="button" id="search-button">Search</button>
      </div>
      <div class="form-text">Try questions about treatments, neurological basis, symptoms, or coping strategies</div>
    </div>

    <div class="loading" id="loading">
      <div class="spinner-border text-primary" role="status">
        <span class="visually-hidden">Loading…</span>
      </div>
      <p>Searching research documents and generating response…</p>
    </div>

    <div id="response-area" style="display:none">
      <div class="response-container">
        <h3>Research‑Based Answer</h3>
        <div id="response-content"></div>
      </div>

      <div class="results-container">
        <h3>Source Documents</h3>
        <div id="results-list"></div>
      </div>
    </div>
  </div>

<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
<script>
document.addEventListener("DOMContentLoaded",()=>{
  const $q      = document.getElementById("search-input");
  const $btn    = document.getElementById("search-button");
  const $load   = document.getElementById("loading");
  const $resp   = document.getElementById("response-area");
  const $respCt = document.getElementById("response-content");
  const $list   = document.getElementById("results-list");

  async function doSearch(){
    const query=$q.value.trim();
    if(!query) return;

    $load.style.display="block";
    $resp.style.display="none";

    try{
      const res=await fetch("/search",{
        method:"POST",
        headers:{"Content-Type":"application/json"},
        body:JSON.stringify({query,limit:5})
      });
      if(!res.ok){
        let msg=`Server error (${res.status})`;
        try{msg=(await res.json()).error||msg;}catch{}
        throw new Error(msg);
      }
      const data=await res.json();

      $respCt.innerHTML=`<p>${String(data.response||"").replace(/\n/g,"<br>")}</p>`;
      $list.innerHTML="";

      (data.results||[]).forEach((r,i)=>{
        const m = r.metadata;
        const card=document.createElement("div");
        card.className="card result-card";
        card.innerHTML=`
          <div class="card-header bg-light d-flex justify-content-between align-items-center"
               data-bs-toggle="collapse" data-bs-target="#chunk-${i}">
            <span>${r.source}</span>
            <span class="badge bg-primary opacity-75">${r.match_type}</span>
          </div>
          <div id="chunk-${i}" class="collapse show">
            <div class="card-body">
              <div class="d-flex justify-content-between align-items-baseline">
                <strong>
                  ${m.journal} (${m.year})
                </strong>
                ${m.doi ? `<a href="https://doi.org/${m.doi}" target="_blank" class="badge bg-primary">DOI</a>` : ''}
              </div>

              <h4 class="card-title">${m.title}</h4>
              <small class="text-muted">${m.section}</small>

              <p class="mb-0">
                <em>${m.authors}</em><br>
                ${m.volume ? `${m.journal} ${m.volume}${m.issue ? \`(\${m.issue})\` : ''}:` : ''}
                ${m.page_range}
              </p>

              <div class="mb-2 small text-muted">Similarity: ${(r.similarity??0).toFixed(4)}</div>
              <pre class="small text-muted mb-0" style="white-space:pre-wrap;">${r.chunk||""}</pre>
            </div>
          </div>`;
        $list.appendChild(card);
      });

    }catch(err){
      $respCt.innerHTML=`<div class="alert alert-warning">${err.message}</div>`;
      $list.innerHTML="";
    }finally{
      $load.style.display="none";
      $resp.style.display="block";
    }
  }

  $btn.addEventListener("click",doSearch);
  $q.addEventListener("keypress",e=>{if(e.key==="Enter")doSearch();});
});
</script>
</body>
</html>


================================================================================


################################################################################
# File: netlify/functions/research.js
################################################################################

// File: netlify/functions/research.js

/**
 * Topic-aware “Research Assistant”
 * Supports OpenAI **or** Gemini depending on AI_PROVIDER env var.
 *
 * POST /.netlify/functions/research
 * Body: { messages:[…], topic?:string }
 *
 * ENV:
 *   AI_PROVIDER          openai | gemini      (default openai)
 *   OPENAI_API_KEY       required if provider=openai
 *   GEMINI_API_KEY       required if provider=gemini
 */

import 'dotenv/config';
import { OpenAI } from 'openai';

// ─────────────────────────────────────────────────────────────
const AI_PROVIDER = (process.env.AI_PROVIDER ?? 'openai').toLowerCase();
// ─────────────────────────────────────────────────────────────

/* -----------------------------------------------------------
 * Common helpers
 * --------------------------------------------------------- */
function badRequest(msg) {
  return {
    statusCode: 400,
    headers: { 'content-type': 'application/json' },
    body: JSON.stringify({ error: msg }),
  };
}

function serverError(msg, details) {
  if (details) console.error(details);
  return {
    statusCode: 500,
    headers: { 'content-type': 'application/json' },
    body: JSON.stringify({ error: msg }),
  };
}

/* ===========================================================
 *  OpenAI branch
 * ========================================================= */
async function openaiHandler(messages, topic) {
  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey) return serverError('OPENAI_API_KEY missing');

  const openai = new OpenAI({ apiKey });

  const systemPrompt = topic
    ? `You are a knowledgeable research assistant focusing on “${topic}”. Provide accurate, concise information about misophonia research.`
    : 'You are a knowledgeable research assistant on misophonia research.';

  const oaMessages = [
    { role: 'system', content: systemPrompt },
    ...messages,
  ];

  const completion = await openai.chat.completions.create({
    model: 'gpt-4o',
    messages: oaMessages,
    max_tokens: 1024,
    temperature: 0.7,
  });

  return {
    statusCode: 200,
    headers: { 'content-type': 'application/json' },
    body: JSON.stringify({
      reply: completion.choices?.[0]?.message?.content ?? '',
      structured: null,
      provider: 'openai',
    }),
  };
}

/* ===========================================================
 *  Gemini branch
 * ========================================================= */
async function geminiHandler(messages, topic) {
  const apiKey = process.env.GEMINI_API_KEY;
  if (!apiKey) return serverError('GEMINI_API_KEY missing');

  // Build user prompt
  let userPrompt = '';
  if (topic) userPrompt += `Topic: ${topic}\n`;
  userPrompt += messages
    .map(
      (m) => `${m.role === 'user' ? 'User' : 'Assistant'}: ${m.content}`
    )
    .join('\n');

  const resp = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?key=${apiKey}`,
    {
      method: 'POST',
      headers: { 'content-type': 'application/json' },
      body: JSON.stringify({
        contents: [{ role: 'user', parts: [{ text: userPrompt }] }],
        generationConfig: { temperature: 0.7, maxOutputTokens: 1024 },
        tools: [
          {
            function_declarations: [
              {
                name: 'structured_output',
                description:
                  'Return information in a structured JSON format for chat display, with sections, bullet points, and highlights.',
              },
            ],
          },
        ],
      }),
    }
  );

  if (!resp.ok) {
    const errText = await resp.text().catch(() => '');
    return serverError('Error from Gemini API', errText);
  }

  const data = await resp.json();

  let structured = null;
  try {
    const part = data.candidates?.[0]?.content?.parts?.[0];
    if (part?.functionCall?.name === 'structured_output') {
      structured = JSON.parse(part.functionCall.args.json || '{}');
    }
  } catch {
    // swallow JSON parse errors – keep structured=null
  }

  const reply =
    data.candidates?.[0]?.content?.parts?.[0]?.text ?? '';

  return {
    statusCode: 200,
    headers: { 'content-type': 'application/json' },
    body: JSON.stringify({ reply, structured, provider: 'gemini' }),
  };
}

/* ===========================================================
 *  Netlify Function entry-point
 * ========================================================= */
export async function handler(event /* , context */) {
  if (event.httpMethod !== 'POST') {
    return { statusCode: 405, body: 'Method Not Allowed' };
  }

  // ─── parse body ──────────────────────────────────────────
  let body;
  try {
    body = JSON.parse(event.body ?? '{}');
  } catch {
    return badRequest('Invalid JSON');
  }

  const { messages, topic } = body;
  if (!Array.isArray(messages) || messages.length === 0) {
    return badRequest('messages array required');
  }

  try {
    if (AI_PROVIDER === 'gemini') {
      return await geminiHandler(messages, topic);
    }
    // default → openai
    return await openaiHandler(messages, topic);
  } catch (err) {
    return serverError(
      `Unexpected ${AI_PROVIDER} handler failure`,
      err
    );
  }
}


================================================================================


################################################################################
# File: netlify/functions/rag_before_copying_python_logic.js
################################################################################

// File: netlify/functions/rag_before_copying_python_logic.js

// netlify/functions/rag.js
import 'dotenv/config';
import { createClient } from '@supabase/supabase-js';
import OpenAI from 'openai';

// ─── clients ──────────────────────────────────────────
const sb = createClient(
  process.env.SUPABASE_URL,
  process.env.SUPABASE_SERVICE_ROLE_KEY
);
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// ─── tiny cosine helper ───────────────────────────────
function cosine(a, b) {
  let dot = 0, na = 0, nb = 0;
  for (let i = 0; i < a.length; i++) {
    dot += a[i] * b[i];
    na  += a[i] * a[i];
    nb  += b[i] * b[i];
  }
  return dot / (Math.sqrt(na) * Math.sqrt(nb) + 1e-9);
}

// ─── Netlify Lambda entry-point ───────────────────────
export async function handler(event) {
  if (event.httpMethod !== 'POST') {
    return { statusCode: 405, body: 'Method Not Allowed' };
  }

  let body;
  try { body = JSON.parse(event.body ?? '{}'); }
  catch { return { statusCode: 400, body: 'Bad JSON' }; }

  const query = (body.query || '').trim();
  const limit = body.limit || 8;
  if (!query) {
    return { statusCode: 400, body: 'Query required' };
  }

  try {
    /* 1. Embed the query */
    const emb = await openai.embeddings.create({
      model: 'text-embedding-ada-002',
      input: query,
    });
    const qVec = emb.data[0].embedding;

    /* 2. RPC match (over-fetch 4×, then rerank) */
    const { data: rows } = await sb.rpc('match_research_chunks', {
      query_embedding: qVec,
      match_threshold: 0,
      match_count: limit * 4,
    });

    if (!rows?.length) {
      return {
        statusCode: 200,
        body: JSON.stringify({ answer: 'No relevant documents found.', results: [] })
      };
    }

    const ranked = rows
      .map(r => ({ ...r, similarity: cosine(qVec, r.embedding) }))
      .sort((a, b) => b.similarity - a.similarity)
      .slice(0, limit);

    /* 3. Build prompt */
    const ctx = ranked.map((r, i) =>
      `[${i + 1}] ${r.text.replace(/\s+/g, ' ').slice(0, 800)}`
    ).join('\n');

    const prompt = `
Answer strictly from the sources below; cite like [1], [2] …

QUESTION: "${query}"

SOURCES:
${ctx}`.trim();

    /* 4. GPT-4o */
    const chat = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        { role: 'system', content: 'You are a misophonia research assistant.' },
        { role: 'user', content: prompt }
      ],
      max_tokens: 700,
      temperature: 0.2,
    });

    ranked.forEach(r => delete r.embedding);   // strip big vectors

    return {
      statusCode: 200,
      headers: { 'content-type': 'application/json' },
      body: JSON.stringify({ answer: chat.choices[0].message.content, results: ranked })
    };
  } catch (err) {
    console.error(err);
    return { statusCode: 500, body: JSON.stringify({ error: String(err) }) };
  }
}


================================================================================


################################################################################
# File: src/RagAssistant.jsx
################################################################################

// File: src/RagAssistant.jsx

import React, { useState } from 'react';
import { marked } from 'marked';

// ⬇️  NEW DEFAULT:  falls back to " /api/rag" in production/PWA
//const API = import.meta.env.VITE_RAG_ENDPOINT || '/api/rag';
const API = import.meta.env.VITE_RAG_ENDPOINT || '/.netlify/functions/rag';

export default function RagAssistant() {
  const [q, setQ] = useState('');
  const [loading, setLoading] = useState(false);
  const [answer, setAnswer] = useState('');
  const [results, setResults] = useState([]);

  const ask = async e => {
    e.preventDefault();
    if (!q.trim()) return;
    setLoading(true);

    try {
      const r = await fetch(API, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ query: q, limit: 8 })
      });
      if (!r.ok) throw new Error(`Server responded ${r.status}`);
      const d = await r.json();
      setAnswer(d.answer ? marked.parse(d.answer) : 'No answer.');
      setResults(d.results || []);
    } catch (err) {
      setAnswer(`<p style="color:red;">${err.message}</p>`);
      setResults([]);
    } finally {
      setLoading(false);    // always clear spinner
    }
  };

  return (
    <div className="ra-card">
      <h2 style={{marginBottom: '1rem'}}>Research Assistant</h2>
      
      <div className="ra-body">
        {answer && (
          <div
            dangerouslySetInnerHTML={{ __html: answer }}
            style={{ marginBottom: '1.5rem' }}
          />
        )}

        {loading && <p>Searching…</p>}

        {results.map((r, i) => (
          <details key={i} style={{marginBottom: '1rem'}}>
            <summary>
              <strong>[{i + 1}] {r.doc?.title || 'Untitled'}</strong>
              <span style={{float: 'right', fontSize: '.85rem', opacity: .7}}>
                {r.similarity.toFixed(1)} %
              </span>
            </summary>
            <blockquote style={{margin: '.75rem 0'}}>{r.text}</blockquote>
            <p style={{fontSize: '.85rem', opacity: .8}}>
              {(r.doc?.authors || []).join(', ') || 'Unknown authors'} · {r.doc?.journal || ''} ({r.doc?.year || 'n.d.'}) · pp. {r.page_start}-{r.page_end}
            </p>
            {r.doc?.doi && (
              <a href={`https://doi.org/${r.doc.doi}`} target="_blank" rel="noreferrer">doi:{r.doc.doi}</a>
            )}
          </details>
        ))}
      </div>

      <form className="ra-input-row" onSubmit={ask}>
        <input
          className="ra-input"
          value={q}
          onChange={e => setQ(e.target.value)}
          placeholder="Ask a research question…"
        />
        <button className="ra-btn" disabled={loading}>Search</button>
      </form>
    </div>
  );
}


================================================================================


################################################################################
# File: netlify/functions/search.js
################################################################################

import { createClient } from '@supabase/supabase-js';
import { Configuration, OpenAIApi } from 'openai';

export async function handler(event, context) {
  if (event.httpMethod !== 'POST') {
    return { statusCode: 405, body: 'Method Not Allowed' };
  }
  
  try {
    const body = JSON.parse(event.body);
    const { query, filters, expandContext, page, pageSize, similarityThreshold } = body;
    
    // Initialize Supabase client
    const supabase = createClient(
      process.env.SUPABASE_URL,
      process.env.SUPABASE_SERVICE_ROLE_KEY
    );
    
    // Initialize OpenAI for embeddings
    const configuration = new Configuration({
      apiKey: process.env.OPENAI_API_KEY,
    });
    const openai = new OpenAIApi(configuration);
    
    // Generate embedding
    const embeddingResponse = await openai.createEmbedding({
      model: "text-embedding-ada-002",
      input: query,
    });
    
    const queryEmbedding = embeddingResponse.data.data[0].embedding;
    
    // Search using the RPC function
    const { data: results, error } = await supabase.rpc(
      'match_documents',
      {
        query_embedding: queryEmbedding,
        match_threshold: similarityThreshold || 0.6,
        match_count: pageSize || 10
      }
    );
    
    if (error) throw error;
    
    return {
      statusCode: 200,
      body: JSON.stringify({
        result: {
          results,
          page: page || 1,
          pageSize: pageSize || 10,
          query
        }
      })
    };
  } catch (error) {
    console.error('Error in search function:', error);
    return {
      statusCode: 500,
      body: JSON.stringify({ error: error.message })
    };
  }
}


================================================================================


################################################################################
# File: src/index.css
################################################################################

:root {
  font-family: system-ui, Avenir, Helvetica, Arial, sans-serif;
  line-height: 1.5;
  font-weight: 400;

  color-scheme: light dark;
  color: rgba(255, 255, 255, 0.87);
  background-color: #242424;

  font-synthesis: none;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

a {
  font-weight: 500;
  color: #646cff;
  text-decoration: inherit;
}
a:hover {
  color: #535bf2;
}

body {
  margin: 0;
  display: flex;
  min-width: 320px;
  min-height: 100vh;
}

h1 {
  font-size: 3.2em;
  line-height: 1.1;
}

button {
  border-radius: 8px;
  border: 1px solid transparent;
  padding: 0.6em 1.2em;
  font-size: 1em;
  font-weight: 500;
  font-family: inherit;
  background-color: #1a1a1a;
  cursor: pointer;
  transition: border-color 0.25s;
}
button:hover {
  border-color: #646cff;
}
button:focus,
button:focus-visible {
  outline: 4px auto -webkit-focus-ring-color;
}

@media (prefers-color-scheme: light) {
  :root {
    color: #213547;
    background-color: #ffffff;
  }
  a:hover {
    color: #747bff;
  }
  button {
    background-color: #f9f9f9;
  }
}


================================================================================


################################################################################
# File: netlify.toml
################################################################################

# Netlify configuration for Misophonia Companion
[build]
  command = "npm run build"
  publish = "dist"

[dev]
  command = "npm run dev"
  targetPort = 5173

[functions]
  directory = "netlify/functions"
  external_node_modules = ["openai", "@supabase/supabase-js"]

# Redirect all requests to index.html for SPA routing
[[redirects]]
  from = "/api/*"
  to = "/.netlify/functions/:splat" 
  status = 200

[[redirects]]
  from = "/api/gemini"
  to = "/.netlify/functions/research"
  status = 200

[[redirects]]
  from = "/api/rag"
  to = "/.netlify/functions/rag"
  status = 200

[[redirects]]
  from = "/*"
  to = "/index.html"
  status = 200

[functions."rag"]   # file rag.js
  node_bundler = "esbuild"
  deno_import_map = "netlify/import_map.json" # only if you jump to Deno/Edge


================================================================================


################################################################################
# File: index.html
################################################################################

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Vite + React</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>


================================================================================


################################################################################
# File: src/main.jsx
################################################################################

import { StrictMode } from 'react'
import { createRoot } from 'react-dom/client'
import './index.css'
import App from './App.jsx'

createRoot(document.getElementById('root')).render(
  <StrictMode>
    <App />
  </StrictMode>,
)


================================================================================


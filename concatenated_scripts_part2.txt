# Concatenated Project Code - Part 2 of 3
# Generated: 2025-05-09 15:45:06
# Root Directory: /Users/gianmariatroiani/Documents/misophonia-companion-v3
================================================================================

################################################################################
# File: scripts/templates/rag_web_interface_gian_v1 copy.py
################################################################################

# File: scripts/templates/rag_web_interface_gian_v1 copy.py

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Interactive RAG demo (Flask) that queries a Supabase vector store
(`research_chunks`) instead of Firebase / Firestore.

Changes in R2 ­­­→  ▸ removes the /stats endpoint + front‑end fetch  
                   ▸ cleaner citation helpers (no "Unknown" placeholders)
"""

from __future__ import annotations

import json
import logging
import os
import sys
from typing import Any, Dict, List, Sequence

from dotenv import load_dotenv
from flask import Flask, jsonify, render_template, request
from openai import OpenAI
from supabase import create_client


# ───────────────────────────── configuration ───────────────────────────── #

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUPABASE_URL   = os.getenv("SUPABASE_URL")
SUPABASE_KEY   = os.getenv("SUPABASE_SERVICE_ROLE_KEY")


# ------------------------------------------------------------------
# Bibliographic fields we want to pull from research_documents
DOC_FIELDS = (
    "title, authors, year, journal, doi"
)

# All columns we need from research_chunks  +  the join
CHUNK_SELECT = (
    "text, page_start, page_end, "
    f"research_documents({DOC_FIELDS})"
)
# ------------------------------------------------------------------

if not OPENAI_API_KEY:
    print("⛔  OPENAI_API_KEY missing — aborting")
    sys.exit(1)

client = OpenAI(api_key=OPENAI_API_KEY)

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s — %(levelname)s — %(message)s"
)
log = logging.getLogger(__name__)

app = Flask(__name__)


# ───────────────────────────── Supabase helpers ─────────────────────────── #

def init_supabase():
    if not SUPABASE_URL or not SUPABASE_KEY:
        log.error("SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY not set.")
        sys.exit(1)
    return create_client(SUPABASE_URL, SUPABASE_KEY)


sb = init_supabase()        # single client reused everywhere


# ──────────────────────── metadata / citation helpers ────────────────────── #

def _first(meta: Dict[str, Any], keys: Sequence[str]):
    """Return first non‑empty, non‑placeholder value in *meta* for any of *keys*."""
    for k in keys:
        v = meta.get(k)
        if v and str(v).strip() and not str(v).lower().startswith("unknown"):
            return v
    return None


def _first_author(meta: Dict[str, Any]) -> str | None:
    if (a := _first(meta, ["primary_author", "author", "creator"])):
        return str(a)
    if (lst := meta.get("authors")):
        if isinstance(lst, list) and lst:
            return str(lst[0])
        # if it's a string "Foo, Bar" take first token
        if isinstance(lst, str) and lst.strip() and not lst.lower().startswith("unknown"):
            return lst.split(",")[0].strip()
    return None


def _page(meta: Dict[str, Any]) -> str | None:
    if (p := _first(meta, ["page", "page_number"])):
        return str(p)
    if (rng := _first(meta, ["pages", "page_range"])):
        # Accept list or "12–15" / "12-15"
        if isinstance(rng, list) and rng:
            return str(rng[0])
        if isinstance(rng, str):
            return rng.split("–")[0].split("-")[0]
    return None


def make_citation(meta: Dict[str, Any]) -> str:
    """
    Compact citation for UI cards – omits any empty element.
    Format:  Author Year – Title. Journal. doi:… p.X
    """
    parts: list[str] = []

    if (a := _first_author(meta)):
        parts.append(a)

    if (y := _first(meta, ["year", "pub_year"])):
        parts.append(str(y))

    if (t := _first(meta, ["title", "document_title", "section"])):
        parts.append(f"– {t.strip()}")

    if (j := _first(meta, ["journal", "source"])):
        parts.append(j)

    if (doi := _first(meta, ["doi"])):
        parts.append(f"doi:{doi}")

    if (pg := _page(meta)):
        parts.append(f"p.{pg}")

    return " ".join(parts) if parts else "No citation metadata"


def long_citation(meta: Dict[str, Any]) -> str:
    """
    Longer citation fed to GPT (always includes Title if available).
    """
    auth  = _first_author(meta) or ""
    year  = _first(meta, ["year", "pub_year"]) or ""
    title = _first(meta, ["title", "document_title", "section"]) or ""
    jour  = _first(meta, ["journal", "source"]) or ""
    doi   = _first(meta, ["doi"]) or ""
    pg    = _page(meta) or ""

    bits = []
    if auth or year:
        bits.append(f"{auth} ({year}).".strip())
    if title:
        bits.append(title)
    if jour:
        bits.append(jour)
    if doi:
        bits.append(f"doi:{doi}")
    if pg:
        bits.append(f"p.{pg}")

    return " ".join(bits)


# keep alias for any legacy code
pretty_source = make_citation


# ────────────────────────────── embedding helpers ────────────────────────── #

def generate_embedding(text: str) -> List[float] | None:
    """Generate an Ada‑002 embedding for *text*."""
    try:
        resp = client.embeddings.create(model="text-embedding-ada-002", input=text)
        return resp.data[0].embedding
    except Exception as exc:                                  # pragma: no cover
        log.error("Embedding failed: %s", exc)
        return None


# ─────────────────────────────── search logic ────────────────────────────── #

def semantic_search(query: str, limit: int = 5) -> List[Dict[str, Any]]:
    """
    Server‑side vector search via RPC.
    """
    vec = generate_embedding(query)
    if vec is None:
        return []

    try:
        resp = (
            sb.rpc(
                "search_research_chunks",
                {
                    "query_embedding": vec,
                    "match_count": limit,
                    "similarity_threshold": 0.6,
                },
            ).execute()
        )
        rows = resp.data or []
        for r in rows:
            r["similarity"] = float(r.get("similarity", 0.0))
            r["match_type"] = f"Semantic ({r['similarity']:.4f})"
            r["chunk_id"]   = r.pop("id", "unknown")
            r["source"]     = make_citation(r.get("metadata", {}))
        return rows
    except Exception as exc:
        log.error("RPC search failed: %s", exc)
        return []


def keyword_search(query: str, limit: int = 5) -> List[Dict[str, Any]]:
    """
    Simple client‑side keyword fallback over a capped set of chunks.
    """
    try:
        resp = (
            sb.table("research_chunks")
            .select(CHUNK_SELECT)
            .limit(2_000)
            .execute()
        )
        all_chunks = resp.data or []
    except Exception as exc:
        log.error("Keyword search fetch failed: %s", exc)
        return []

    stop  = {"the","and","or","in","of","to","a","is","that","for","on","with"}
    terms = {t for t in query.lower().split() if t not in stop}
    if not terms:
        return []

    results: List[Dict[str, Any]] = []
    for ch in all_chunks:
        txt = str(ch.get("text", "")).lower()
        if not txt:
            continue

        matches = sum(1 for t in terms if t in txt)
        if matches == 0:
            continue

        score = matches / len(terms)
        if query.lower() in txt:
            score += 0.3
        if score < 0.3:
            continue

        meta = ch.get("metadata") or {}
        results.append(
            {
                "chunk_id": meta.get("chunk_id", "unknown"),
                "text": ch.get("text", ""),
                "metadata": meta,
                "similarity": float(score),
                "match_type": f"Keyword ({score:.4f})",
                "source": make_citation(meta),
            }
        )

    results.sort(key=lambda r: r["similarity"], reverse=True)
    return results[:limit]


def comprehensive_search(query: str, limit: int = 5) -> List[Dict[str, Any]]:
    sem = semantic_search(query, limit)
    kw  = keyword_search(query, limit)

    combined: List[Dict[str, Any]] = []
    seen = set()

    for r in sem + kw:
        cid = r.get("chunk_id")
        if cid not in seen:
            combined.append(r)
            seen.add(cid)

    combined.sort(key=lambda r: r["similarity"], reverse=True)
    return combined[:limit]


# ──────────────────────────── RAG generation ─────────────────────────────── #

def generate_rag_response(
    query: str,
    docs: List[Dict[str, Any]],
    max_tokens: int = 500,
) -> str:
    """Produce the final GPT‑4o answer with inline citations."""
    try:
        snippets = [
            f"Source {i} — {long_citation(d.get('metadata', {}))}\n{d.get('text','')}\n"
            for i, d in enumerate(docs, 1)
        ]

        prompt = f"""You are a helpful AI assistant specialised in misophonia research.

Answer the question below using *only* the provided research snippets.

Question: "{query}"

Snippets:
{''.join(snippets)}

Give a concise, structured answer with numbered citations like [1], [2] … .
If evidence is insufficient, say so explicitly."""
        comp = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "You answer misophonia questions strictly from the provided sources, citing them.",
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=max_tokens,
        )
        return comp.choices[0].message.content
    except Exception as exc:                                                  # pragma: no cover
        log.error("RAG generation failed: %s", exc)
        return f"Error generating response: {exc}"


# ───────────────────────────── Flask endpoints ───────────────────────────── #

@app.route("/")
def index():
    return render_template("index.html")


@app.route("/search", methods=["POST"])
def search():
    try:
        data = request.get_json(force=True, silent=True)
        if not data or not data.get("query"):
            return jsonify({"error": "No query provided"}), 400

        query = data["query"].strip()
        limit = int(data.get("limit", 5))

        results = comprehensive_search(query, limit)

        if not results:
            return jsonify(
                {"error": "No relevant documents found for your query", "results": []}
            ), 404

        # After you get results from the database:
        sources = []

        for r in results:
            # PostgREST nests the 1‑to‑1 join as an OBJECT, not a list
            doc = r.get("research_documents") or {}

            # authors is a Postgres text[] → Python list; stringify for HTML
            authors = ", ".join(doc["authors"]) if isinstance(doc.get("authors"), list) else doc.get("authors")

            # page range: prefer the 'pages' column; otherwise build it
            page_range = (
                doc.get("pages")
                or f"{r.get('page_start', '?')}-{r.get('page_end', '?')}"
            )

            sources.append(
                {
                    "chunk"      : r.get("text", ""),         # the snippet
                    "title"      : doc.get("title")   or "Unknown Title",
                    "section"    : "Unknown Section",    # add later if you store it
                    "authors"    : authors            or "Unknown Authors",
                    "journal"    : doc.get("journal") or "Unknown Journal",
                    "year"       : doc.get("year")    or "Unknown",
                    "volume"     : doc.get("volume"),
                    "issue"      : doc.get("issue"),
                    "doi"        : doc.get("doi"),
                    "page_range" : page_range,
                }
            )

        answer = generate_rag_response(query, results)
        return jsonify({"results": sources, "response": answer})
    except Exception as e:
        logging.exception("Search failed")
        return jsonify({"error": str(e)}), 500


@app.route("/stats")
def stats():
    try:
        # 1) total rows in the vector table ----------------------------
        total_chunks = (
            sb.table("research_chunks")
              .select("id", count="exact")
              .execute()
              .count
            or 0
        )

        return jsonify({"total_chunks": total_chunks}), 200

    except Exception as e:
        logging.exception("Stats endpoint failed")
        return jsonify({"error": str(e)}), 500


# ────────────────────────────── bootstrap html ───────────────────────────── #

if __name__ == "__main__":
    tpl_dir   = os.path.join(os.path.dirname(__file__), "templates")
    os.makedirs(tpl_dir, exist_ok=True)
    index_path = os.path.join(tpl_dir, "index.html")

    if not os.path.exists(index_path):
        with open(index_path, "w") as fp:
            fp.write(
"""<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Misophonia Research RAG Interface</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
  <style>
    body{padding:20px;background:#f8f9fa}
    .container{max-width:1000px;margin:0 auto}
    .header{text-align:center;margin-bottom:30px}
    .search-box{margin-bottom:20px}
    .results-container{margin-top:20px}
    .result-card{margin-bottom:15px;border-radius:8px;box-shadow:0 2px 5px rgba(0,0,0,.1)}
    .result-card .card-header{font-weight:bold;display:flex;justify-content:space-between}
    .loading{text-align:center;padding:20px;display:none}
    .response-container{margin-top:30px;padding:20px;background:#fff;border-radius:8px;box-shadow:0 2px 5px rgba(0,0,0,.1)}
    pre { background:#f8f9fa;border:0; }
  </style>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>Misophonia Research RAG Interface</h1>
      <p class="text-muted">Search across research documents with semantic and keyword retrieval</p>
    </div>

    <div class="search-box">
      <div class="input-group mb-3">
        <input type="text" id="search-input" class="form-control form-control-lg"
               placeholder="Ask a question about misophonia…" aria-label="Search query">
        <button class="btn btn-primary" type="button" id="search-button">Search</button>
      </div>
      <div class="form-text">Try questions about treatments, neurological basis, symptoms, or coping strategies</div>
    </div>

    <div class="loading" id="loading">
      <div class="spinner-border text-primary" role="status">
        <span class="visually-hidden">Loading…</span>
      </div>
      <p>Searching research documents and generating response…</p>
    </div>

    <div id="response-area" style="display:none">
      <div class="response-container">
        <h3>Research‑Based Answer</h3>
        <div id="response-content"></div>
      </div>

      <div class="results-container">
        <h3>Source Documents</h3>
        <div id="results-list"></div>
      </div>
    </div>
  </div>

<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
<script>
document.addEventListener("DOMContentLoaded",()=>{
  const $q      = document.getElementById("search-input");
  const $btn    = document.getElementById("search-button");
  const $load   = document.getElementById("loading");
  const $resp   = document.getElementById("response-area");
  const $respCt = document.getElementById("response-content");
  const $list   = document.getElementById("results-list");

  async function search(){
    const query=$q.value.trim();
    if(!query) return;

    $load.style.display="block";
    $resp.style.display="none";

    try{
      const res=await fetch("/search",{
        method:"POST",
        headers:{"Content-Type":"application/json"},
        body:JSON.stringify({query,limit:5})
      });
      if(!res.ok){
        let msg=`Server error (${res.status})`;
        try{msg=(await res.json()).error||msg;}catch{}
        throw new Error(msg);
      }
      const data=await res.json();
      const answerHtml=String(data.response||"").replace(/\\n/g,"<br>");
      $respCt.innerHTML=`<p>${answerHtml}</p>`;
      $list.innerHTML="";

      (data.results||[]).forEach((r,i)=>{
        const card=document.createElement("div");
        card.className="card result-card";
        const src = data.results[i].metadata;
        card.innerHTML=`
          <div class="card-header bg-light d-flex justify-content-between align-items-center"
               data-bs-toggle="collapse" data-bs-target="#chunk-${i}">
            <span>${r.source}</span>
            <span class="badge bg-primary opacity-75">${r.match_type}</span>
          </div>
          <div id="chunk-${i}" class="collapse show">
            <div class="card-body">
              <div class="d-flex justify-content-between align-items-baseline">
                <strong>
                  ${src.journal} (${src.year})
                </strong>
                ${src.doi ? `<a href="https://doi.org/${src.doi}" target="_blank" class="badge bg-primary">DOI</a>` : ''}
              </div>

              <h4 class="card-title">${src.title}</h4>
              <small class="text-muted">
                ${src.section}
              </small>

              <p class="mb-0">
                <em>${src.authors}</em><br>
                ${src.volume ? `${src.journal} ${src.volume}${src.issue ? `(${src.issue})` : ''}:` : ''}
                ${src.page_range}
              </p>
              
              <div class="mb-2 small text-muted">Similarity: ${(r.similarity??0).toFixed(4)}</div>
              <pre class="small text-muted mb-0" style="white-space:pre-wrap;">${r.text||""}</pre>
            </div>
          </div>`;
        $list.appendChild(card);
      });

    }catch(err){
      $respCt.innerHTML=`<div class="alert alert-warning">${err.message}</div>`;
      $list.innerHTML="";
    }finally{
      $load.style.display="none";
      $resp.style.display="block";
    }
  }

  $btn.addEventListener("click",search);
  $q.addEventListener("keypress",e=>{if(e.key==="Enter")search();});
});
</script>
</body>
</html>"""
            )

    log.info("Starting Flask on 0.0.0.0:8080 …")
    app.run(host="0.0.0.0", port=8080, debug=True)


================================================================================


################################################################################
# File: scripts/rag_web_app_v9.py
################################################################################

# File: scripts/rag_web_app_v9.py

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Mini Flask app that answers misophonia questions with Retrieval‑Augmented
Generation (gpt-4.1-mini-2025-04-14 + Supabase pgvector).

### Patch 2  (2025‑05‑06)
• **Embeddings** now created with **text‑embedding‑ada‑002** (1536‑D).  
• Similarity is re‑computed client‑side with a **plain cosine function** so the
  ranking no longer depends on pgvector's built‑in distance or any RPC
  threshold quirks.

The rest of the grounded‑answer logic (added in Patch 1) is unchanged.
"""
from __future__ import annotations

import logging
import math
import os
import re
from pathlib import Path        # (unused but left in to mirror original)
from typing import Dict, List
import json

from dotenv import load_dotenv
from flask import Flask, jsonify, request, make_response
from openai import OpenAI
from supabase import create_client
from flask_compress import Compress
from flask_cors import CORS

# ────────────────────────────── configuration ───────────────────────────── #

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUPABASE_URL   = os.getenv("SUPABASE_URL")
SUPABASE_KEY   = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
PORT           = int(os.getenv("PORT", 8080))

if not (OPENAI_API_KEY and SUPABASE_URL and SUPABASE_KEY):
    raise SystemExit(
        "❌  Required env vars: OPENAI_API_KEY, SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY"
    )

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s",
)
log = logging.getLogger("rag_app")

sb = create_client(SUPABASE_URL, SUPABASE_KEY)
oa = OpenAI(api_key=OPENAI_API_KEY)

app = Flask(__name__)
CORS(app)
app.config['COMPRESS_ALGORITHM'] = 'gzip'
Compress(app)

# ────────────────────────────── helper functions ────────────────────────── #


def embed(text: str) -> List[float]:
    """
    Return OpenAI embedding vector for *text* using text‑embedding‑ada‑002.

    ada‑002 has 1536 dimensions and is inexpensive yet solid for similarity.
    """
    resp = oa.embeddings.create(
        model="text-embedding-ada-002",
        input=text[:8192],  # safety slice
    )
    return resp.data[0].embedding


def cosine_similarity(a: List[float], b: List[float]) -> float:
    """Plain cosine similarity between two equal‑length vectors."""
    dot = sum(x * y for x, y in zip(a, b))
    na = math.sqrt(sum(x * x for x in a))
    nb = math.sqrt(sum(y * y for y in b))
    return dot / (na * nb + 1e-9)


# Add in-memory embedding cache
_qcache = {}
def embed_cached(text):
    if text in _qcache: return _qcache[text]
    vec = embed(text)
    _qcache[text] = vec
    return vec


# Add regex patterns for bibliography detection
_DOI_RE   = re.compile(r'\b10\.\d{4,9}/[-._;()/:A-Z0-9]+\b', re.I)
_YEAR_RE  = re.compile(r'\b(19|20)\d{2}\b')

def looks_like_refs(text: str) -> bool:
    """
    Return True if this chunk is likely just a bibliography list:
      • more than 12 DOIs, or
      • more than 15 year mentions.
    """
    doi_count  = len(_DOI_RE.findall(text))
    year_count = len(_YEAR_RE.findall(text))
    return doi_count > 12 or year_count > 15


def semantic_search(
    query: str,
    *,
    limit: int = 8,
    threshold: float = 0.0,
) -> List[Dict]:
    """
    Retrieve candidate chunks via the pgvector RPC, then re-rank with an
    **explicit cosine similarity** so the final score is always in
    **[-100 … +100] percent**.

    Why the extra work?
    -------------------
    •  The SQL function returns a raw inner-product that can be > 1.  
       (embeddings are *not* unit-length.)  
    •  By pulling the real 1 536-D vectors and re-computing a cosine we get a
       true, bounded similarity that front-end code can safely show.

    The -100 … +100 range is produced by:  
        pct = clamp(cosine × 100, -100, 100)
    """
    # 1. Embed the query once and keep it cached
    q_vec = embed_cached(query)

    # 2. Fast ANN search in Postgres (over-fetch 4× so we can re-rank)
    rows = (
        sb.rpc(
            "match_research_chunks",
            {
                "query_embedding": q_vec,
                "match_threshold": threshold,
                "match_count": limit * 4,
            },
        )
        .execute()
        .data
    ) or []

    # 3. Filter out bibliography-only chunks
    rows = [r for r in rows if not looks_like_refs(r["text"])]

    if not rows:
        return []

    # 4. Fetch document metadata (title, authors …) in one round-trip
    doc_ids = {r["document_id"] for r in rows}
    meta = {
        d["id"]: d
        for d in (
            sb.table("research_documents")
              .select("id,title,authors,year,journal,doi,abstract,keywords,research_topics,source_pdf")
              .in_("id", list(doc_ids))
              .execute()
              .data
            or []
        )
    }

    # 5. Pull embeddings once and compute **plain cosine** (no scaling)
    chunk_ids = [r["id"] for r in rows]

    emb_rows = (
        sb.table("research_chunks")
          .select("id, embedding")
          .in_("id", chunk_ids)
          .execute()
          .data
    ) or []

    emb_map: Dict[str, List[float]] = {}
    for e in emb_rows:
        raw = e["embedding"]
        if isinstance(raw, list):                    # list[Decimal]
            emb_map[e["id"]] = [float(x) for x in raw]
        elif isinstance(raw, str) and raw.startswith('['):   # TEXT  "[…]"
            emb_map[e["id"]] = [float(x) for x in raw.strip('[]').split(',')]

    for r in rows:
        vec = emb_map.get(r["id"])
        if vec:                                     # we now have the real vector
            cos = cosine_similarity(q_vec, vec)
            r["similarity"] = round(cos * 100, 1)   # –100…+100 % (or 0…100 %)
        else:                                       # fallback if something failed
            dist = float(r.get("similarity", 1.0))  # 0…2 cosine-distance
            r["similarity"] = round((1.0 - dist) * 100, 1)

        r["doc"] = meta.get(r["document_id"], {})

    # 6. Keep the top *limit* rows after proper re-ranking
    ranked = sorted(rows, key=lambda x: x["similarity"], reverse=True)[:limit]
    return ranked


# ──────────────────────── NEW RAG‑PROMPT HELPERS ───────────────────────── #

MAX_PROMPT_CHARS: int = 24_000  # ~6 k tokens @ 4 chars/token heuristic


def trim_chunks(chunks: List[Dict]) -> List[Dict]:
    """
    Fail‑safe guard: ensure concatenated chunk texts remain under the
    MAX_PROMPT_CHARS budget.  Keeps highest‑similarity chunks first.
    """
    sorted_chunks = sorted(chunks, key=lambda c: c.get("similarity", 0), reverse=True)
    output: List[Dict] = []
    total_chars = 0
    for c in sorted_chunks:
        chunk_len = len(c["text"])
        if total_chars + chunk_len > MAX_PROMPT_CHARS:
            break
        output.append(c)
        total_chars += chunk_len
    return output


def build_prompt(question: str, chunks: List[Dict]) -> str:
    """
    Build a structured prompt that asks GPT to:
      • answer in Markdown with short intro + numbered list of key points
      • cite inline like [1], [2] …
      • finish with a Bibliography that includes the *paper title*
    """
    snippet_lines, biblio_lines = [], []
    for i, c in enumerate(chunks, 1):
        snippet_lines.append(
            f"[{i}] \"{c['text'].strip()}\" "
            f"(pp. {c['page_start']}-{c['page_end']})"
        )

        d = c["doc"]
        title   = d.get("title", "Untitled")
        authors = ", ".join(d.get("authors") or ["Unknown"])
        journal = d.get("journal", "Unknown journal")
        year    = d.get("year", "n.d.")
        pages   = f"pp. {c['page_start']}-{c['page_end']}"
        doi_raw = d.get("doi")
        doi_md  = f"[doi:{doi_raw}](https://doi.org/{doi_raw})" if doi_raw else ""

        # Title now comes first ↓↓↓
        biblio_lines.append(
            f"[{i}] *{title}* · {authors} · {journal} ({year}) · {pages} {doi_md}"
        )

    prompt_parts = [
        "You are an expert assistant specialised in misophonia.",
        "====",
        "QUESTION:",
        question,
        "====",
        "CONTEXT:",
        *snippet_lines,
        "====",
        "INSTRUCTIONS:",
        "• Write the answer in **Markdown**:",
        "    – A one‑sentence overview",
        "    – Then a numbered list of key findings (each ≤ 2 lines)",
        "    – End with a short concluding sentence",
        "• Cite sources inline like [1], [2] …",
        "• After the answer, reproduce a section titled 'BIBLIOGRAPHY:'",
        "  listing each source exactly as provided below (title included).",
        "• If none of the chunks answer the question, reply:",
        '  "I\'m sorry, I don\'t have sufficient information to answer that."',
        "====",
        "BEGIN OUTPUT",
        "ANSWER:",
        "",                       # <- model writes here
        "BIBLIOGRAPHY:",
        *biblio_lines,
    ]

    return '\n'.join(prompt_parts)


def extract_citations(answer: str) -> List[str]:
    """
    Parse numeric citations (e.g., "[1]", "[2]") from the answer text.
    Returns unique citation numbers in ascending order.
    """
    citations = re.findall(r"\[(\d+)\]", answer)
    return sorted(set(citations), key=int)


# ──────────────────────────────── routes ────────────────────────────────── #

@app.post("/search")
def search():
    payload = request.get_json(force=True, silent=True) or {}
    question = (payload.get("query") or "").strip()
    if not question:
        return jsonify({"error": "Missing 'query'"}), 400

    try:
        # Retrieve semantic matches (client‑side cosine re‑ranked)
        raw_matches = semantic_search(question, limit=int(payload.get("limit", 8)))

        if not raw_matches:
            return jsonify(
                {
                    "answer": "I'm sorry, I don't have sufficient information to answer that.",
                    "citations": [],
                    "results": [],
                }
            )

        # ──────────────────── TRIM CHUNKS TO BUDGET ──────────────────── #
        chunks = trim_chunks(raw_matches)

        # ──────────────────── BUILD PROMPT & CALL LLM ─────────────────── #
        prompt = build_prompt(question, chunks)

        completion = oa.chat.completions.create(
            model="gpt-4.1-mini-2025-04-14",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
        )
        answer_text: str = completion.choices[0].message.content.strip()

        # ──────────────────── EXTRACT CITATIONS ──────────────────────── #
        citations = extract_citations(answer_text)

        # Remove embedding vectors before sending back to the browser
        for m in raw_matches:
            m.pop("embedding", None)

        # ──────────────────── RETURN JSON ─────────────────────────────── #
        response = jsonify(
            {
                "answer": answer_text,
                "citations": citations,
                "results": raw_matches,
            }
        )
        response.headers['Connection'] = 'keep-alive'
        return response
    except Exception as exc:  # noqa: BLE001
        log.exception("search failed")
        return jsonify({"error": str(exc)}), 500


@app.get("/stats")
def stats():
    """Tiny ops endpoint—count total chunks."""
    resp = sb.table("research_chunks").select("id", count="exact").execute()
    return jsonify({"total_chunks": resp.count})


# ──────────────────────────────── main ─────────────────────────────────── #

if __name__ == "__main__":
    log.info("Starting Flask on 0.0.0.0:%s …", PORT)
    app.run(host="0.0.0.0", port=PORT, debug=True)


================================================================================


################################################################################
# File: scripts/templates/rag_web_interface_gian_v1.py
################################################################################

# File: scripts/templates/rag_web_interface_gian_v1.py

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Interactive RAG demo (Flask) that queries a Supabase vector store
(`research_chunks`) instead of Firebase / Firestore.

R5  →  • robust metadata parsing (string → JSON → dict)
        • never throws on missing / malformed metadata
        • identical external API; front-end needs **no** changes
"""

from __future__ import annotations

import json
import logging
import os
import sys
from typing import Any, Dict, List, Sequence

from dotenv import load_dotenv
from flask import Flask, jsonify, render_template, request
from openai import OpenAI
from supabase import create_client


# ───────────────────────────── configuration ───────────────────────────── #

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUPABASE_URL   = os.getenv("SUPABASE_URL")
SUPABASE_KEY   = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

DOC_FIELDS = (
    "title, authors, year, journal, doi, pages, volume, issue"
)

if not OPENAI_API_KEY:
    print("⛔  OPENAI_API_KEY missing — aborting")
    sys.exit(1)

client = OpenAI(api_key=OPENAI_API_KEY)

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s — %(levelname)s — %(message)s"
)
log = logging.getLogger(__name__)
app = Flask(__name__)


# ───────────────────────────── Supabase helpers ─────────────────────────── #

def init_supabase():
    if not SUPABASE_URL or not SUPABASE_KEY:
        log.error("SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY not set.")
        sys.exit(1)
    return create_client(SUPABASE_URL, SUPABASE_KEY)


sb = init_supabase()


# ──────────────────────── metadata / citation helpers ────────────────────── #

def _as_meta(obj: Any) -> Dict[str, Any]:
    """Best‑effort coercion to dict for the *metadata* column."""
    if isinstance(obj, dict):
        return obj
    if isinstance(obj, str):
        try:
            return json.loads(obj) or {}
        except Exception:
            return {}
    return {}


def _first(meta: Dict[str, Any], keys: Sequence[str]):
    for k in keys:
        v = meta.get(k)
        if v and str(v).strip() and not str(v).lower().startswith("unknown"):
            return v
    return None


def _first_author(meta: Dict[str, Any]) -> str | None:
    if (a := _first(meta, ["primary_author", "author", "creator"])):
        return str(a)
    if (lst := meta.get("authors")):
        if isinstance(lst, list) and lst:
            return str(lst[0])
        if isinstance(lst, str) and lst.strip() and not lst.lower().startswith("unknown"):
            return lst.split(",")[0].strip()
    return None


def _page(meta: Dict[str, Any]) -> str | None:
    if (p := _first(meta, ["page", "page_number"])):
        return str(p)
    if (rng := _first(meta, ["pages", "page_range"])):
        if isinstance(rng, list) and rng:
            return str(rng[0])
        if isinstance(rng, str):
            return rng.split("–")[0].split("-")[0]
    if meta.get("page_start") is not None:
        return str(meta["page_start"])
    return None


def make_citation(meta_raw: Any) -> str:
    meta = _as_meta(meta_raw)

    parts = []
    if (a := _first_author(meta)):
        parts.append(a)
    if (y := _first(meta, ["year", "pub_year"])):
        parts.append(str(y))
    if (t := _first(meta, ["title", "document_title", "section"])):
        parts.append(f"– {t.strip()}")
    if (j := _first(meta, ["journal", "source"])):
        parts.append(j)
    if (doi := _first(meta, ["doi"])):
        parts.append(f"doi:{doi}")
    if (pg := _page(meta)):
        parts.append(f"p.{pg}")

    return " ".join(parts) if parts else "No citation metadata"


def long_citation(meta_raw: Any) -> str:
    meta = _as_meta(meta_raw)

    auth  = _first_author(meta) or ""
    year  = _first(meta, ["year", "pub_year"]) or ""
    title = _first(meta, ["title", "document_title", "section"]) or ""
    jour  = _first(meta, ["journal", "source"]) or ""
    doi   = _first(meta, ["doi"]) or ""
    pg    = _page(meta) or ""

    bits = []
    if auth or year:
        bits.append(f"{auth} ({year}).".strip())
    if title:
        bits.append(title)
    if jour:
        bits.append(jour)
    if doi:
        bits.append(f"doi:{doi}")
    if pg:
        bits.append(f"p.{pg}")

    return " ".join(bits)


# ────────────────────────────── embedding helpers ────────────────────────── #

def generate_embedding(text: str) -> List[float] | None:
    try:
        resp = client.embeddings.create(model="text-embedding-ada-002", input=text)
        return resp.data[0].embedding
    except Exception as exc:
        log.error("Embedding failed: %s", exc)
        return None


# ─────────────────────────────── search logic ────────────────────────────── #

def semantic_search(query: str, limit: int = 5) -> List[Dict[str, Any]]:
    vec = generate_embedding(query)
    if vec is None:
        return []

    try:
        resp = (
            sb.rpc(
                "search_research_chunks",
                {
                    "query_embedding": vec,
                    "match_count": limit,
                    "similarity_threshold": 0.6,
                },
            ).execute()
        )
        rows = resp.data or []
        for r in rows:
            r["similarity"] = float(r.get("similarity", 0.0))
            r["match_type"] = f"Semantic ({r['similarity']:.4f})"
            r["chunk_id"]   = r.pop("id", "unknown")
            r["metadata"]   = _as_meta(r.get("metadata", {}))
            r["source"]     = make_citation(r["metadata"])
        return rows
    except Exception as exc:
        log.error("RPC search failed: %s", exc)
        return []


def comprehensive_search(query: str, limit: int = 5) -> List[Dict[str, Any]]:
    return semantic_search(query, limit)


# ──────────────────────────── RAG generation ─────────────────────────────── #

def generate_rag_response(
    query: str,
    docs: List[Dict[str, Any]],
    max_tokens: int = 500,
) -> str:
    try:
        snippets = [
            f"Source {i} — {long_citation(d.get('metadata', {}))}\n{d.get('text','')}\n"
            for i, d in enumerate(docs, 1)
        ]

        prompt = f"""You are a helpful AI assistant specialised in misophonia research.

Answer the question below using *only* the provided research snippets.

Question: "{query}"

Snippets:
{''.join(snippets)}

Give a concise, structured answer with numbered citations like [1], [2] … .
If evidence is insufficient, say so explicitly."""
        comp = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "You answer misophonia questions strictly from the provided sources, citing them.",
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=max_tokens,
        )
        return comp.choices[0].message.content
    except Exception as exc:
        log.error("RAG generation failed: %s", exc)
        return f"Error generating response: {exc}"


# ───────────────────────────── Flask endpoints ───────────────────────────── #

@app.route("/")
def index():
    return render_template("index.html")


@app.route("/search", methods=["POST"])
def search():
    try:
        data = request.get_json(force=True, silent=True)
        if not data or not data.get("query"):
            return jsonify({"error": "No query provided"}), 400

        query = data["query"].strip()
        limit = int(data.get("limit", 5))

        results = comprehensive_search(query, limit)

        if not results:
            return jsonify(
                {"error": "No relevant documents found for your query", "results": []}
            ), 404

        # 1.  Bulk‑fetch document metadata
        doc_map: Dict[str, Dict[str, Any]] = {}
        doc_ids = {r["document_id"] for r in results if r.get("document_id")}
        if doc_ids:
            q = (
                sb.table("research_documents")
                  .select(f"id,{DOC_FIELDS}")
                  .in_("id", list(doc_ids))
                  .execute()
            )
            for d in q.data or []:
                doc_map[d["id"]] = d

        # 2.  Build front‑end payload
        sources = []
        for r in results:
            doc = doc_map.get(r.get("document_id"), {})

            authors = (
                ", ".join(doc["authors"])
                if isinstance(doc.get("authors"), list)
                else doc.get("authors")
            )

            page_range = (
                doc.get("pages")
                or f"{r.get('page_start', '?')}-{r.get('page_end', '?')}"
            )

            sources.append(
                {
                    "chunk":       r.get("text", ""),
                    "source":      r.get("source"),
                    "match_type":  r.get("match_type"),
                    "similarity":  r.get("similarity"),
                    "chunk_id":    r.get("chunk_id"),
                    "metadata": {
                        "title":      doc.get("title")   or "Unknown Title",
                        "section":    "Unknown Section",
                        "authors":    authors            or "Unknown Authors",
                        "journal":    doc.get("journal") or "Unknown Journal",
                        "year":       doc.get("year")    or "Unknown",
                        "volume":     doc.get("volume"),
                        "issue":      doc.get("issue"),
                        "doi":        doc.get("doi"),
                        "page_range": page_range,
                    },
                }
            )

        answer = generate_rag_response(query, results) or ""
        return jsonify({"results": sources, "response": answer})
    except Exception as e:
        logging.exception("Search failed")
        return jsonify({"error": str(e)}), 500


@app.route("/stats")
def stats():
    try:
        total_chunks = (
            sb.table("research_chunks")
              .select("id", count="exact")
              .execute()
              .count
            or 0
        )
        return jsonify({"total_chunks": total_chunks}), 200
    except Exception as e:
        logging.exception("Stats endpoint failed")
        return jsonify({"error": str(e)}), 500


# ────────────────────────────── bootstrap html ───────────────────────────── #

if __name__ == "__main__":
    tpl_dir   = os.path.join(os.path.dirname(__file__), "templates")
    os.makedirs(tpl_dir, exist_ok=True)
    index_path = os.path.join(tpl_dir, "index.html")

    if not os.path.exists(index_path):
        with open(index_path, "w") as fp:
            fp.write("""<!DOCTYPE html>
<!-- identical template as previous version omitted for brevity -->""")

    log.info("Starting Flask on 0.0.0.0:8080 …")
    app.run(host="0.0.0.0", port=8080, debug=True)


================================================================================


################################################################################
# File: scripts/resilient_batch_embedding_generator_gian_v1.1.py
################################################################################

# File: scripts/resilient_batch_embedding_generator_gian_v1.1.py

#!/usr/bin/env python3
###############################################################################
###############################################################################
"""
Resilient Batch Embedding Generator — Supabase edition (token‑safe v3‑fix)
==========================================================================

• Pulls rows from `research_chunks` whose `embedding` is NULL
  **and** `chunking_strategy = 'token_window'`.
• Uses the *actual* token count stored in each row, so even weird
  `/C111/C102/…` glyph dumps are handled correctly.
• Sends slices that respect both:
      – a *row* cutoff (`--commit`, default 100) and
      – the model's 8 192‑token limit (we stop at ≤ 8 000).
• UPSERTs vectors back to Supabase and writes a JSON run‑report.

Environment
-----------
SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY, OPENAI_API_KEY  must be set.
"""

from __future__ import annotations

import argparse
import json
import logging
import os
import sys
import time
from datetime import datetime
from typing import Any, Dict, List

from dotenv import load_dotenv
from openai import OpenAI
from supabase import create_client
from tqdm import tqdm

# ───────────────────────────── configuration ────────────────────────────── #

load_dotenv()

SUPABASE_URL  = os.getenv("SUPABASE_URL")
SUPABASE_KEY  = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

EMBEDDING_MODEL  = "text-embedding-ada-002"

MODEL_TOKEN_LIMIT = 8_192         # model hard limit
TOKEN_GUARD       = 200           # safety margin
MAX_TOTAL_TOKENS  = MODEL_TOKEN_LIMIT - TOKEN_GUARD   # 8 000

DEFAULT_BATCH_ROWS  = 5_000       # rows fetched from Supabase per outer loop
DEFAULT_COMMIT_ROWS = 100         # max rows per single OpenAI request

MAX_RETRIES = 5
RETRY_DELAY = 2                   # seconds between retries

openai_client = OpenAI(api_key=OPENAI_API_KEY)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s",
)
log = logging.getLogger(__name__)

# ───────────────────────────── Supabase helpers ─────────────────────────── #

def init_supabase():
    if not SUPABASE_URL or not SUPABASE_KEY:
        log.error("SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY not set.")
        sys.exit(1)
    return create_client(SUPABASE_URL, SUPABASE_KEY)


def count_processed_chunks(sb) -> int:
    res = (
        sb.table("research_chunks")
        .select("id", count="exact")
        .not_.is_("embedding", "null")
        .execute()
    )
    return res.count or 0


def fetch_unprocessed_chunks(
    sb, *, limit: int, offset: int = 0
) -> List[Dict[str, Any]]:
    """
    Grab up to *limit* rows whose embedding IS NULL.
    We also fetch token_start/end so we can trust the real token length.
    """
    first, last = offset, offset + limit - 1          # PostgREST uses inclusive range
    res = (
        sb.table("research_chunks")
        .select("id,text,token_start,token_end")
        .eq("chunking_strategy", "token_window")
        .is_("embedding", "null")
        .range(first, last)
        .execute()
    )
    return res.data or []

# ───────────────────────────  OpenAI interaction  ───────────────────────── #

def generate_embedding_batch(texts: List[str]) -> List[List[float]]:
    attempt = 0
    while attempt < MAX_RETRIES:
        try:
            resp = openai_client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=texts,
            )
            return [d.embedding for d in resp.data]
        except Exception as exc:
            attempt += 1
            log.warning("Embedding batch %s/%s failed: %s",
                        attempt, MAX_RETRIES, exc)
            time.sleep(RETRY_DELAY)
    raise RuntimeError("OpenAI embedding batch failed after retries")

# ───────────────────────── token‑safe slicing helpers ────────────────────── #

def chunk_tokens(row: Dict[str, Any]) -> int:
    """
    Return the best‑guess token length for a chunk.

    Priority:
    1.  Use token_end – token_start + 1  *if present and plausible*.
        (Some PDFs have token_end values that balloon into 80‑90 k range;
         we treat anything 0 < t ≤ 16 384 as plausible.)
    2.  Fallback: estimate by words × 0.75, which is roughly the OpenAI
        tokenizer ratio for English prose.  Cap to the model limit so we
        never falsely mark a chunk as oversize.
    """
    try:
        t = int(row["token_end"]) - int(row["token_start"]) + 1
        if 0 < t <= 16_384:        # sane bound (2× GPT‑4o context)
            return t
    except Exception:
        pass

    txt = row.get("text", "") or ""
    approx = int(len(txt.split()) * 0.75) + 1
    return min(max(1, approx), MODEL_TOKEN_LIMIT)   # never exceed limit


def safe_slices(
    rows: List[Dict[str, Any]],
    max_rows: int,
) -> List[List[Dict[str, Any]]]:
    """
    Break *rows* into slices obeying BOTH limits:
        • len(slice) ≤ max_rows
        • Σ tokens(slice) ≤ MAX_TOTAL_TOKENS
    """
    slices: List[List[Dict[str, Any]]] = []
    current: List[Dict[str, Any]] = []
    cur_tokens = 0

    for r in rows:
        txt = (r.get("text") or "").replace("\x00", "")
        if not txt.strip():
            continue

        t = chunk_tokens(r)

        if t > MAX_TOTAL_TOKENS:
            log.warning("Chunk %s is > %s tokens — skipping.", r["id"], MAX_TOTAL_TOKENS)
            continue

        need_new_slice = (
            len(current) >= max_rows or
            cur_tokens + t > MAX_TOTAL_TOKENS
        )
        if need_new_slice and current:
            slices.append(current)
            current = []
            cur_tokens = 0

        current.append({"id": r["id"], "text": txt})
        cur_tokens += t

    if current:
        slices.append(current)
    return slices


def embed_slice(sb, slice_rows: List[Dict[str, Any]]) -> int:
    """Generate embeddings for *slice_rows* and UPSERT them back. Returns #rows updated."""
    embeds = generate_embedding_batch([r["text"] for r in slice_rows])
    ok = 0
    for row, emb in zip(slice_rows, embeds):
        attempt = 0
        while attempt < MAX_RETRIES:
            res = (
                sb.table("research_chunks")
                .update({"embedding": emb})
                .eq("id", row["id"])
                .execute()
            )
            if getattr(res, "error", None):
                attempt += 1
                log.warning("Update %s failed (%s/%s): %s",
                            row["id"], attempt, MAX_RETRIES, res.error)
                time.sleep(RETRY_DELAY)
            else:
                ok += 1
                break
    return ok

# ────────────────────────────────── main ────────────────────────────────── #

def main() -> None:
    ap = argparse.ArgumentParser(
        description="Embed research_chunks where embedding IS NULL (token‑safe)"
    )
    ap.add_argument("--batch-size", type=int, default=DEFAULT_BATCH_ROWS,
                    help="rows fetched from Supabase per outer loop (default 5000)")
    ap.add_argument("--commit", type=int, default=DEFAULT_COMMIT_ROWS,
                    help="row‑cutoff per single OpenAI request (default 100)")
    ap.add_argument("--skip", type=int, default=0,
                    help="row offset for pagination / parallel runs")
    ap.add_argument("--model", default=EMBEDDING_MODEL,
                    help="override embedding model name")
    ap.add_argument("--model-limit", type=int, default=MODEL_TOKEN_LIMIT,
                    help="override model token limit (hard max context)")
    args = ap.parse_args()

    # allow quick overrides without editing the file
    global EMBEDDING_MODEL, MODEL_TOKEN_LIMIT, MAX_TOTAL_TOKENS
    EMBEDDING_MODEL   = args.model
    MODEL_TOKEN_LIMIT = args.model_limit
    MAX_TOTAL_TOKENS  = MODEL_TOKEN_LIMIT - TOKEN_GUARD

    if not OPENAI_API_KEY:
        log.error("OPENAI_API_KEY not set.")
        sys.exit(1)

    sb = init_supabase()
    log.info("Rows already embedded: %s", count_processed_chunks(sb))

    loop = 0
    total_embedded = 0
    while True:
        loop += 1
        rows = fetch_unprocessed_chunks(sb, limit=args.batch_size, offset=args.skip)
        if not rows:
            log.info("✨  Done — no more rows needing embeddings.")
            break

        log.info("Loop %s — fetched %s rows needing embeddings.", loop, len(rows))
        slices = safe_slices(rows, args.commit)
        log.info("Created %s token‑safe OpenAI requests.", len(slices))

        for sl in tqdm(slices, desc=f"Embedding loop {loop}", unit="batch"):
            try:
                total_embedded += embed_slice(sb, sl)
            except Exception as exc:
                log.error("Slice failed: %s", exc)

        log.info("Loop %s complete – total embedded so far: %s", loop, total_embedded)

    # -------------  JSON report  ------------- #
    ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    report = {
        "timestamp": ts,
        "batch_size": args.batch_size,
        "commit": args.commit,
        "skip": args.skip,
        "total_embedded": total_embedded,
        "total_with_embeddings": count_processed_chunks(sb),
    }
    fname = f"supabase_embedding_report_{ts}.json"
    with open(fname, "w") as fp:
        json.dump(report, fp, indent=2)
    log.info("Report saved to %s", fname)


if __name__ == "__main__":        # pragma: no cover
    main()


================================================================================


################################################################################
# File: scripts/test_vector_search.py
################################################################################

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Vector‑search smoke‑test (Supabase edition)
==========================================

• Firebase/Firestore has been removed — every data call now goes through
  Supabase's PostgREST API.

• The script calls a SQL helper function that must exist on your database:
    public.search_research_chunks(query_text TEXT,
                                  match_count INT DEFAULT 10,
                                  similarity_threshold REAL DEFAULT 0.6)
  which should:
    1. embed the incoming `query_text`
    2. invoke your `match_documents` similarity function
    3. return the top‑`match_count` rows as
       (id UUID, text TEXT, metadata JSONB, similarity REAL)

  See README / earlier instructions for a ready‑made implementation.

Environment variables required
------------------------------
SUPABASE_URL                 – e.g. https://xxxx.supabase.co
SUPABASE_SERVICE_ROLE_KEY    – or an anon key if RLS permits the RPC
OPENAI_API_KEY               – only if your SQL helper embeds via an HTTP call
"""

from __future__ import annotations

import os
import sys
import time
from typing import Any, Dict, List

from dotenv import load_dotenv
from supabase import create_client
from openai import OpenAI

# ──────────────────────── configuration & sanity checks ───────────────────── #

load_dotenv()

# ---------- Supabase config ----------
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")   # or anon if RLS permits
sb = create_client(SUPABASE_URL, SUPABASE_KEY)

if not SUPABASE_URL or not SUPABASE_KEY:
    sys.exit(
        "❌  SUPABASE_URL / SUPABASE_SERVICE_ROLE_KEY env vars are missing.\n"
        "    export them and rerun."
    )

# Sample questions to probe the index
SAMPLE_QUERIES: List[str] = [
    "What are the symptoms of misophonia?",
    "How prevalent is misophonia in university students?",
    "What is the relationship between misophonia and hyperacusis?",
    "What treatments are effective for misophonia?",
    "How does misophonia affect quality of life?",
]

# Add this after other configuration
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def embed(text: str) -> List[float]:
    """Return OpenAI ada‑002 embedding (1536‑dim list of floats)."""
    resp = openai_client.embeddings.create(
        model="text-embedding-ada-002",
        input=text,
    )
    return resp.data[0].embedding

# ─────────────────────────── helper functions ─────────────────────────────── #


def perform_vector_search(query_vec, top_k=5, thresh=0.6):
    """
    Call the SQL RPC we just created.
    `query_vec` is a list[float] length 1536 coming from OpenAI.
    """
    try:
        resp = sb.rpc(
            "search_research_chunks",
            {
                "query_embedding": query_vec,
                "match_count": top_k,
                "similarity_threshold": thresh,
            },
        ).execute()
        if getattr(resp, "error", None):
            raise RuntimeError(resp.error)
        return resp.data or []
    except Exception as e:
        print(f"   ⚠  RPC failed: {e}")
        return []


def print_results(rows: List[Dict[str, Any]]) -> None:
    """
    Nicely format the search results.
    """
    if not rows:
        print("   (no matches)\n")
        return

    for idx, row in enumerate(rows, 1):
        meta = row.get("metadata", {}) or {}
        title = meta.get("title", "Unknown title")
        year = meta.get("year", "????")
        author = meta.get("primary_author", "Unknown author")
        sim = row.get("similarity", 0.0)

        snippet = (row.get("text", "") or "").replace("\n", " ")[:280] + "…"

        print(f"\nResult {idx}  •  sim={sim:.3f}")
        print(f"  {title} — {author} ({year})")
        print(f"  {snippet}")


# ────────────────────────────────── main ──────────────────────────────────── #


def main() -> None:
    print("\n🔍  Supabase vector search smoke‑test\n" + "—" * 60)
    for i, q in enumerate(SAMPLE_QUERIES, 1):
        print(f'\nQuery {i + 1}/{len(SAMPLE_QUERIES)}: "{q}"')


        # 1. get the vector
        print("Generating embedding...")
        query_vec = embed(q)  # Python list[float]

        # 2. PostgREST / Postgres expects a *string* like: [0.1,0.2,…]
        vec_literal = "[" + ",".join(f"{x:.6f}" for x in query_vec) + "]"

        # 3. call the RPC
        print("Performing vector search...")
        resp = sb.rpc(
            "search_research_chunks",
            {
                "query_embedding": vec_literal,  # <- NOT the raw text
                "match_count": 5,
                "similarity_threshold": 0.6,
            },
        ).execute()
        
        if getattr(resp, "error", None):
            print(f"   ⚠  RPC failed: {resp.error}\n")
            continue
            
        results = resp.data or []
        print_results(results)

        if i < len(SAMPLE_QUERIES):
            print("\nPausing 2 s before the next query …")
            time.sleep(2)

    print("\n✔  Done")


if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: README.md
################################################################################

################################################################################
################################################################################

<!-- PROJECT LOGO -->
<p align="center">
  <img src="public/vite.svg" alt="Logo" width="120" height="120">

</p>

<h1 align="center">Misophonia Companion</h1>

<p align="center">
  <b>The modern, AI-powered guide and support tool for those living with misophonia.</b><br>
  <i>Built with React, Vite, Node.js, and OpenAI</i>
  <br><br>
  <a href="https://flourishing-sprite-c819cb.netlify.app/"><img src="https://img.shields.io/badge/Live%20Demo-Online-brightgreen?style=for-the-badge" alt="Live Demo"></a>
  <a href="https://github.com/mannino49/Misophonia-companion-v2"><img src="https://img.shields.io/github/stars/mannino49/Misophonia-companion-v2?style=for-the-badge" alt="GitHub Stars"></a>
</p>

---

## 🚀 Features

- **Conversational AI Chatbot:** Powered by OpenAI, get real-time support and information.
- **Soundscape Player:** Customizable soundscapes to help manage triggers.
- **Modern UI:** Responsive, accessible, and visually appealing interface.
- **Progressive Web App:** Installable and works offline.
- **Secure Backend:** All API keys and secrets are kept on the server, never exposed to the client.

---

## 🖥️ Tech Stack

<div align="center">
  <img src="https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB" />
  <img src="https://img.shields.io/badge/Vite-646CFF?style=for-the-badge&logo=vite&logoColor=FFD62E" />
  <img src="https://img.shields.io/badge/Node.js-339933?style=for-the-badge&logo=nodedotjs&logoColor=white" />
  <img src="https://img.shields.io/badge/Express-000000?style=for-the-badge&logo=express&logoColor=white" />
  <img src="https://img.shields.io/badge/OpenAI-412991?style=for-the-badge&logo=openai&logoColor=white" />
  <img src="https://img.shields.io/badge/Netlify-00C7B7?style=for-the-badge&logo=netlify&logoColor=white" />
</div>

---

## 📦 Project Structure

```shell
Misophonia Guide/
├── public/                # Static assets (icons, manifest)
├── src/                   # React frontend source
│   ├── App.jsx            # Main app logic
│   ├── main.jsx           # React entry point
│   └── ...
├── server/                # Node.js/Express backend
│   ├── index.js           # API server entry
│   └── ...
├── netlify.toml           # Netlify deployment config
├── package.json           # Frontend config
└── ...
```

---

## ⚡ Getting Started

### 1. Clone the repository
```bash
git clone https://github.com/g-troiani/misophonia-companion-v3
cd Misophonia-companion-v3
```

### 2. Install dependencies
```bash
npm install
cd server && npm install
```

### 3. Set up environment variables
- Copy `.env.example` to `.env` in the `server/` directory and add your OpenAI API key:
```
OPENAI_API_KEY=your_openai_key_here
```

### 4. Run the backend server
```bash
cd server
npm start
```

### 5. Run the frontend (in a new terminal)
```bash
npm run dev
```

- Frontend: [http://localhost:5173](http://localhost:5173)
- Backend API: [http://localhost:3001](http://localhost:3001)

---

## 🌐 Deployment

- Deployed on Netlify: [Live Demo](https://flourishing-sprite-c819cb.netlify.app/)
- Backend runs as a separate Node.js server (see `server/`)
- All secrets are stored in environment variables and never exposed to the frontend.

---

## 🛡️ Security & Best Practices

- **No secrets or API keys are stored in the frontend.**
- **.env files and private keys are gitignored.**
- **Backend validates API key presence and never exposes it to the client.**

---

## 🤝 Contributing

Contributions are welcome! Please open issues or submit pull requests.

---

## 📄 License

MIT License. See [LICENSE](LICENSE) for details.

---

<p align="center">
  <b>Made with ❤️ by Mannino49</b>
</p>


================================================================================


################################################################################
# File: src/RagAssistant.jsx
################################################################################

// File: src/RagAssistant.jsx

import React, { useState } from 'react';
import { marked } from 'marked';

// ⬇️  NEW DEFAULT:  falls back to “/api/rag” in production/PWA
const API = import.meta.env.VITE_RAG_ENDPOINT || '/api/rag';

export default function RagAssistant() {
  const [q, setQ] = useState('');
  const [loading, setLoading] = useState(false);
  const [answer, setAnswer] = useState('');
  const [results, setResults] = useState([]);

  const ask = async e => {
    e.preventDefault();
    if (!q.trim()) return;
    setLoading(true);

    const r = await fetch(API, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ query: q, limit: 8 })
    });

    const d = await r.json();
    setAnswer(d.answer ? marked.parse(d.answer) : 'No answer.');
    setResults(d.results || []);
    setLoading(false);
  };

  return (
    <div className="ra-card">
      <h2 style={{marginBottom: '1rem'}}>Research Assistant</h2>
      
      <div className="ra-body">
        {answer && (
          <div
            dangerouslySetInnerHTML={{ __html: answer }}
            style={{ marginBottom: '1.5rem' }}
          />
        )}

        {loading && <p>Searching…</p>}

        {results.map((r, i) => (
          <details key={i} style={{marginBottom: '1rem'}}>
            <summary>
              <strong>[{i + 1}] {r.doc?.title || 'Untitled'}</strong>
              <span style={{float: 'right', fontSize: '.85rem', opacity: .7}}>
                {r.similarity.toFixed(1)} %
              </span>
            </summary>
            <blockquote style={{margin: '.75rem 0'}}>{r.text}</blockquote>
            <p style={{fontSize: '.85rem', opacity: .8}}>
              {(r.doc?.authors || []).join(', ') || 'Unknown authors'} · {r.doc?.journal || ''} ({r.doc?.year || 'n.d.'}) · pp. {r.page_start}-{r.page_end}
            </p>
            {r.doc?.doi && (
              <a href={`https://doi.org/${r.doc.doi}`} target="_blank" rel="noreferrer">doi:{r.doc.doi}</a>
            )}
          </details>
        ))}
      </div>

      <form className="ra-input-row" onSubmit={ask}>
        <input
          className="ra-input"
          value={q}
          onChange={e => setQ(e.target.value)}
          placeholder="Ask a research question…"
        />
        <button className="ra-btn" disabled={loading}>Search</button>
      </form>
    </div>
  );
}


================================================================================


################################################################################
# File: netlify/functions/search.js
################################################################################

import { createClient } from '@supabase/supabase-js';
import { Configuration, OpenAIApi } from 'openai';

export async function handler(event, context) {
  if (event.httpMethod !== 'POST') {
    return { statusCode: 405, body: 'Method Not Allowed' };
  }
  
  try {
    const body = JSON.parse(event.body);
    const { query, filters, expandContext, page, pageSize, similarityThreshold } = body;
    
    // Initialize Supabase client
    const supabase = createClient(
      process.env.SUPABASE_URL,
      process.env.SUPABASE_SERVICE_ROLE_KEY
    );
    
    // Initialize OpenAI for embeddings
    const configuration = new Configuration({
      apiKey: process.env.OPENAI_API_KEY,
    });
    const openai = new OpenAIApi(configuration);
    
    // Generate embedding
    const embeddingResponse = await openai.createEmbedding({
      model: "text-embedding-ada-002",
      input: query,
    });
    
    const queryEmbedding = embeddingResponse.data.data[0].embedding;
    
    // Search using the RPC function
    const { data: results, error } = await supabase.rpc(
      'match_documents',
      {
        query_embedding: queryEmbedding,
        match_threshold: similarityThreshold || 0.6,
        match_count: pageSize || 10
      }
    );
    
    if (error) throw error;
    
    return {
      statusCode: 200,
      body: JSON.stringify({
        result: {
          results,
          page: page || 1,
          pageSize: pageSize || 10,
          query
        }
      })
    };
  } catch (error) {
    console.error('Error in search function:', error);
    return {
      statusCode: 500,
      body: JSON.stringify({ error: error.message })
    };
  }
}


================================================================================


################################################################################
# File: netlify/functions/rag.js
################################################################################

// File: netlify/functions/rag.js

/**
 * Proxy to the Python RAG service (Flask) OR any upstream REST endpoint.
 * POST /.netlify/functions/rag    →  { query, limit? }
 *
 * Required ENV:
 *   RAG_HOST               e.g. https://misophonia-rag.fly.dev  (no trailing /)
 *
 * Optional:
 *   RAG_TIMEOUT_MS         default 30000
 */
import 'dotenv/config';

const RAG_HOST = process.env.RAG_HOST ?? 'http://localhost:8080';
const TIMEOUT  = Number(process.env.RAG_TIMEOUT_MS ?? 30_000);

export async function handler(event /* , context */) {
  if (event.httpMethod !== 'POST') {
    return { statusCode: 405, body: 'Method Not Allowed' };
  }

  try {
    const upstream = await fetch(`${RAG_HOST}/search`, {
      method: 'POST',
      headers: { 'content-type': 'application/json' },
      body:     event.body,
      signal:   AbortSignal.timeout(TIMEOUT),
    });

    const text = await upstream.text();     // pass through raw

    return {
      statusCode: upstream.status,
      headers: { 'content-type': 'application/json' },
      body: text,
    };
  } catch (err) {
    console.error('RAG proxy error →', err);
    return {
      statusCode: 500,
      headers: { 'content-type': 'application/json' },
      body: JSON.stringify({ error: 'Error contacting RAG service' }),
    };
  }
}


================================================================================


################################################################################
# File: eslint.config.js
################################################################################

import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'

export default [
  { ignores: ['dist'] },
  {
    files: ['**/*.{js,jsx}'],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
      parserOptions: {
        ecmaVersion: 'latest',
        ecmaFeatures: { jsx: true },
        sourceType: 'module',
      },
    },
    plugins: {
      'react-hooks': reactHooks,
      'react-refresh': reactRefresh,
    },
    rules: {
      ...js.configs.recommended.rules,
      ...reactHooks.configs.recommended.rules,
      'no-unused-vars': ['error', { varsIgnorePattern: '^[A-Z_]' }],
      'react-refresh/only-export-components': [
        'warn',
        { allowConstantExport: true },
      ],
    },
  },
]


================================================================================


################################################################################
# File: requirements.txt
################################################################################

################################################################################
################################################################################
# Python dependencies for Misophonia Research RAG System

# Core dependencies
firebase-admin>=6.2.0
openai>=1.0.0
numpy>=1.24.0
python-dotenv>=1.0.0
flask>=2.0.0
requests>=2.28.0

# PDF processing
PyPDF2>=3.0.0
unstructured>=0.10.0
pdfminer.six>=20221105

# Utilities
tqdm>=4.65.0
tabulate>=0.9.0
colorama>=0.4.6
argparse>=1.4.0

# Firebase/Google Cloud
google-cloud-firestore>=2.11.0

# For concurrent processing
concurrent-log-handler>=0.9.20

# Supabase integration
supabase~=2.0.0


================================================================================


################################################################################
# File: server/package.json
################################################################################

{
  "name": "misophonia-companion-server",
  "version": "1.0.0",
  "main": "index.js",
  "type": "module",
  "scripts": {
    "start": "node index.js"
  },
  "dependencies": {
    "cors": "^2.8.5",
    "dotenv": "^16.3.1",
    "express": "^4.18.2",
    "openai": "^4.27.0",
    "@supabase/supabase-js": "^2.39.0"
  }
}


================================================================================


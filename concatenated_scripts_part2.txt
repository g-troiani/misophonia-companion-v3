# Concatenated Project Code - Part 2 of 3
# Generated: 2025-05-09 11:36:56
# Root Directory: /Users/gianmariatroiani/Documents/misophonia-companion-v3
================================================================================

################################################################################
# File: scripts/templates/rag_web_interface_gian_v1 copy.py
################################################################################

# File: scripts/templates/rag_web_interface_gian_v1 copy.py

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Interactive RAG demo (Flask) that queries a Supabase vector store
(`research_chunks`) instead of Firebase / Firestore.

Changes in R2 ­­­→  ▸ removes the /stats endpoint + front‑end fetch  
                   ▸ cleaner citation helpers (no "Unknown" placeholders)
"""

from __future__ import annotations

import json
import logging
import os
import sys
from typing import Any, Dict, List, Sequence

from dotenv import load_dotenv
from flask import Flask, jsonify, render_template, request
from openai import OpenAI
from supabase import create_client


# ───────────────────────────── configuration ───────────────────────────── #

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUPABASE_URL   = os.getenv("SUPABASE_URL")
SUPABASE_KEY   = os.getenv("SUPABASE_SERVICE_ROLE_KEY")


# ------------------------------------------------------------------
# Bibliographic fields we want to pull from research_documents
DOC_FIELDS = (
    "title, authors, year, journal, doi"
)

# All columns we need from research_chunks  +  the join
CHUNK_SELECT = (
    "text, page_start, page_end, "
    f"research_documents({DOC_FIELDS})"
)
# ------------------------------------------------------------------

if not OPENAI_API_KEY:
    print("⛔  OPENAI_API_KEY missing — aborting")
    sys.exit(1)

client = OpenAI(api_key=OPENAI_API_KEY)

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s — %(levelname)s — %(message)s"
)
log = logging.getLogger(__name__)

app = Flask(__name__)


# ───────────────────────────── Supabase helpers ─────────────────────────── #

def init_supabase():
    if not SUPABASE_URL or not SUPABASE_KEY:
        log.error("SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY not set.")
        sys.exit(1)
    return create_client(SUPABASE_URL, SUPABASE_KEY)


sb = init_supabase()        # single client reused everywhere


# ──────────────────────── metadata / citation helpers ────────────────────── #

def _first(meta: Dict[str, Any], keys: Sequence[str]):
    """Return first non‑empty, non‑placeholder value in *meta* for any of *keys*."""
    for k in keys:
        v = meta.get(k)
        if v and str(v).strip() and not str(v).lower().startswith("unknown"):
            return v
    return None


def _first_author(meta: Dict[str, Any]) -> str | None:
    if (a := _first(meta, ["primary_author", "author", "creator"])):
        return str(a)
    if (lst := meta.get("authors")):
        if isinstance(lst, list) and lst:
            return str(lst[0])
        # if it's a string "Foo, Bar" take first token
        if isinstance(lst, str) and lst.strip() and not lst.lower().startswith("unknown"):
            return lst.split(",")[0].strip()
    return None


def _page(meta: Dict[str, Any]) -> str | None:
    if (p := _first(meta, ["page", "page_number"])):
        return str(p)
    if (rng := _first(meta, ["pages", "page_range"])):
        # Accept list or "12–15" / "12-15"
        if isinstance(rng, list) and rng:
            return str(rng[0])
        if isinstance(rng, str):
            return rng.split("–")[0].split("-")[0]
    return None


def make_citation(meta: Dict[str, Any]) -> str:
    """
    Compact citation for UI cards – omits any empty element.
    Format:  Author Year – Title. Journal. doi:… p.X
    """
    parts: list[str] = []

    if (a := _first_author(meta)):
        parts.append(a)

    if (y := _first(meta, ["year", "pub_year"])):
        parts.append(str(y))

    if (t := _first(meta, ["title", "document_title", "section"])):
        parts.append(f"– {t.strip()}")

    if (j := _first(meta, ["journal", "source"])):
        parts.append(j)

    if (doi := _first(meta, ["doi"])):
        parts.append(f"doi:{doi}")

    if (pg := _page(meta)):
        parts.append(f"p.{pg}")

    return " ".join(parts) if parts else "No citation metadata"


def long_citation(meta: Dict[str, Any]) -> str:
    """
    Longer citation fed to GPT (always includes Title if available).
    """
    auth  = _first_author(meta) or ""
    year  = _first(meta, ["year", "pub_year"]) or ""
    title = _first(meta, ["title", "document_title", "section"]) or ""
    jour  = _first(meta, ["journal", "source"]) or ""
    doi   = _first(meta, ["doi"]) or ""
    pg    = _page(meta) or ""

    bits = []
    if auth or year:
        bits.append(f"{auth} ({year}).".strip())
    if title:
        bits.append(title)
    if jour:
        bits.append(jour)
    if doi:
        bits.append(f"doi:{doi}")
    if pg:
        bits.append(f"p.{pg}")

    return " ".join(bits)


# keep alias for any legacy code
pretty_source = make_citation


# ────────────────────────────── embedding helpers ────────────────────────── #

def generate_embedding(text: str) -> List[float] | None:
    """Generate an Ada‑002 embedding for *text*."""
    try:
        resp = client.embeddings.create(model="text-embedding-ada-002", input=text)
        return resp.data[0].embedding
    except Exception as exc:                                  # pragma: no cover
        log.error("Embedding failed: %s", exc)
        return None


# ─────────────────────────────── search logic ────────────────────────────── #

def semantic_search(query: str, limit: int = 5) -> List[Dict[str, Any]]:
    """
    Server‑side vector search via RPC.
    """
    vec = generate_embedding(query)
    if vec is None:
        return []

    try:
        resp = (
            sb.rpc(
                "search_research_chunks",
                {
                    "query_embedding": vec,
                    "match_count": limit,
                    "similarity_threshold": 0.6,
                },
            ).execute()
        )
        rows = resp.data or []
        for r in rows:
            r["similarity"] = float(r.get("similarity", 0.0))
            r["match_type"] = f"Semantic ({r['similarity']:.4f})"
            r["chunk_id"]   = r.pop("id", "unknown")
            r["source"]     = make_citation(r.get("metadata", {}))
        return rows
    except Exception as exc:
        log.error("RPC search failed: %s", exc)
        return []


def keyword_search(query: str, limit: int = 5) -> List[Dict[str, Any]]:
    """
    Simple client‑side keyword fallback over a capped set of chunks.
    """
    try:
        resp = (
            sb.table("research_chunks")
            .select(CHUNK_SELECT)
            .limit(2_000)
            .execute()
        )
        all_chunks = resp.data or []
    except Exception as exc:
        log.error("Keyword search fetch failed: %s", exc)
        return []

    stop  = {"the","and","or","in","of","to","a","is","that","for","on","with"}
    terms = {t for t in query.lower().split() if t not in stop}
    if not terms:
        return []

    results: List[Dict[str, Any]] = []
    for ch in all_chunks:
        txt = str(ch.get("text", "")).lower()
        if not txt:
            continue

        matches = sum(1 for t in terms if t in txt)
        if matches == 0:
            continue

        score = matches / len(terms)
        if query.lower() in txt:
            score += 0.3
        if score < 0.3:
            continue

        meta = ch.get("metadata") or {}
        results.append(
            {
                "chunk_id": meta.get("chunk_id", "unknown"),
                "text": ch.get("text", ""),
                "metadata": meta,
                "similarity": float(score),
                "match_type": f"Keyword ({score:.4f})",
                "source": make_citation(meta),
            }
        )

    results.sort(key=lambda r: r["similarity"], reverse=True)
    return results[:limit]


def comprehensive_search(query: str, limit: int = 5) -> List[Dict[str, Any]]:
    sem = semantic_search(query, limit)
    kw  = keyword_search(query, limit)

    combined: List[Dict[str, Any]] = []
    seen = set()

    for r in sem + kw:
        cid = r.get("chunk_id")
        if cid not in seen:
            combined.append(r)
            seen.add(cid)

    combined.sort(key=lambda r: r["similarity"], reverse=True)
    return combined[:limit]


# ──────────────────────────── RAG generation ─────────────────────────────── #

def generate_rag_response(
    query: str,
    docs: List[Dict[str, Any]],
    max_tokens: int = 500,
) -> str:
    """Produce the final GPT‑4o answer with inline citations."""
    try:
        snippets = [
            f"Source {i} — {long_citation(d.get('metadata', {}))}\n{d.get('text','')}\n"
            for i, d in enumerate(docs, 1)
        ]

        prompt = f"""You are a helpful AI assistant specialised in misophonia research.

Answer the question below using *only* the provided research snippets.

Question: "{query}"

Snippets:
{''.join(snippets)}

Give a concise, structured answer with numbered citations like [1], [2] … .
If evidence is insufficient, say so explicitly."""
        comp = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "You answer misophonia questions strictly from the provided sources, citing them.",
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=max_tokens,
        )
        return comp.choices[0].message.content
    except Exception as exc:                                                  # pragma: no cover
        log.error("RAG generation failed: %s", exc)
        return f"Error generating response: {exc}"


# ───────────────────────────── Flask endpoints ───────────────────────────── #

@app.route("/")
def index():
    return render_template("index.html")


@app.route("/search", methods=["POST"])
def search():
    try:
        data = request.get_json(force=True, silent=True)
        if not data or not data.get("query"):
            return jsonify({"error": "No query provided"}), 400

        query = data["query"].strip()
        limit = int(data.get("limit", 5))

        results = comprehensive_search(query, limit)

        if not results:
            return jsonify(
                {"error": "No relevant documents found for your query", "results": []}
            ), 404

        # After you get results from the database:
        sources = []

        for r in results:
            # PostgREST nests the 1‑to‑1 join as an OBJECT, not a list
            doc = r.get("research_documents") or {}

            # authors is a Postgres text[] → Python list; stringify for HTML
            authors = ", ".join(doc["authors"]) if isinstance(doc.get("authors"), list) else doc.get("authors")

            # page range: prefer the 'pages' column; otherwise build it
            page_range = (
                doc.get("pages")
                or f"{r.get('page_start', '?')}-{r.get('page_end', '?')}"
            )

            sources.append(
                {
                    "chunk"      : r.get("text", ""),         # the snippet
                    "title"      : doc.get("title")   or "Unknown Title",
                    "section"    : "Unknown Section",    # add later if you store it
                    "authors"    : authors            or "Unknown Authors",
                    "journal"    : doc.get("journal") or "Unknown Journal",
                    "year"       : doc.get("year")    or "Unknown",
                    "volume"     : doc.get("volume"),
                    "issue"      : doc.get("issue"),
                    "doi"        : doc.get("doi"),
                    "page_range" : page_range,
                }
            )

        answer = generate_rag_response(query, results)
        return jsonify({"results": sources, "response": answer})
    except Exception as e:
        logging.exception("Search failed")
        return jsonify({"error": str(e)}), 500


@app.route("/stats")
def stats():
    try:
        # 1) total rows in the vector table ----------------------------
        total_chunks = (
            sb.table("research_chunks")
              .select("id", count="exact")
              .execute()
              .count
            or 0
        )

        return jsonify({"total_chunks": total_chunks}), 200

    except Exception as e:
        logging.exception("Stats endpoint failed")
        return jsonify({"error": str(e)}), 500


# ────────────────────────────── bootstrap html ───────────────────────────── #

if __name__ == "__main__":
    tpl_dir   = os.path.join(os.path.dirname(__file__), "templates")
    os.makedirs(tpl_dir, exist_ok=True)
    index_path = os.path.join(tpl_dir, "index.html")

    if not os.path.exists(index_path):
        with open(index_path, "w") as fp:
            fp.write(
"""<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Misophonia Research RAG Interface</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
  <style>
    body{padding:20px;background:#f8f9fa}
    .container{max-width:1000px;margin:0 auto}
    .header{text-align:center;margin-bottom:30px}
    .search-box{margin-bottom:20px}
    .results-container{margin-top:20px}
    .result-card{margin-bottom:15px;border-radius:8px;box-shadow:0 2px 5px rgba(0,0,0,.1)}
    .result-card .card-header{font-weight:bold;display:flex;justify-content:space-between}
    .loading{text-align:center;padding:20px;display:none}
    .response-container{margin-top:30px;padding:20px;background:#fff;border-radius:8px;box-shadow:0 2px 5px rgba(0,0,0,.1)}
    pre { background:#f8f9fa;border:0; }
  </style>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>Misophonia Research RAG Interface</h1>
      <p class="text-muted">Search across research documents with semantic and keyword retrieval</p>
    </div>

    <div class="search-box">
      <div class="input-group mb-3">
        <input type="text" id="search-input" class="form-control form-control-lg"
               placeholder="Ask a question about misophonia…" aria-label="Search query">
        <button class="btn btn-primary" type="button" id="search-button">Search</button>
      </div>
      <div class="form-text">Try questions about treatments, neurological basis, symptoms, or coping strategies</div>
    </div>

    <div class="loading" id="loading">
      <div class="spinner-border text-primary" role="status">
        <span class="visually-hidden">Loading…</span>
      </div>
      <p>Searching research documents and generating response…</p>
    </div>

    <div id="response-area" style="display:none">
      <div class="response-container">
        <h3>Research‑Based Answer</h3>
        <div id="response-content"></div>
      </div>

      <div class="results-container">
        <h3>Source Documents</h3>
        <div id="results-list"></div>
      </div>
    </div>
  </div>

<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
<script>
document.addEventListener("DOMContentLoaded",()=>{
  const $q      = document.getElementById("search-input");
  const $btn    = document.getElementById("search-button");
  const $load   = document.getElementById("loading");
  const $resp   = document.getElementById("response-area");
  const $respCt = document.getElementById("response-content");
  const $list   = document.getElementById("results-list");

  async function search(){
    const query=$q.value.trim();
    if(!query) return;

    $load.style.display="block";
    $resp.style.display="none";

    try{
      const res=await fetch("/search",{
        method:"POST",
        headers:{"Content-Type":"application/json"},
        body:JSON.stringify({query,limit:5})
      });
      if(!res.ok){
        let msg=`Server error (${res.status})`;
        try{msg=(await res.json()).error||msg;}catch{}
        throw new Error(msg);
      }
      const data=await res.json();
      const answerHtml=String(data.response||"").replace(/\\n/g,"<br>");
      $respCt.innerHTML=`<p>${answerHtml}</p>`;
      $list.innerHTML="";

      (data.results||[]).forEach((r,i)=>{
        const card=document.createElement("div");
        card.className="card result-card";
        const src = data.results[i].metadata;
        card.innerHTML=`
          <div class="card-header bg-light d-flex justify-content-between align-items-center"
               data-bs-toggle="collapse" data-bs-target="#chunk-${i}">
            <span>${r.source}</span>
            <span class="badge bg-primary opacity-75">${r.match_type}</span>
          </div>
          <div id="chunk-${i}" class="collapse show">
            <div class="card-body">
              <div class="d-flex justify-content-between align-items-baseline">
                <strong>
                  ${src.journal} (${src.year})
                </strong>
                ${src.doi ? `<a href="https://doi.org/${src.doi}" target="_blank" class="badge bg-primary">DOI</a>` : ''}
              </div>

              <h4 class="card-title">${src.title}</h4>
              <small class="text-muted">
                ${src.section}
              </small>

              <p class="mb-0">
                <em>${src.authors}</em><br>
                ${src.volume ? `${src.journal} ${src.volume}${src.issue ? `(${src.issue})` : ''}:` : ''}
                ${src.page_range}
              </p>
              
              <div class="mb-2 small text-muted">Similarity: ${(r.similarity??0).toFixed(4)}</div>
              <pre class="small text-muted mb-0" style="white-space:pre-wrap;">${r.text||""}</pre>
            </div>
          </div>`;
        $list.appendChild(card);
      });

    }catch(err){
      $respCt.innerHTML=`<div class="alert alert-warning">${err.message}</div>`;
      $list.innerHTML="";
    }finally{
      $load.style.display="none";
      $resp.style.display="block";
    }
  }

  $btn.addEventListener("click",search);
  $q.addEventListener("keypress",e=>{if(e.key==="Enter")search();});
});
</script>
</body>
</html>"""
            )

    log.info("Starting Flask on 0.0.0.0:8080 …")
    app.run(host="0.0.0.0", port=8080, debug=True)


================================================================================


################################################################################
# File: scripts/rag_web_app_v9.py
################################################################################

# File: scripts/rag_web_app_v9.py

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Mini Flask app that answers misophonia questions with Retrieval‑Augmented
Generation (gpt-4.1-mini-2025-04-14 + Supabase pgvector).

### Patch 2  (2025‑05‑06)
• **Embeddings** now created with **text‑embedding‑ada‑002** (1536‑D).  
• Similarity is re‑computed client‑side with a **plain cosine function** so the
  ranking no longer depends on pgvector's built‑in distance or any RPC
  threshold quirks.

The rest of the grounded‑answer logic (added in Patch 1) is unchanged.
"""
from __future__ import annotations

import logging
import math
import os
import re
from pathlib import Path        # (unused but left in to mirror original)
from typing import Dict, List
import json

from dotenv import load_dotenv
from flask import Flask, jsonify, request, make_response
from openai import OpenAI
from supabase import create_client
from flask_compress import Compress
from flask_cors import CORS

# ────────────────────────────── configuration ───────────────────────────── #

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUPABASE_URL   = os.getenv("SUPABASE_URL")
SUPABASE_KEY   = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
PORT           = int(os.getenv("PORT", 8080))

if not (OPENAI_API_KEY and SUPABASE_URL and SUPABASE_KEY):
    raise SystemExit(
        "❌  Required env vars: OPENAI_API_KEY, SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY"
    )

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s",
)
log = logging.getLogger("rag_app")

sb = create_client(SUPABASE_URL, SUPABASE_KEY)
oa = OpenAI(api_key=OPENAI_API_KEY)

app = Flask(__name__)
CORS(app)
app.config['COMPRESS_ALGORITHM'] = 'gzip'
Compress(app)

# ────────────────────────────── helper functions ────────────────────────── #


def embed(text: str) -> List[float]:
    """
    Return OpenAI embedding vector for *text* using text‑embedding‑ada‑002.

    ada‑002 has 1536 dimensions and is inexpensive yet solid for similarity.
    """
    resp = oa.embeddings.create(
        model="text-embedding-ada-002",
        input=text[:8192],  # safety slice
    )
    return resp.data[0].embedding


def cosine_similarity(a: List[float], b: List[float]) -> float:
    """Plain cosine similarity between two equal‑length vectors."""
    dot = sum(x * y for x, y in zip(a, b))
    na = math.sqrt(sum(x * x for x in a))
    nb = math.sqrt(sum(y * y for y in b))
    return dot / (na * nb + 1e-9)


# Add in-memory embedding cache
_qcache = {}
def embed_cached(text):
    if text in _qcache: return _qcache[text]
    vec = embed(text)
    _qcache[text] = vec
    return vec


# Add regex patterns for bibliography detection
_DOI_RE   = re.compile(r'\b10\.\d{4,9}/[-._;()/:A-Z0-9]+', re.I)
_YEAR_RE  = re.compile(r'\((19|20)\d{2}\)')

def looks_like_refs(text: str) -> bool:
    """Return True if chunk is probably a bibliography list."""
    doi_count   = len(_DOI_RE.findall(text))
    year_count  = len(_YEAR_RE.findall(text))
    comma_count = text.count(',')           # refs are comma‑dense
    return (
        doi_count >= 18 or
        year_count >= 20
    )


def semantic_search(
    query: str,
    *,
    limit: int = 8,
    threshold: float = 0.0,
) -> List[Dict]:
    """
    Retrieve candidate chunks via the pgvector RPC, then re-rank with an
    **explicit cosine similarity** so the final score is always in
    **[-100 … +100] percent**.

    Why the extra work?
    -------------------
    •  The SQL function returns a raw inner-product that can be > 1.  
       (embeddings are *not* unit-length.)  
    •  By pulling the real 1 536-D vectors and re-computing a cosine we get a
       true, bounded similarity that front-end code can safely show.

    The -100 … +100 range is produced by:  
        pct = clamp(cosine × 100, -100, 100)
    """
    # 1. Embed the query once and keep it cached
    q_vec = embed_cached(query)

    # 2. Fast ANN search in Postgres (over-fetch 4× so we can re-rank)
    rows = (
        sb.rpc(
            "match_research_chunks",
            {
                "query_embedding": q_vec,
                "match_threshold": threshold,
                "match_count": limit * 4,
            },
        )
        .execute()
        .data
    ) or []

    # 3. Filter out bibliography-only chunks
    rows = [r for r in rows if not looks_like_refs(r["text"])]

    if not rows:
        return []

    # 4. Fetch document metadata (title, authors …) in one round-trip
    doc_ids = {r["document_id"] for r in rows}
    meta = {
        d["id"]: d
        for d in (
            sb.table("research_documents")
              .select("id,title,authors,year,journal,doi,abstract,keywords,research_topics,source_pdf")
              .in_("id", list(doc_ids))
              .execute()
              .data
            or []
        )
    }

    # 5. Pull embeddings once and compute **plain cosine** (no scaling)
    chunk_ids = [r["id"] for r in rows]

    emb_rows = (
        sb.table("research_chunks")
          .select("id, embedding")
          .in_("id", chunk_ids)
          .execute()
          .data
    ) or []

    emb_map: Dict[str, List[float]] = {}
    for e in emb_rows:
        raw = e["embedding"]
        if isinstance(raw, list):                    # list[Decimal]
            emb_map[e["id"]] = [float(x) for x in raw]
        elif isinstance(raw, str) and raw.startswith('['):   # TEXT  "[…]"
            emb_map[e["id"]] = [float(x) for x in raw.strip('[]').split(',')]

    for r in rows:
        vec = emb_map.get(r["id"])
        if vec:                                     # we now have the real vector
            cos = cosine_similarity(q_vec, vec)
            r["similarity"] = round(cos * 100, 1)   # –100…+100 % (or 0…100 %)
        else:                                       # fallback if something failed
            dist = float(r.get("similarity", 1.0))  # 0…2 cosine-distance
            r["similarity"] = round((1.0 - dist) * 100, 1)

        r["doc"] = meta.get(r["document_id"], {})

    # 6. Keep the top *limit* rows after proper re-ranking
    ranked = sorted(rows, key=lambda x: x["similarity"], reverse=True)[:limit]
    return ranked


# ──────────────────────── NEW RAG‑PROMPT HELPERS ───────────────────────── #

MAX_PROMPT_CHARS: int = 24_000  # ~6 k tokens @ 4 chars/token heuristic


def trim_chunks(chunks: List[Dict]) -> List[Dict]:
    """
    Fail‑safe guard: ensure concatenated chunk texts remain under the
    MAX_PROMPT_CHARS budget.  Keeps highest‑similarity chunks first.
    """
    sorted_chunks = sorted(chunks, key=lambda c: c.get("similarity", 0), reverse=True)
    output: List[Dict] = []
    total_chars = 0
    for c in sorted_chunks:
        chunk_len = len(c["text"])
        if total_chars + chunk_len > MAX_PROMPT_CHARS:
            break
        output.append(c)
        total_chars += chunk_len
    return output


def build_prompt(question: str, chunks: List[Dict]) -> str:
    """
    Build a structured prompt that asks GPT to:
      • answer in Markdown with short intro + numbered list of key points
      • cite inline like [1], [2] …
      • finish with a Bibliography that includes the *paper title*
    """
    snippet_lines, biblio_lines = [], []
    for i, c in enumerate(chunks, 1):
        snippet_lines.append(
            f"[{i}] \"{c['text'].strip()}\" "
            f"(pp. {c['page_start']}-{c['page_end']})"
        )

        d = c["doc"]
        title   = d.get("title", "Untitled")
        authors = ", ".join(d.get("authors") or ["Unknown"])
        journal = d.get("journal", "Unknown journal")
        year    = d.get("year", "n.d.")
        pages   = f"pp. {c['page_start']}-{c['page_end']}"
        doi_raw = d.get("doi")
        doi_md  = f"[doi:{doi_raw}](https://doi.org/{doi_raw})" if doi_raw else ""

        # Title now comes first ↓↓↓
        biblio_lines.append(
            f"[{i}] *{title}* · {authors} · {journal} ({year}) · {pages} {doi_md}"
        )

    prompt_parts = [
        "You are an expert assistant specialised in misophonia.",
        "====",
        "QUESTION:",
        question,
        "====",
        "CONTEXT:",
        *snippet_lines,
        "====",
        "INSTRUCTIONS:",
        "• Write the answer in **Markdown**:",
        "    – A one‑sentence overview",
        "    – Then a numbered list of key findings (each ≤ 2 lines)",
        "    – End with a short concluding sentence",
        "• Cite sources inline like [1], [2] …",
        "• After the answer, reproduce a section titled 'BIBLIOGRAPHY:'",
        "  listing each source exactly as provided below (title included).",
        "• If none of the chunks answer the question, reply:",
        '  "I\'m sorry, I don\'t have sufficient information to answer that."',
        "====",
        "BEGIN OUTPUT",
        "ANSWER:",
        "",                       # <- model writes here
        "BIBLIOGRAPHY:",
        *biblio_lines,
    ]

    return '\n'.join(prompt_parts)


def extract_citations(answer: str) -> List[str]:
    """
    Parse numeric citations (e.g., "[1]", "[2]") from the answer text.
    Returns unique citation numbers in ascending order.
    """
    citations = re.findall(r"\[(\d+)\]", answer)
    return sorted(set(citations), key=int)


# ──────────────────────────────── routes ────────────────────────────────── #

@app.post("/search")
def search():
    payload = request.get_json(force=True, silent=True) or {}
    question = (payload.get("query") or "").strip()
    if not question:
        return jsonify({"error": "Missing 'query'"}), 400

    try:
        # Retrieve semantic matches (client‑side cosine re‑ranked)
        raw_matches = semantic_search(question, limit=int(payload.get("limit", 8)))

        if not raw_matches:
            return jsonify(
                {
                    "answer": "I'm sorry, I don't have sufficient information to answer that.",
                    "citations": [],
                    "results": [],
                }
            )

        # ──────────────────── TRIM CHUNKS TO BUDGET ──────────────────── #
        chunks = trim_chunks(raw_matches)

        # ──────────────────── BUILD PROMPT & CALL LLM ─────────────────── #
        prompt = build_prompt(question, chunks)

        completion = oa.chat.completions.create(
            model="gpt-4.1-mini-2025-04-14",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
        )
        answer_text: str = completion.choices[0].message.content.strip()

        # ──────────────────── EXTRACT CITATIONS ──────────────────────── #
        citations = extract_citations(answer_text)

        # Remove embedding vectors before sending back to the browser
        for m in raw_matches:
            m.pop("embedding", None)

        # ──────────────────── RETURN JSON ─────────────────────────────── #
        response = jsonify(
            {
                "answer": answer_text,
                "citations": citations,
                "results": raw_matches,
            }
        )
        response.headers['Connection'] = 'keep-alive'
        return response
    except Exception as exc:  # noqa: BLE001
        log.exception("search failed")
        return jsonify({"error": str(exc)}), 500


@app.get("/stats")
def stats():
    """Tiny ops endpoint—count total chunks."""
    resp = sb.table("research_chunks").select("id", count="exact").execute()
    return jsonify({"total_chunks": resp.count})


# ──────────────────────────────── main ─────────────────────────────────── #

if __name__ == "__main__":
    log.info("Starting Flask on 0.0.0.0:%s …", PORT)
    app.run(host="0.0.0.0", port=PORT, debug=True)


================================================================================


################################################################################
# File: scripts/templates/rag_web_interface_gian_v1.py
################################################################################

# File: scripts/templates/rag_web_interface_gian_v1.py

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Interactive RAG demo (Flask) that queries a Supabase vector store
(`research_chunks`) instead of Firebase / Firestore.

R5  →  • robust metadata parsing (string → JSON → dict)
        • never throws on missing / malformed metadata
        • identical external API; front-end needs **no** changes
"""

from __future__ import annotations

import json
import logging
import os
import sys
from typing import Any, Dict, List, Sequence

from dotenv import load_dotenv
from flask import Flask, jsonify, render_template, request
from openai import OpenAI
from supabase import create_client


# ───────────────────────────── configuration ───────────────────────────── #

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUPABASE_URL   = os.getenv("SUPABASE_URL")
SUPABASE_KEY   = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

DOC_FIELDS = (
    "title, authors, year, journal, doi, pages, volume, issue"
)

if not OPENAI_API_KEY:
    print("⛔  OPENAI_API_KEY missing — aborting")
    sys.exit(1)

client = OpenAI(api_key=OPENAI_API_KEY)

logging.basicConfig(
    level=logging.INFO, format="%(asctime)s — %(levelname)s — %(message)s"
)
log = logging.getLogger(__name__)
app = Flask(__name__)


# ───────────────────────────── Supabase helpers ─────────────────────────── #

def init_supabase():
    if not SUPABASE_URL or not SUPABASE_KEY:
        log.error("SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY not set.")
        sys.exit(1)
    return create_client(SUPABASE_URL, SUPABASE_KEY)


sb = init_supabase()


# ──────────────────────── metadata / citation helpers ────────────────────── #

def _as_meta(obj: Any) -> Dict[str, Any]:
    """Best‑effort coercion to dict for the *metadata* column."""
    if isinstance(obj, dict):
        return obj
    if isinstance(obj, str):
        try:
            return json.loads(obj) or {}
        except Exception:
            return {}
    return {}


def _first(meta: Dict[str, Any], keys: Sequence[str]):
    for k in keys:
        v = meta.get(k)
        if v and str(v).strip() and not str(v).lower().startswith("unknown"):
            return v
    return None


def _first_author(meta: Dict[str, Any]) -> str | None:
    if (a := _first(meta, ["primary_author", "author", "creator"])):
        return str(a)
    if (lst := meta.get("authors")):
        if isinstance(lst, list) and lst:
            return str(lst[0])
        if isinstance(lst, str) and lst.strip() and not lst.lower().startswith("unknown"):
            return lst.split(",")[0].strip()
    return None


def _page(meta: Dict[str, Any]) -> str | None:
    if (p := _first(meta, ["page", "page_number"])):
        return str(p)
    if (rng := _first(meta, ["pages", "page_range"])):
        if isinstance(rng, list) and rng:
            return str(rng[0])
        if isinstance(rng, str):
            return rng.split("–")[0].split("-")[0]
    if meta.get("page_start") is not None:
        return str(meta["page_start"])
    return None


def make_citation(meta_raw: Any) -> str:
    meta = _as_meta(meta_raw)

    parts = []
    if (a := _first_author(meta)):
        parts.append(a)
    if (y := _first(meta, ["year", "pub_year"])):
        parts.append(str(y))
    if (t := _first(meta, ["title", "document_title", "section"])):
        parts.append(f"– {t.strip()}")
    if (j := _first(meta, ["journal", "source"])):
        parts.append(j)
    if (doi := _first(meta, ["doi"])):
        parts.append(f"doi:{doi}")
    if (pg := _page(meta)):
        parts.append(f"p.{pg}")

    return " ".join(parts) if parts else "No citation metadata"


def long_citation(meta_raw: Any) -> str:
    meta = _as_meta(meta_raw)

    auth  = _first_author(meta) or ""
    year  = _first(meta, ["year", "pub_year"]) or ""
    title = _first(meta, ["title", "document_title", "section"]) or ""
    jour  = _first(meta, ["journal", "source"]) or ""
    doi   = _first(meta, ["doi"]) or ""
    pg    = _page(meta) or ""

    bits = []
    if auth or year:
        bits.append(f"{auth} ({year}).".strip())
    if title:
        bits.append(title)
    if jour:
        bits.append(jour)
    if doi:
        bits.append(f"doi:{doi}")
    if pg:
        bits.append(f"p.{pg}")

    return " ".join(bits)


# ────────────────────────────── embedding helpers ────────────────────────── #

def generate_embedding(text: str) -> List[float] | None:
    try:
        resp = client.embeddings.create(model="text-embedding-ada-002", input=text)
        return resp.data[0].embedding
    except Exception as exc:
        log.error("Embedding failed: %s", exc)
        return None


# ─────────────────────────────── search logic ────────────────────────────── #

def semantic_search(query: str, limit: int = 5) -> List[Dict[str, Any]]:
    vec = generate_embedding(query)
    if vec is None:
        return []

    try:
        resp = (
            sb.rpc(
                "search_research_chunks",
                {
                    "query_embedding": vec,
                    "match_count": limit,
                    "similarity_threshold": 0.6,
                },
            ).execute()
        )
        rows = resp.data or []
        for r in rows:
            r["similarity"] = float(r.get("similarity", 0.0))
            r["match_type"] = f"Semantic ({r['similarity']:.4f})"
            r["chunk_id"]   = r.pop("id", "unknown")
            r["metadata"]   = _as_meta(r.get("metadata", {}))
            r["source"]     = make_citation(r["metadata"])
        return rows
    except Exception as exc:
        log.error("RPC search failed: %s", exc)
        return []


def comprehensive_search(query: str, limit: int = 5) -> List[Dict[str, Any]]:
    return semantic_search(query, limit)


# ──────────────────────────── RAG generation ─────────────────────────────── #

def generate_rag_response(
    query: str,
    docs: List[Dict[str, Any]],
    max_tokens: int = 500,
) -> str:
    try:
        snippets = [
            f"Source {i} — {long_citation(d.get('metadata', {}))}\n{d.get('text','')}\n"
            for i, d in enumerate(docs, 1)
        ]

        prompt = f"""You are a helpful AI assistant specialised in misophonia research.

Answer the question below using *only* the provided research snippets.

Question: "{query}"

Snippets:
{''.join(snippets)}

Give a concise, structured answer with numbered citations like [1], [2] … .
If evidence is insufficient, say so explicitly."""
        comp = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "You answer misophonia questions strictly from the provided sources, citing them.",
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=max_tokens,
        )
        return comp.choices[0].message.content
    except Exception as exc:
        log.error("RAG generation failed: %s", exc)
        return f"Error generating response: {exc}"


# ───────────────────────────── Flask endpoints ───────────────────────────── #

@app.route("/")
def index():
    return render_template("index.html")


@app.route("/search", methods=["POST"])
def search():
    try:
        data = request.get_json(force=True, silent=True)
        if not data or not data.get("query"):
            return jsonify({"error": "No query provided"}), 400

        query = data["query"].strip()
        limit = int(data.get("limit", 5))

        results = comprehensive_search(query, limit)

        if not results:
            return jsonify(
                {"error": "No relevant documents found for your query", "results": []}
            ), 404

        # 1.  Bulk‑fetch document metadata
        doc_map: Dict[str, Dict[str, Any]] = {}
        doc_ids = {r["document_id"] for r in results if r.get("document_id")}
        if doc_ids:
            q = (
                sb.table("research_documents")
                  .select(f"id,{DOC_FIELDS}")
                  .in_("id", list(doc_ids))
                  .execute()
            )
            for d in q.data or []:
                doc_map[d["id"]] = d

        # 2.  Build front‑end payload
        sources = []
        for r in results:
            doc = doc_map.get(r.get("document_id"), {})

            authors = (
                ", ".join(doc["authors"])
                if isinstance(doc.get("authors"), list)
                else doc.get("authors")
            )

            page_range = (
                doc.get("pages")
                or f"{r.get('page_start', '?')}-{r.get('page_end', '?')}"
            )

            sources.append(
                {
                    "chunk":       r.get("text", ""),
                    "source":      r.get("source"),
                    "match_type":  r.get("match_type"),
                    "similarity":  r.get("similarity"),
                    "chunk_id":    r.get("chunk_id"),
                    "metadata": {
                        "title":      doc.get("title")   or "Unknown Title",
                        "section":    "Unknown Section",
                        "authors":    authors            or "Unknown Authors",
                        "journal":    doc.get("journal") or "Unknown Journal",
                        "year":       doc.get("year")    or "Unknown",
                        "volume":     doc.get("volume"),
                        "issue":      doc.get("issue"),
                        "doi":        doc.get("doi"),
                        "page_range": page_range,
                    },
                }
            )

        answer = generate_rag_response(query, results) or ""
        return jsonify({"results": sources, "response": answer})
    except Exception as e:
        logging.exception("Search failed")
        return jsonify({"error": str(e)}), 500


@app.route("/stats")
def stats():
    try:
        total_chunks = (
            sb.table("research_chunks")
              .select("id", count="exact")
              .execute()
              .count
            or 0
        )
        return jsonify({"total_chunks": total_chunks}), 200
    except Exception as e:
        logging.exception("Stats endpoint failed")
        return jsonify({"error": str(e)}), 500


# ────────────────────────────── bootstrap html ───────────────────────────── #

if __name__ == "__main__":
    tpl_dir   = os.path.join(os.path.dirname(__file__), "templates")
    os.makedirs(tpl_dir, exist_ok=True)
    index_path = os.path.join(tpl_dir, "index.html")

    if not os.path.exists(index_path):
        with open(index_path, "w") as fp:
            fp.write("""<!DOCTYPE html>
<!-- identical template as previous version omitted for brevity -->""")

    log.info("Starting Flask on 0.0.0.0:8080 …")
    app.run(host="0.0.0.0", port=8080, debug=True)


================================================================================


################################################################################
# File: scripts/process_research_metadata.py
################################################################################

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Research Metadata Processor

This script:
1. Iterates over text files in documents/research/txt
2. For each file:
   - Takes first 3000 words from the text
   - Makes an OpenAI API call with a specific prompt
   - Updates the corresponding JSON file in documents/research/json
   - Records processed files to avoid reprocessing on subsequent runs

Required environment variables:
------------------------------
OPENAI_API_KEY

Usage:
-----
python process_research_metadata.py [--batch-size N] [--force] [--model MODEL]
"""

import argparse
import json
import logging
import os
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Set

from dotenv import load_dotenv
from openai import OpenAI
from tqdm import tqdm

# ─────────────────────────── Configuration ─────────────────────────────── #

load_dotenv()

OPENAI_API_KEY: Optional[str] = os.environ.get("OPENAI_API_KEY")
DEFAULT_MODEL = "gpt-4.1-mini-2025-04-14"

# Paths
SCRIPT_DIR = Path(__file__).resolve().parent
ROOT_DIR = SCRIPT_DIR.parent
TXT_DIR = ROOT_DIR / "documents" / "research" / "txt"
JSON_DIR = ROOT_DIR / "documents" / "research" / "json"
PROCESSED_FILE = ROOT_DIR / "scripts" / "processed_files.json"

# Initialize logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s",
)
log = logging.getLogger(__name__)

# ─────────────────────────────── Helpers ────────────────────────────────── #

def load_processed_files() -> Set[str]:
    """Load the set of already processed files."""
    if PROCESSED_FILE.exists():
        with open(PROCESSED_FILE, "r") as f:
            try:
                return set(json.load(f))
            except json.JSONDecodeError:
                log.warning("Error decoding processed files list, starting fresh")
    return set()

def save_processed_files(processed: Set[str]) -> None:
    """Save the set of processed files."""
    with open(PROCESSED_FILE, "w") as f:
        json.dump(list(processed), f, indent=2)

def extract_first_n_words(text: str, n: int = 3000) -> str:
    """Extract first n words from text."""
    words = text.split()
    return " ".join(words[:n])

def get_corresponding_json_path(txt_path: Path) -> Path:
    """Get the path to the corresponding JSON file."""
    return JSON_DIR / f"{txt_path.stem}.json"

def generate_metadata(client: OpenAI, text: str, model: str) -> Dict[str, Any]:
    """Call OpenAI API to generate metadata from text."""
    prompt = f"""
Extract the following metadata from this scientific paper and return exactly one JSON object with keys:
  • doc_type (e.g. "scientific paper")
  • title
  • authors (array of strings)
  • year (integer)
  • journal (string or null)
  • DOI (string or null)
  • abstract (string or null)
  • keywords (array of strings)
  • research_topics (array of strings)

If a field is not present, set it to null or an empty array. Here is the paper's full text:

{text}
"""

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are an assistant that extracts structured metadata from scientific papers."},
                {"role": "user", "content": prompt}
            ]
        )
        
        content = response.choices[0].message.content
        
        # Extract JSON from response (in case there's additional text)
        json_match = re.search(r'({[\s\S]*})', content)
        if json_match:
            content = json_match.group(1)
            
        return json.loads(content)
    except Exception as e:
        log.error(f"Error calling OpenAI API: {e}")
        raise

def update_json_file(json_path: Path, metadata: Dict[str, Any]) -> None:
    """Update JSON file with metadata from API response."""
    try:
        # Create if not exists
        if not json_path.exists():
            json_data = {
                "doc_type": "scientific paper",
                "title": "",
                "authors": [],
                "year": None,
                "journal": None,
                "doi": None,
                "abstract": None,
                "keywords": [],
                "research_topics": [],
                "created_at": datetime.utcnow().isoformat() + "Z",
                "source_pdf": "",
                "sections": []
            }
        else:
            # Read existing JSON file
            with open(json_path, "r", encoding="utf-8") as f:
                json_data = json.load(f)

        # Update with new metadata
        for key, value in metadata.items():
            if key in json_data and value not in (None, [], ""):
                json_data[key] = value

        # Write updated JSON file
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
            
        return True
    except Exception as e:
        log.error(f"Error updating JSON file {json_path}: {e}")
        return False

# ────────────────────────────── Main Process ────────────────────────────── #

def process_files(client: OpenAI, batch_size: int, force: bool, model: str) -> Dict[str, Any]:
    """Process text files and update JSON files with metadata."""
    results = {
        "processed": 0,
        "skipped": 0,
        "errors": 0,
        "file_results": []
    }
    
    # Get all text files
    txt_files = list(TXT_DIR.glob("*.txt"))
    log.info(f"Found {len(txt_files)} text files in {TXT_DIR}")
    
    # Load set of processed files
    processed_files = set() if force else load_processed_files()
    
    # Filter unprocessed files or limit to batch size
    if not force:
        txt_files = [f for f in txt_files if f.name not in processed_files]
    
    log.info(f"Found {len(txt_files)} unprocessed files")
    if batch_size > 0:
        txt_files = txt_files[:batch_size]
        log.info(f"Processing batch of {len(txt_files)} files")
    
    # Process each file
    for txt_path in tqdm(txt_files, desc="Processing files"):
        # Log the current file being processed
        log.info(f"Processing file: {txt_path.name}")
        
        file_result = {
            "file": txt_path.name,
            "success": False,
            "error": None
        }
        
        try:
            # Read text file
            with open(txt_path, "r", encoding="utf-8") as f:
                text = f.read()
            
            # Extract first 3000 words
            truncated_text = extract_first_n_words(text)
            
            # Generate metadata - log that we're calling the API
            log.info(f"Calling OpenAI API for: {txt_path.name}")
            metadata = generate_metadata(client, truncated_text, model)
            
            # Get corresponding JSON path
            json_path = get_corresponding_json_path(txt_path)
            
            # Update JSON file
            log.info(f"Updating JSON file for: {txt_path.name}")
            if update_json_file(json_path, metadata):
                file_result["success"] = True
                processed_files.add(txt_path.name)
                results["processed"] += 1
                log.info(f"Successfully processed: {txt_path.name}")
            else:
                file_result["error"] = "Failed to update JSON file"
                results["errors"] += 1
                log.error(f"Failed to update JSON for: {txt_path.name}")
                
        except Exception as e:
            file_result["error"] = str(e)
            results["errors"] += 1
            log.error(f"Error processing {txt_path.name}: {e}")
            
        results["file_results"].append(file_result)
    
    # Save processed files
    save_processed_files(processed_files)
    
    return results

def main() -> None:
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Process research files and extract metadata using OpenAI API")
    parser.add_argument("--batch-size", type=int, default=0, help="Number of files to process (0 = all)")
    parser.add_argument("--force", action="store_true", help="Process all files even if previously processed")
    parser.add_argument("--model", type=str, default=DEFAULT_MODEL, help="OpenAI model to use")
    args = parser.parse_args()
    
    # Check for OpenAI API key
    if not OPENAI_API_KEY:
        log.error("OPENAI_API_KEY not set. Please set it in your environment variables.")
        sys.exit(1)
    
    # Initialize OpenAI client
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    # Create directories if they don't exist
    TXT_DIR.mkdir(parents=True, exist_ok=True)
    JSON_DIR.mkdir(parents=True, exist_ok=True)
    
    # Process files
    log.info(f"Starting metadata extraction with model: {args.model}")
    results = process_files(client, args.batch_size, args.force, args.model)
    
    # Output results
    log.info(f"Processing complete: {results['processed']} processed, {results['skipped']} skipped, {results['errors']} errors")
    
    # Save report
    ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    report_path = ROOT_DIR / f"metadata_extraction_report_{ts}.json"
    with open(report_path, "w") as f:
        json.dump(results, f, indent=2)
    log.info(f"Report saved to {report_path}")

if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: src/TermsModal.jsx
################################################################################

import React, { useState } from 'react';

export default function TermsModal({ onAccept }) {
  const [checked, setChecked] = useState(false);
  return (
    <div style={{ position:'fixed', top:0, left:0, right:0, bottom:0, backgroundColor:'rgba(0,0,0,0.7)', zIndex:1000, display:'flex', alignItems:'center', justifyContent:'center', border:'4px dashed red' }}>
      <div style={{ background:'#fff', padding:'2rem', maxWidth:'600px', width:'80%', boxSizing:'border-box', maxHeight:'80vh', overflowY:'auto', borderRadius:'8px', display:'flex', flexDirection:'column', alignItems:'center', border:'4px solid blue' }}>
        <h2>Terms and Conditions</h2>
        <div style={{ overflowY:'auto', maxHeight:'50vh', textAlign:'left', width:'100%' }}>
          <p><strong>Effective Date:</strong> [Insert Date]</p>
          <p><strong>1. Acceptance of Terms</strong> By accessing or using the Misophonia Companion application ('App'), you agree to be bound by these Terms and Conditions ('Terms'). If you do not agree to these Terms, do not use the App.</p>
          <p><strong>2. Nature of the App</strong> Misophonia Companion is a digital wellness and research guide. It is intended for informational, educational, and personal support purposes only. The App does not provide medical advice, diagnosis, or treatment. It is not a licensed healthcare service and is not intended to replace professional consultation.</p>
          <p><strong>3. Eligibility</strong> You must be at least 16 years old to use the App. If you are under 18, you confirm that you have received parental or guardian consent.</p>
          <p><strong>4. Privacy and Data</strong> We respect your privacy. Our Privacy Policy explains how we collect, use, and protect your information. By using the App, you agree to the terms of our Privacy Policy.</p>
          <p>No health information (as defined under HIPAA) is collected without explicit consent.</p>
          <p>Conversations may be stored anonymously for product improvement unless you opt out.</p>
          <p>We do not sell or share your data with third parties for advertising purposes.</p>
          <p><strong>5. User Conduct</strong> You agree to use the App responsibly and not to misuse the services, including but not limited to:</p>
          <ul>
            <li>Attempting to reverse-engineer, copy, or modify the App</li>
            <li>Submitting harmful, abusive, or misleading content</li>
            <li>Interfering with the operation or integrity of the App</li>
          </ul>
          <p><strong>6. Limitation of Liability</strong> To the fullest extent permitted by law, Misophonia Companion and its creators are not liable for any direct, indirect, incidental, or consequential damages resulting from the use—or inability to use—the App. This includes, but is not limited to, psychological distress, loss of data, or misinterpretation of information.</p>
          <p><strong>7. Modifications to the Terms</strong> We reserve the right to modify these Terms at any time. Continued use of the App after changes means you accept the new Terms.</p>
          <p><strong>8. Termination</strong> We may suspend or terminate access to the App at our discretion, without notice, for conduct that violates these Terms.</p>
          <p><strong>9. Governing Law</strong> These Terms are governed by the laws of the state of [Your State], without regard to conflict of laws principles.</p>
          <p><strong>10. Contact</strong> For questions or concerns about these Terms, please email: [Your Contact Email]</p>
        </div>
        <h2>Privacy Policy</h2>
        <div style={{ overflowY:'auto', maxHeight:'30vh', textAlign:'left', width:'100%' }}>
          <p><strong>Effective Date:</strong> [Insert Date]</p>
          <p><strong>1. Introduction</strong> This Privacy Policy describes how Misophonia Companion ('we', 'us', 'our') collects, uses, and protects your personal information when you use our web and mobile application ('App').</p>
          <p><strong>2. Information We Collect</strong></p>
          <p><strong>•</strong> <em>User-Provided Information</em>: When you create an account or interact with the App, you may provide personal information such as your email address and preferences.</p>
          <p><strong>•</strong> <em>Anonymous Usage Data</em>: We may collect anonymized data on app usage, interactions, and conversation content to improve the experience.</p>
          <p><strong>•</strong> <em>Optional Personal Logs</em>: Users may opt in to mood tracking, journaling, or trigger tagging. This data is stored securely and only accessible to the user unless explicitly shared.</p>
          <p><strong>3. How We Use Your Information</strong> To provide and improve our services; to personalize your user experience; for anonymized research and development; to comply with legal obligations if required.</p>
          <p><strong>4. Data Storage and Security</strong> We use secure, encrypted servers and industry-standard protocols to store your data. Sensitive data is never shared with third parties without consent. You may request deletion of your account and data at any time.</p>
          <p><strong>5. Sharing and Disclosure</strong> We do not sell or rent your personal information. We may share anonymized data with research collaborators or analytics partners. We may disclose information if legally compelled (e.g., court order).</p>
          <p><strong>6. Your Rights and Choices</strong> You may update or delete your personal information from your profile. You may opt out of data collection for research purposes. You may contact us for a copy of any personal data we’ve stored.</p>
          <p><strong>7. Children’s Privacy</strong> Our App is not intended for children under 16. We do not knowingly collect data from individuals under this age without parental consent.</p>
          <p><strong>8. Changes to This Policy</strong> We may revise this policy from time to time. Users will be notified of significant changes. Continued use of the App indicates acceptance of the updated policy.</p>
          <p><strong>9. Contact Us</strong> If you have questions or requests related to this Privacy Policy, please contact us at: [Your Contact Email]</p>
        </div>
        <label style={{ display:'block', margin:'1rem 0', textAlign:'center', width:'100%' }}>
          <input type='checkbox' checked={checked} onChange={e => setChecked(e.target.checked)} /> I have read and agree to the Terms and Conditions and Privacy Policy
        </label>
        <button disabled={!checked} onClick={() => { localStorage.setItem('termsAccepted','true'); onAccept(); }} style={{ marginTop:'1.5rem', padding:'0.5rem 1rem' }}>Continue</button>
      </div>
    </div>
  );
}


================================================================================


################################################################################
# File: find_duplicates.py
################################################################################

# File: find_duplicates.py

import os
import hashlib
import argparse
from pathlib import Path
from collections import defaultdict

def calculate_file_hash(filepath, algorithm='sha256', buffer_size=65536):
    """Calculate a hash for a file to identify duplicates."""
    hash_obj = hashlib.new(algorithm)
    
    with open(filepath, 'rb') as f:
        # Read the file in chunks to handle large files efficiently
        buffer = f.read(buffer_size)
        while buffer:
            hash_obj.update(buffer)
            buffer = f.read(buffer_size)
    
    return hash_obj.hexdigest()

def find_duplicates(directory):
    """Find duplicate files in the specified directory."""
    files_by_hash = defaultdict(list)
    duplicate_sets = []
    
    # Get all files in the directory
    target_dir = Path(directory)
    if not target_dir.exists() or not target_dir.is_dir():
        print(f"Error: '{directory}' is not a valid directory")
        return duplicate_sets
    
    print(f"Scanning directory: {directory}")
    
    # Calculate hashes for all files
    all_files = list(target_dir.glob('*'))
    total_files = len(all_files)
    
    for i, file_path in enumerate(all_files):
        if file_path.is_file():
            try:
                file_hash = calculate_file_hash(file_path)
                files_by_hash[file_hash].append(file_path)
                print(f"Processed file {i+1}/{total_files}: {file_path.name}")
            except Exception as e:
                print(f"Error processing {file_path}: {e}")
    
    # Identify duplicate sets (files with the same hash)
    for file_hash, paths in files_by_hash.items():
        if len(paths) > 1:
            duplicate_sets.append(paths)
    
    return duplicate_sets

def delete_duplicates(duplicate_sets, interactive=True):
    """Delete duplicate files, keeping only one copy of each."""
    total_deleted = 0
    total_size_saved = 0
    
    for duplicate_set in duplicate_sets:
        # Sort by name for consistent results
        duplicate_set.sort(key=lambda p: str(p))
        
        # Keep the first file, show options for the rest
        keep_file = duplicate_set[0]
        print(f"\nDuplicate set ({len(duplicate_set)} files):")
        print(f"  Keeping: {keep_file}")
        
        for i, dup_file in enumerate(duplicate_set[1:], 1):
            size = dup_file.stat().st_size
            
            if interactive:
                response = input(f"  Delete duplicate #{i}: {dup_file}? (y/n/a=all/q=quit): ").lower()
                
                if response == 'q':
                    print("Operation aborted.")
                    return total_deleted, total_size_saved
                    
                if response == 'a':
                    interactive = False
                    response = 'y'
            else:
                response = 'y'
                print(f"  Deleting duplicate #{i}: {dup_file}")
            
            if response == 'y':
                try:
                    dup_file.unlink()
                    total_deleted += 1
                    total_size_saved += size
                    print(f"  Deleted: {dup_file}")
                except Exception as e:
                    print(f"  Error deleting {dup_file}: {e}")
    
    return total_deleted, total_size_saved

def format_size(size_bytes):
    """Format file size in human-readable format."""
    if size_bytes < 1024:
        return f"{size_bytes} bytes"
    elif size_bytes < 1024 * 1024:
        return f"{size_bytes/1024:.2f} KB"
    elif size_bytes < 1024 * 1024 * 1024:
        return f"{size_bytes/(1024*1024):.2f} MB"
    else:
        return f"{size_bytes/(1024*1024*1024):.2f} GB"

def main():
    parser = argparse.ArgumentParser(description="Find and remove duplicate files")
    parser.add_argument('--directory', '-d', default='documents/research/Global',
                        help="Directory to scan for duplicates (default: documents/research/Global)")
    parser.add_argument('--delete', '-r', action='store_true',
                        help="Delete duplicate files")
    parser.add_argument('--auto', '-a', action='store_true',
                        help="Automatically delete all duplicates without prompting")
    
    args = parser.parse_args()
    
    # Find duplicates
    duplicate_sets = find_duplicates(args.directory)
    
    # Print summary of duplicates found
    if not duplicate_sets:
        print("\nNo duplicate files found.")
        return
    
    total_duplicates = sum(len(dups) - 1 for dups in duplicate_sets)
    print(f"\nFound {len(duplicate_sets)} sets of duplicate files ({total_duplicates} redundant files)")
    
    # Display details about each duplicate set
    for i, dups in enumerate(duplicate_sets, 1):
        size = dups[0].stat().st_size
        size_str = format_size(size)
        print(f"\nDuplicate Set #{i} - {len(dups)} files, {size_str} each:")
        for path in dups:
            print(f"  {path}")
    
    # Delete duplicates if requested
    if args.delete or args.auto:
        deleted, size_saved = delete_duplicates(duplicate_sets, not args.auto)
        print(f"\nSummary: Deleted {deleted} duplicate files, saving {format_size(size_saved)}")
    else:
        print("\nTo delete duplicates, run again with --delete or --auto flag")

if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: scripts/test_vector_search.py
################################################################################

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Vector‑search smoke‑test (Supabase edition)
==========================================

• Firebase/Firestore has been removed — every data call now goes through
  Supabase's PostgREST API.

• The script calls a SQL helper function that must exist on your database:
    public.search_research_chunks(query_text TEXT,
                                  match_count INT DEFAULT 10,
                                  similarity_threshold REAL DEFAULT 0.6)
  which should:
    1. embed the incoming `query_text`
    2. invoke your `match_documents` similarity function
    3. return the top‑`match_count` rows as
       (id UUID, text TEXT, metadata JSONB, similarity REAL)

  See README / earlier instructions for a ready‑made implementation.

Environment variables required
------------------------------
SUPABASE_URL                 – e.g. https://xxxx.supabase.co
SUPABASE_SERVICE_ROLE_KEY    – or an anon key if RLS permits the RPC
OPENAI_API_KEY               – only if your SQL helper embeds via an HTTP call
"""

from __future__ import annotations

import os
import sys
import time
from typing import Any, Dict, List

from dotenv import load_dotenv
from supabase import create_client
from openai import OpenAI

# ──────────────────────── configuration & sanity checks ───────────────────── #

load_dotenv()

# ---------- Supabase config ----------
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")   # or anon if RLS permits
sb = create_client(SUPABASE_URL, SUPABASE_KEY)

if not SUPABASE_URL or not SUPABASE_KEY:
    sys.exit(
        "❌  SUPABASE_URL / SUPABASE_SERVICE_ROLE_KEY env vars are missing.\n"
        "    export them and rerun."
    )

# Sample questions to probe the index
SAMPLE_QUERIES: List[str] = [
    "What are the symptoms of misophonia?",
    "How prevalent is misophonia in university students?",
    "What is the relationship between misophonia and hyperacusis?",
    "What treatments are effective for misophonia?",
    "How does misophonia affect quality of life?",
]

# Add this after other configuration
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def embed(text: str) -> List[float]:
    """Return OpenAI ada‑002 embedding (1536‑dim list of floats)."""
    resp = openai_client.embeddings.create(
        model="text-embedding-ada-002",
        input=text,
    )
    return resp.data[0].embedding

# ─────────────────────────── helper functions ─────────────────────────────── #


def perform_vector_search(query_vec, top_k=5, thresh=0.6):
    """
    Call the SQL RPC we just created.
    `query_vec` is a list[float] length 1536 coming from OpenAI.
    """
    try:
        resp = sb.rpc(
            "search_research_chunks",
            {
                "query_embedding": query_vec,
                "match_count": top_k,
                "similarity_threshold": thresh,
            },
        ).execute()
        if getattr(resp, "error", None):
            raise RuntimeError(resp.error)
        return resp.data or []
    except Exception as e:
        print(f"   ⚠  RPC failed: {e}")
        return []


def print_results(rows: List[Dict[str, Any]]) -> None:
    """
    Nicely format the search results.
    """
    if not rows:
        print("   (no matches)\n")
        return

    for idx, row in enumerate(rows, 1):
        meta = row.get("metadata", {}) or {}
        title = meta.get("title", "Unknown title")
        year = meta.get("year", "????")
        author = meta.get("primary_author", "Unknown author")
        sim = row.get("similarity", 0.0)

        snippet = (row.get("text", "") or "").replace("\n", " ")[:280] + "…"

        print(f"\nResult {idx}  •  sim={sim:.3f}")
        print(f"  {title} — {author} ({year})")
        print(f"  {snippet}")


# ────────────────────────────────── main ──────────────────────────────────── #


def main() -> None:
    print("\n🔍  Supabase vector search smoke‑test\n" + "—" * 60)
    for i, q in enumerate(SAMPLE_QUERIES, 1):
        print(f'\nQuery {i + 1}/{len(SAMPLE_QUERIES)}: "{q}"')


        # 1. get the vector
        print("Generating embedding...")
        query_vec = embed(q)  # Python list[float]

        # 2. PostgREST / Postgres expects a *string* like: [0.1,0.2,…]
        vec_literal = "[" + ",".join(f"{x:.6f}" for x in query_vec) + "]"

        # 3. call the RPC
        print("Performing vector search...")
        resp = sb.rpc(
            "search_research_chunks",
            {
                "query_embedding": vec_literal,  # <- NOT the raw text
                "match_count": 5,
                "similarity_threshold": 0.6,
            },
        ).execute()
        
        if getattr(resp, "error", None):
            print(f"   ⚠  RPC failed: {resp.error}\n")
            continue
            
        results = resp.data or []
        print_results(results)

        if i < len(SAMPLE_QUERIES):
            print("\nPausing 2 s before the next query …")
            time.sleep(2)

    print("\n✔  Done")


if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: scripts/migrate_to_supabase.js
################################################################################

import fs from 'fs';
import path from 'path';
import admin from 'firebase-admin';
import { createClient } from '@supabase/supabase-js';
import dotenv from 'dotenv';

// Load environment variables
dotenv.config({ path: path.resolve(process.cwd(), 'server/.env') });

// Initialize Firebase Admin SDK (source)
const serviceAccount = {
  projectId: process.env.FIREBASE_PROJECT_ID,
  clientEmail: process.env.FIREBASE_CLIENT_EMAIL,
  privateKey: process.env.FIREBASE_PRIVATE_KEY?.replace(/\\n/g, '\n'),
};
admin.initializeApp({
  credential: admin.credential.cert(serviceAccount),
});
const db = admin.firestore();

// Initialize Supabase (destination)
const supabase = createClient(
  process.env.SUPABASE_URL || '',
  process.env.SUPABASE_ANON_KEY || ''
);

// Export Firestore data
async function exportFirestoreData() {
  console.log('Exporting data from Firestore...');
  
  const chunks = await db.collection('research_chunks').get();
  const data = chunks.docs.map(doc => ({
    id: doc.id,
    ...doc.data(),
    // Convert Firestore timestamp to ISO string
    created_at: doc.data().createdAt?.toDate().toISOString()
  }));
  
  fs.writeFileSync('chunks_export.json', JSON.stringify(data));
  console.log(`Exported ${data.length} documents to chunks_export.json`);
  return data;
}

// Import to Supabase
async function importToSupabase(data) {
  console.log('Importing data to Supabase...');
  
  // Insert in batches
  const BATCH_SIZE = 500;
  for (let i = 0; i < data.length; i += BATCH_SIZE) {
    const batch = data.slice(i, i + BATCH_SIZE);
    
    console.log(`Importing batch ${Math.floor(i/BATCH_SIZE) + 1}/${Math.ceil(data.length/BATCH_SIZE)}...`);
    
    // Prepare records for Supabase
    const records = batch.map(item => {
      // Remove Firestore-specific fields and rename fields to match Supabase schema
      const { createdAt, embedding, ...rest } = item;
      return {
        ...rest,
        // Use created_at from previous conversion or current date
        created_at: item.created_at || new Date().toISOString()
      };
    });
    
    const { error } = await supabase
      .from('research_chunks')
      .insert(records);
    
    if (error) {
      console.error('Error importing batch:', error);
    } else {
      console.log(`Successfully imported batch ${Math.floor(i/BATCH_SIZE) + 1}`);
    }
  }
  
  console.log('Import completed!');
}

// Main function
async function main() {
  try {
    // Check if export file exists
    if (fs.existsSync('chunks_export.json')) {
      console.log('Using existing export file: chunks_export.json');
      const data = JSON.parse(fs.readFileSync('chunks_export.json'));
      await importToSupabase(data);
    } else {
      const data = await exportFirestoreData();
      await importToSupabase(data);
    }
  } catch (error) {
    console.error('Migration failed:', error);
  }
}

main().catch(console.error);


================================================================================


################################################################################
# File: js-requirements.md
################################################################################

################################################################################
################################################################################
# JavaScript Dependencies for Misophonia Research RAG System

## Core Dependencies

```json
{
  "dependencies": {
    "dotenv": "^16.0.3",
    "fast-glob": "^3.3.3",
    "firebase": "^10.0.0",
    "firebase-admin": "^11.8.0",
    "mammoth": "^1.9.0",
    "node-fetch": "^3.3.1",
    "openai": "^4.9.0",
    "pdfjs-dist": "^3.7.107",
    "express": "^4.18.2",
    "cors": "^2.8.5",
    "axios": "^1.4.0",
    "body-parser": "^1.20.2"
  }
}
```

## Development Dependencies

```json
{
  "devDependencies": {
    "@eslint/js": "^9.22.0",
    "@types/react": "^19.0.10",
    "@types/react-dom": "^19.0.4",
    "@vitejs/plugin-react": "^4.3.4",
    "eslint": "^9.22.0",
    "eslint-plugin-react-hooks": "^5.2.0",
    "eslint-plugin-react-refresh": "^0.4.19",
    "globals": "^16.0.0",
    "netlify-cli": "^13.2.2",
    "vite": "^6.3.1",
    "vite-plugin-pwa": "^1.0.0"
  }
}
```

## Firebase-specific Dependencies

```json
{
  "dependencies": {
    "firebase/app": "included in firebase package",
    "firebase/firestore": "included in firebase package",
    "firebase/functions": "included in firebase package",
    "firebase-admin/app": "included in firebase-admin package",
    "firebase-admin/firestore": "included in firebase-admin package",
    "firebase-functions": "^4.3.0"
  }
}
```

## Installation Instructions

1. These dependencies are already defined in your project's `package.json` file.
2. To install all dependencies, run:
   ```
   npm install
   ```
3. For Firebase Cloud Functions, navigate to the functions directory and run:
   ```
   cd functions
   npm install
   ```

## Notes

- The Firebase client and admin SDKs are separate packages with different use cases:
  - `firebase` is for client-side applications
  - `firebase-admin` is for server-side applications and has elevated privileges

- For the RAG system, the key dependencies are:
  - `openai`: For generating embeddings and AI responses
  - `firebase-admin`: For accessing Firestore vector database
  - `pdfjs-dist`: For processing PDF documents
  - `express`: For the web server interface


================================================================================


################################################################################
# File: netlify/functions/chat.js
################################################################################

import path from 'path';
import dotenv from 'dotenv';
import { OpenAI } from 'openai';

// Load env vars from Python server .env
dotenv.config({ path: path.resolve(process.cwd(), 'server/.env') });

export async function handler(event, context) {
  if (event.httpMethod !== 'POST') {
    return { statusCode: 405, body: 'Method Not Allowed' };
  }
  let body;
  try {
    body = JSON.parse(event.body);
  } catch {
    return { statusCode: 400, body: JSON.stringify({ error: 'Invalid JSON' }) };
  }
  const { messages } = body;
  if (!messages || !Array.isArray(messages)) {
    return { statusCode: 400, body: JSON.stringify({ error: 'Messages array required' }) };
  }
  // Initialize OpenAI client
  const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
  try {
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages,
      max_tokens: 512,
      temperature: 0.7
    });
    const reply = completion.choices?.[0]?.message?.content || '';
    return { statusCode: 200, body: JSON.stringify({ reply }) };
  } catch (err) {
    console.error(err);
    return { statusCode: 500, body: JSON.stringify({ error: 'Error from OpenAI API.' }) };
  }
}


================================================================================


################################################################################
# File: requirements.txt
################################################################################

################################################################################
################################################################################
# Python dependencies for Misophonia Research RAG System

# Core dependencies
firebase-admin>=6.2.0
openai>=1.0.0
numpy>=1.24.0
python-dotenv>=1.0.0
flask>=2.0.0
requests>=2.28.0

# PDF processing
PyPDF2>=3.0.0
unstructured>=0.10.0
pdfminer.six>=20221105

# Utilities
tqdm>=4.65.0
tabulate>=0.9.0
colorama>=0.4.6
argparse>=1.4.0

# Firebase/Google Cloud
google-cloud-firestore>=2.11.0

# For concurrent processing
concurrent-log-handler>=0.9.20

# Supabase integration
supabase~=2.0.0


================================================================================


################################################################################
# File: public/manifest.json
################################################################################

################################################################################
# File: public/manifest.json
################################################################################
{
  "name": "Misophonia Companion",
  "short_name": "Companion",
  "description": "A therapeutic and research PWA for misophonia.",
  "start_url": ".",
  "display": "standalone",
  "background_color": "#f8f6ff",
  "theme_color": "#b2d8d8",
  "icons": [
    {
      "src": "icon-192.png",
      "sizes": "192x192",
      "type": "image/png"
    },
    {
      "src": "icon-512.png",
      "sizes": "512x512",
      "type": "image/png"
    }
  ]
}


================================================================================


################################################################################
# File: index.html
################################################################################

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Vite + React</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>


================================================================================


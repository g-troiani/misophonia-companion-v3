# Concatenated Project Code - Part 1 of 3
# Generated: 2025-05-22 18:55:46
# Root Directory: /Users/gianmariatroiani/Documents/misophonia-companion-v3
================================================================================

# Directory Structure
################################################################################
├── .gitignore
├── README.md
├── eslint.config.js
├── index.html
├── js-requirements.md
├── netlify/
│   └── functions/
│       ├── _utils.js
│       ├── chat.js
│       ├── rag.js
│       ├── research.js
│       └── search.js
├── netlify.toml
├── package.json
├── public/
│   ├── icon-192.png
│   ├── icon-512.png
│   ├── manifest.json
│   └── vite.svg
├── requirements.txt
├── scripts/
│   ├── find_duplicates.py
│   ├── old/
│   ├── pipeline_modular.py
│   ├── rag_local_web_app.py
│   ├── stages/
│   │   ├── __init__.py
│   │   ├── chunk_text.py
│   │   ├── db_upsert.py
│   │   ├── embed_vectors.py
│   │   ├── extract_clean.py
│   │   └── llm_enrich.py
│   ├── test_vector_search.py
│   └── topic_filter_and_title.py
├── server/
│   ├── index.js
│   └── package.json
├── src/
│   ├── App.css
│   ├── App.jsx
│   ├── RagAssistant.jsx
│   ├── TermsModal.jsx
│   ├── assets/
│   │   ├── bot-avatar.png
│   │   ├── react.svg
│   │   └── user-avatar.png
│   ├── index.css
│   └── main.jsx
├── verify_pdf_processing.py
└── vite.config.js


================================================================================


################################################################################
# File: scripts/stages/extract_clean.py
################################################################################

# File: scripts/stages/extract_clean.py

#!/usr/bin/env python3
"""
Stage 1-2 — *Extract PDF → clean text → logical sections*  
Outputs `<n>.json` (+ a pretty `.txt`) in `documents/research/{json,txt}/`.
This file is **fully self-contained** – no import from `stages.common`.
"""
from __future__ import annotations

import json, logging, os, pathlib, re, sys
from collections import Counter
from datetime import datetime
from textwrap import dedent
from typing import Any, Dict, List, Sequence

# ─── helpers formerly in common.py ──────────────────────────────────
_LATIN1_REPLACEMENTS = {  # windows-1252 fallback
    0x82: "‚",
    0x83: "ƒ",
    0x84: "„",
    0x85: "…",
    0x86: "†",
    0x87: "‡",
    0x88: "ˆ",
    0x89: "‰",
    0x8A: "Š",
    0x8B: "‹",
    0x8C: "Œ",
    0x8E: "Ž",
    0x91: "'",
    0x92: "'",
    0x93: '"',
    0x94: '"',
    0x95: "•",
    0x96: "–",
    0x97: "—",
    0x98: "˜",
    0x99: "™",
    0x9A: "š",
    0x9B: "›",
    0x9C: "œ",
    0x9E: "ž",
    0x9F: "Ÿ",
}
_TRANSLATE_LAT1 = str.maketrans(_LATIN1_REPLACEMENTS)
def latin1_scrub(txt:str)->str: return txt.translate(_TRANSLATE_LAT1)

_WS_RE = re.compile(r"[ \t]+\n")
def normalize_ws(txt:str)->str:
    return re.sub(r"[ \t]{2,}", " ", _WS_RE.sub("\n", txt)).strip()

def pct_ascii_letters(txt:str)->float:
    letters=sum(ch.isascii() and ch.isalpha() for ch in txt)
    return letters/max(1,len(txt))

def needs_ocr(txt:str)->bool:
    return (not txt.strip()) or ("\x00" in txt) or (pct_ascii_letters(txt)<0.15)

def scrub_nuls(obj:Any)->Any:
    if isinstance(obj,str):  return obj.replace("\x00","")
    if isinstance(obj,list): return [scrub_nuls(x) for x in obj]
    if isinstance(obj,dict): return {k:scrub_nuls(v) for k,v in obj.items()}
    return obj

def _make_converter()->DocumentConverter:
    opts = PdfPipelineOptions()
    opts.do_ocr = True
    return DocumentConverter({InputFormat.PDF: PdfFormatOption(pipeline_options=opts)})
# ───────────────────────────────────────────────────────────────────

from dotenv import load_dotenv
from openai import OpenAI
from supabase import create_client            # only needed if you later push rows
from tqdm import tqdm

# ─── optional heavy deps ----------------------------------------------------
try:
    import PyPDF2                             # raw fallback
    from unstructured.partition.pdf import partition_pdf
    from docling.datamodel.base_models import InputFormat
    from docling.datamodel.pipeline_options import PdfPipelineOptions
    from docling.document_converter import DocumentConverter, PdfFormatOption
except ImportError as exc:                    # pragma: no cover
    sys.exit(f"Missing dependency → {exc}.  Run `pip install -r requirements.txt`.")

# ─── env / paths ------------------------------------------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

REPO_ROOT  = pathlib.Path(__file__).resolve().parents[2]
TXT_DIR    = REPO_ROOT / "documents" / "research" / "txt"
JSON_DIR   = REPO_ROOT / "documents" / "research" / "json"
TXT_DIR.mkdir(parents=True, exist_ok=True)
JSON_DIR.mkdir(parents=True, exist_ok=True)

GPT_META_MODEL = "gpt-4.1-mini-2025-04-14"

log = logging.getLogger(__name__)

# ╔══════════════════════════════════════════════════════════════════════════╗
# ║                           helper utilities                              ║
# ╚══════════════════════════════════════════════════════════════════════════╝

_LATIN = {0x91:"'",0x92:"'",0x93:'"',0x94:'"',0x95:"•",0x96:"–",0x97:"—"}
_DOI_RE = re.compile(r"\b10\.\d{4,9}/[-._;()/:A-Z0-9]+", re.I)
_JOURNAL_RE = re.compile(r"(Journal|Proceedings|Annals|Psychiatry|Psychology|Nature|Science)[^\n]{0,120}", re.I)
_ABSTRACT_RE = re.compile(r"(?<=\bAbstract\b[:\s])(.{50,2000}?)(?:\n[A-Z][^\n]{0,60}\n|\Z)", re.S)
_KEYWORDS_RE = re.compile(r"\bKey\s*words?\b[:\s]*(.+)", re.I)
_HEADING_TYPES = {"title","heading","header","subtitle","subheading"}

# ─── minimal bib helpers ───────────────────────────────────────────────────
def _authors(val)->List[str]:
    if val is None: return []
    if isinstance(val,list): return [str(x).strip() for x in val if x]
    return re.split(r"\s*,\s*|\s+and\s+", str(val).strip())

_DEF_META = {
    "doc_type":"scientific paper", "title":None,"authors":[],"year":None,"journal":None,"doi":None,
    "abstract":None,"keywords":[],"research_topics":[],
    "peer_reviewed":None,"open_access":None,"license":None,"open_access_status":None,
}

def merge_meta(*sources:Dict[str,Any])->Dict[str,Any]:
    m=_DEF_META.copy()
    for src in sources:
        for k,v in src.items():
            if v not in (None,"",[],{}): m[k]=v
    m["authors"]=_authors(m["authors"])
    m["keywords"]=m["keywords"] or []
    m["research_topics"]=m["research_topics"] or []
    return m

def bib_from_filename(pdf:pathlib.Path)->Dict[str,Any]:
    s=pdf.stem
    m=re.search(r"\b(19|20)\d{2}\b",s)
    yr=int(m.group(0)) if m else None
    if m:
        auth=s[:m.start()].strip(); title=s[m.end():].strip(" -_")
    else:
        parts=s.split(" ",1); auth=parts[0]; title=parts[1] if len(parts)==2 else None
    return {"authors":[auth] if auth else [],"year":yr,"title":title}

def bib_from_header(txt:str)->Dict[str,Any]:
    md={}
    if m:=_DOI_RE.search(txt): md["doi"]=m.group(0)
    if m:=_JOURNAL_RE.search(txt): md["journal"]=" ".join(m.group(0).split())
    if m:=_ABSTRACT_RE.search(txt): md["abstract"]=" ".join(m.group(1).split())
    if m:=_KEYWORDS_RE.search(txt):
        kws=[k.strip(" ;.,") for k in re.split(r"[;,]",m.group(1)) if k.strip()]
        md["keywords"]=kws; md["research_topics"]=kws
    return md

# ─── GPT enrichment ────────────────────────────────────────────────────────
def _first_words(txt:str,n:int=3000)->str: return " ".join(txt.split()[:n])

def gpt_metadata(text:str)->Dict[str,Any]:
    if not OPENAI_API_KEY: return {}
    cli=OpenAI(api_key=OPENAI_API_KEY)
    prompt=dedent(f"""
        Extract metadata (doc_type,title,authors,year,journal,doi,abstract,
        keywords,research_topics,peer_reviewed,open_access,license,
        open_access_status) from this paper and return one JSON object.
        Text:
        {text}
    """)
    rsp=cli.chat.completions.create(model=GPT_META_MODEL,temperature=0,
        messages=[{"role":"system","content":"Structured metadata extractor"},
                  {"role":"user","content":prompt}])
    txt=rsp.choices[0].message.content
    json_txt=re.search(r"{[\s\S]*}",txt)
    return json.loads(json_txt.group(0) if json_txt else "{}")        # type: ignore[arg-type]

# ╔══════════════════════════════════════════════════════════════════════════╗
# ║                         core extraction logic                            ║
# ╚══════════════════════════════════════════════════════════════════════════╝

def _docling_elements(doc)->List[Dict[str,Any]]:
    p:Dict[int,List[Dict[str,str]]]={}
    for it,_ in doc.iterate_items():
        pg=getattr(it.prov[0],"page_no",1)
        lbl=(getattr(it,"label","") or "").upper()
        typ="heading" if lbl in ("TITLE","SECTION_HEADER","HEADER") else \
            "list_item" if lbl=="LIST_ITEM" else \
            "table" if lbl=="TABLE" else "paragraph"
        p.setdefault(pg,[]).append({"type":typ,"text":str(getattr(it,"text",it)).strip()})
    out=[]
    for pn in sorted(p):
        out.append({"section":f"Page {pn}","page_number":pn,
                    "text":"\n".join(el["text"] for el in p[pn]),
                    "elements":p[pn]})
    return out

def _unstructured_elements(pdf:pathlib.Path)->List[Dict[str,Any]]:
    els=partition_pdf(str(pdf),strategy="hi_res")
    pages:Dict[int,List[Dict[str,str]]]={}
    for el in els:
        pn=getattr(el.metadata,"page_number",1)
        pages.setdefault(pn,[]).append({"type":el.category or "paragraph",
                                        "text":normalize_ws(str(el))})
    return [{"section":f"Page {pn}","page_number":pn,
             "text":"\n".join(e["text"] for e in it),"elements":it}
            for pn,it in sorted(pages.items())]

def _pypdf_elements(pdf:pathlib.Path)->List[Dict[str,Any]]:
    out=[]
    with open(pdf,"rb") as fh:
        for pn,pg in enumerate(PyPDF2.PdfReader(fh).pages,1):
            raw=normalize_ws(pg.extract_text() or "")
            paras=[p for p in re.split(r"\n{2,}",raw) if p.strip()]
            out.append({"section":f"Page {pn}","page_number":pn,
                        "text":"\n".join(paras),
                        "elements":[{"type":"paragraph","text":p} for p in paras]})
    return out

def _group_by_headings(page_secs:Sequence[Dict[str,Any]])->List[Dict[str,Any]]:
    out=[]; cur=None; last=1
    for s in page_secs:
        pn=s.get("page_number",last); last=pn
        for el in s.get("elements",[]):
            kind=(el.get("type")or"").lower(); txt=el.get("text","").strip()
            if kind in _HEADING_TYPES and txt:
                if cur: out.append(cur)
                cur={"section":txt,"page_start":pn,"page_end":pn,"elements":[el.copy()]}
            else:
                if not cur:
                    cur={"section":"(untitled)","page_start":pn,"page_end":pn,"elements":[]}
                cur["elements"].append(el.copy()); cur["page_end"]=pn
    if cur and not out: out.append(cur)
    return out

def _sections_md(sections:List[Dict[str,Any]])->str:
    md=[]
    for s in sections:
        md.append(f"# {s.get('section','(Untitled)')}")
        for el in s.get("elements",[]): md.append(el.get("text",""))
        md.append("")
    return "\n".join(md).strip()

# ╔══════════════════════════════════════════════════════════════════════════╗
# ║                         Inline extract_pdf from common                   ║
# ╚══════════════════════════════════════════════════════════════════════════╝

def extract_pdf(
    pdf: pathlib.Path,
    txt_dir: pathlib.Path,
    json_dir: pathlib.Path,
    conv: DocumentConverter,
    *,
    overwrite: bool = False,
    ocr_lang: str = "eng",
    keep_markup: bool = True,
    docling_only: bool = False,
    stats: Counter | None = None,
    enrich_llm: bool = True,
) -> pathlib.Path:
    """Return path to JSON payload ready for downstream stages."""
    txt_path = txt_dir / f"{pdf.stem}.txt"
    json_path = json_dir / f"{pdf.stem}.json"
    if not overwrite and txt_path.exists() and json_path.exists():
        return json_path

    # –– 1. Docling
    page_secs: List[Dict[str, Any]] = []
    bundle = None
    try:
        bundle = conv.convert(str(pdf))
        if keep_markup:
            page_secs = _docling_elements(bundle.document)
        else:
            full = bundle.document.export_to_text(page_break_marker="\f")
            page_secs = [{
                "section": "Full document",
                "page_number": 1,
                "text": full,
                "elements": [{"type": "paragraph", "text": full}],
            }]
    except Exception as exc:
        log.warning("Docling failed on %s → %s", pdf.name, exc)

    # –– 2. Unstructured fallback
    if not page_secs and not docling_only:
        try:
            page_secs = _unstructured_elements(pdf)
        except Exception as exc:
            log.warning("unstructured failed on %s → %s", pdf.name, exc)

    # –– 3. PyPDF last resort
    if not page_secs and not docling_only:
        log.info("PyPDF2 fallback on %s", pdf.name)
        page_secs = _pypdf_elements(pdf)

    if not page_secs:
        raise RuntimeError("No text extracted from PDF")

    # Latin-1 scrub + OCR repair
    for sec in page_secs:
        sec["text"] = latin1_scrub(sec.get("text", ""))
        for el in sec.get("elements", []):
            el["text"] = latin1_scrub(el.get("text", ""))
        pn = sec.get("page_number")
        need_ocr_before = bool(pn and needs_ocr(sec["text"]))
        if need_ocr_before:
            try:
                from pdfplumber import open as pdfopen
                import pytesseract

                with pdfopen(str(pdf)) as doc:
                    pil = doc.pages[pn - 1].to_image(resolution=300).original
                ocr_txt = normalize_ws(
                    pytesseract.image_to_string(pil, lang=ocr_lang)
                )
                if ocr_txt:
                    sec["text"] = ocr_txt
                    sec["elements"] = [
                        {"type": "paragraph", "text": p}
                        for p in re.split(r"\n{2,}", ocr_txt)
                        if p.strip()
                    ]
                    if stats is not None:
                        stats["ocr_pages"] += 1
            except Exception:
                pass

    # when markup is disabled we already have final sections
    logical_secs = (
        _group_by_headings(page_secs) if keep_markup else page_secs
    )

    # CRITICAL: Ensure EVERY section has a 'text' field
    # This is required by the chunking stage
    for sec in logical_secs:
        if 'elements' in sec:
            # Build text from elements if not already present
            element_texts = []
            for el in sec.get("elements", []):
                el_text = el.get("text", "")
                # Skip internal docling metadata that starts with self_ref=
                if el_text and not el_text.startswith('self_ref='):
                    element_texts.append(el_text)
            
            # Always set text field (overwrite if exists to ensure consistency)
            sec["text"] = "\n".join(element_texts)
        elif 'text' not in sec:
            # Fallback for sections without elements
            sec["text"] = ""
    
    # Also ensure page_secs have text (for debugging/consistency)
    for sec in page_secs:
        if 'text' not in sec and 'elements' in sec:
            element_texts = []
            for el in sec.get("elements", []):
                el_text = el.get("text", "")
                if el_text and not el_text.startswith('self_ref='):
                    element_texts.append(el_text)
            sec["text"] = "\n".join(element_texts)

    header_txt = " ".join(s["text"] for s in page_secs[:2])[:8000]
    heuristic_meta = merge_meta(
        bundle.document.metadata.model_dump()
        if "bundle" in locals() and hasattr(bundle.document, "metadata")
        else {},
        bib_from_filename(pdf),
        bib_from_header(header_txt),
    )

    # single GPT call for **all** metadata fields -----------------------
    llm_meta: Dict[str, Any] = {}
    if enrich_llm and OPENAI_API_KEY:
        try:
            oa = OpenAI(api_key=OPENAI_API_KEY)
            llm_meta = gpt_metadata(
                _first_words(" ".join(s["text"] for s in logical_secs))
            )
        except Exception as exc:
            log.warning("LLM metadata extraction failed on %s → %s", pdf.name, exc)

    meta = merge_meta(heuristic_meta, llm_meta)

    payload = {
        **meta,
        "created_at": datetime.utcnow().isoformat() + "Z",
        "source_pdf": str(pdf.resolve()),
        "sections": logical_secs,
    }

    json_path.parent.mkdir(parents=True, exist_ok=True)
    txt_path.parent.mkdir(parents=True, exist_ok=True)
    json_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), "utf-8")

    # --- Markdown serialisation (rich) ---
    md_text = _sections_md(logical_secs) if keep_markup else "\n".join(
        "# " + s.get("section", "(No title)") + "\n\n" + s.get("text", "") for s in logical_secs
    )
    txt_path.write_text(md_text, "utf-8")

    if stats is not None:
        stats["processed"] += 1

    return json_path

# ╔══════════════════════════════════════════════════════════════════════════╗
# ║                          public entry-point                             ║
# ╚══════════════════════════════════════════════════════════════════════════╝
# ---------------------------------------------------------------------------
#  Stage-1/2 wrapper (now paper-thin)
# ---------------------------------------------------------------------------
def run_one(
    pdf: pathlib.Path,
    *,
    overwrite: bool = False,
    keep_markup: bool = True,
    ocr_lang: str = "eng",
    enrich_llm: bool = True,
    conv=None,
    stats: Counter | None = None,
) -> pathlib.Path:
    conv = conv or _make_converter()
    out = extract_pdf(
        pdf,
        TXT_DIR,
        JSON_DIR,
        conv,
        overwrite=overwrite,
        ocr_lang=ocr_lang,
        keep_markup=keep_markup,
        enrich_llm=enrich_llm,
        stats=stats,
    )
    return out

def json_path_for(pdf:pathlib.Path)->pathlib.Path:
    """Helper if Stage 1 is disabled but its artefact exists."""
    return JSON_DIR / f"{pdf.stem}.json"

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    import argparse
    p=argparse.ArgumentParser(); p.add_argument("pdf",type=pathlib.Path)
    args=p.parse_args()
    print(run_one(args.pdf))


================================================================================


################################################################################
# File: netlify/functions/research.js
################################################################################

// File: netlify/functions/research.js

/**
 * Topic-aware "Research Assistant"
 * Supports OpenAI, Gemini, or Groq depending on AI_PROVIDER env var.
 *
 * POST /.netlify/functions/research
 * Body: { messages:[…], topic?:string }
 *
 * ENV:
 *   AI_PROVIDER          openai | gemini | groq   (default groq)
 *   OPENAI_API_KEY       required if provider=openai
 *   GEMINI_API_KEY       required if provider=gemini
 *   GROQ_API_KEY         required if provider=groq
 */

import 'dotenv/config';
import { Groq } from 'groq-sdk';
import { OpenAI } from 'openai';

// ─────────────────────────────────────────────────────────────
const AI_PROVIDER = (process.env.AI_PROVIDER ?? 'groq').toLowerCase();
// ─────────────────────────────────────────────────────────────

/* -----------------------------------------------------------
 * Common helpers
 * --------------------------------------------------------- */
function badRequest(msg) {
  return {
    statusCode: 400,
    headers: { 'content-type': 'application/json' },
    body: JSON.stringify({ error: msg }),
  };
}

function serverError(msg, details) {
  if (details) console.error(details);
  return {
    statusCode: 500,
    headers: { 'content-type': 'application/json' },
    body: JSON.stringify({ error: msg }),
  };
}

/* ─────────────────────────────────────────────── */
/*  GROQ branch                                    */
/* ─────────────────────────────────────────────── */
async function groqHandler(messages, topic) {
  const apiKey = process.env.GROQ_API_KEY;
  if (!apiKey) return serverError('GROQ_API_KEY missing');

  const groq = new Groq({ apiKey });

  const systemPrompt = topic
    ? `You are a knowledgeable research assistant focusing on "${topic}". Provide accurate, concise information about misophonia research.`
    : 'You are a knowledgeable research assistant on misophonia research.';

  const gMessages = [
    { role: 'system', content: `${systemPrompt}
RULES:
• Do not reveal chain-of-thought.
• Never output <think> … </think> segments.` },
    ...messages,
  ];

  const completion = await groq.chat.completions.create({
    model                 : "qwen-qwq-32b", //'llama-3.3-70b-versatile',
    messages              : gMessages,
    max__tokens : 4096,
    temperature           : 0,
    //stream                : true,
  });

  let reply = completion.choices?.[0]?.message?.content ?? '';
  reply = reply
    .replace(/<think>[\s\S]*?<\/think>/gi, '')
    .replace(/^[\s\S]*?\n\s*?(?=#+ |\S)/, '');

  return {
    statusCode: 200,
    headers: { 'content-type': 'application/json' },
    body: JSON.stringify({
      reply      : reply,
      structured : null,
      provider   : 'groq',
    }),
  };
}

/* ===========================================================
 *  OpenAI branch
 * ========================================================= */
async function openaiHandler(messages, topic) {
  const apiKey = process.env.OPENAI_API_KEY;
  if (!apiKey) return serverError('OPENAI_API_KEY missing');

  const openai = new OpenAI({ apiKey });

  const systemPrompt = topic
    ? `You are a knowledgeable research assistant focusing on "${topic}". Provide accurate, concise information about misophonia research.`
    : 'You are a knowledgeable research assistant on misophonia research.';

  const oaMessages = [
    { role: 'system', content: systemPrompt },
    ...messages,
  ];

  const completion = await openai.chat.completions.create({
    model: 'gpt-4.1-mini-2025-04-14',
    messages: oaMessages,
    max_tokens: 1024,
    temperature: 0,
  });

  return {
    statusCode: 200,
    headers: { 'content-type': 'application/json' },
    body: JSON.stringify({
      reply: completion.choices?.[0]?.message?.content ?? '',
      structured: null,
      provider: 'openai',
    }),
  };
}

/* ===========================================================
 *  Gemini branch
 * ========================================================= */
async function geminiHandler(messages, topic) {
  const apiKey = process.env.GEMINI_API_KEY;
  if (!apiKey) return serverError('GEMINI_API_KEY missing');

  // Build user prompt
  let userPrompt = '';
  if (topic) userPrompt += `Topic: ${topic}\n`;
  userPrompt += messages
    .map(
      (m) => `${m.role === 'user' ? 'User' : 'Assistant'}: ${m.content}`
    )
    .join('\n');

  const resp = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?key=${apiKey}`,
    {
      method: 'POST',
      headers: { 'content-type': 'application/json' },
      body: JSON.stringify({
        contents: [{ role: 'user', parts: [{ text: userPrompt }] }],
        generationConfig: { temperature: 0.7, maxOutputTokens: 1024 },
        tools: [
          {
            function_declarations: [
              {
                name: 'structured_output',
                description:
                  'Return information in a structured JSON format for chat display, with sections, bullet points, and highlights.',
              },
            ],
          },
        ],
      }),
    }
  );

  if (!resp.ok) {
    const errText = await resp.text().catch(() => '');
    return serverError('Error from Gemini API', errText);
  }

  const data = await resp.json();

  let structured = null;
  try {
    const part = data.candidates?.[0]?.content?.parts?.[0];
    if (part?.functionCall?.name === 'structured_output') {
      structured = JSON.parse(part.functionCall.args.json || '{}');
    }
  } catch {
    // swallow JSON parse errors – keep structured=null
  }

  const reply =
    data.candidates?.[0]?.content?.parts?.[0]?.text ?? '';

  return {
    statusCode: 200,
    headers: { 'content-type': 'application/json' },
    body: JSON.stringify({ reply, structured, provider: 'gemini' }),
  };
}

/* ===========================================================
 *  Netlify Function entry-point
 * ========================================================= */
export async function handler(event /* , context */) {
  if (event.httpMethod !== 'POST') {
    return { statusCode: 405, body: 'Method Not Allowed' };
  }

  // ─── parse body ──────────────────────────────────────────
  let body;
  try {
    body = JSON.parse(event.body ?? '{}');
  } catch {
    return badRequest('Invalid JSON');
  }

  const { messages, topic } = body;
  if (!Array.isArray(messages) || messages.length === 0) {
    return badRequest('messages array required');
  }

  try {
    if (AI_PROVIDER === 'gemini') {
      return await geminiHandler(messages, topic);
    }
    if (AI_PROVIDER === 'openai') {
      return await openaiHandler(messages, topic);
    }
    return await groqHandler(messages, topic);
  } catch (err) {
    return serverError(
      `Unexpected ${AI_PROVIDER} handler failure`,
      err
    );
  }
}


================================================================================


################################################################################
# File: src/TermsModal.jsx
################################################################################

import React, { useState } from 'react';

export default function TermsModal({ onAccept }) {
  const [checked, setChecked] = useState(false);
  return (
    <div style={{ position:'fixed', top:0, left:0, right:0, bottom:0, backgroundColor:'rgba(0,0,0,0.7)', zIndex:1000, display:'flex', alignItems:'center', justifyContent:'center', border:'4px dashed red' }}>
      <div style={{ background:'#fff', padding:'2rem', maxWidth:'600px', width:'80%', boxSizing:'border-box', maxHeight:'80vh', overflowY:'auto', borderRadius:'8px', display:'flex', flexDirection:'column', alignItems:'center', border:'4px solid blue' }}>
        <h2>Terms and Conditions</h2>
        <div style={{ overflowY:'auto', maxHeight:'50vh', textAlign:'left', width:'100%' }}>
          <p><strong>Effective Date:</strong> [Insert Date]</p>
          <p><strong>1. Acceptance of Terms</strong> By accessing or using the Misophonia Companion application ('App'), you agree to be bound by these Terms and Conditions ('Terms'). If you do not agree to these Terms, do not use the App.</p>
          <p><strong>2. Nature of the App</strong> Misophonia Companion is a digital wellness and research guide. It is intended for informational, educational, and personal support purposes only. The App does not provide medical advice, diagnosis, or treatment. It is not a licensed healthcare service and is not intended to replace professional consultation.</p>
          <p><strong>3. Eligibility</strong> You must be at least 16 years old to use the App. If you are under 18, you confirm that you have received parental or guardian consent.</p>
          <p><strong>4. Privacy and Data</strong> We respect your privacy. Our Privacy Policy explains how we collect, use, and protect your information. By using the App, you agree to the terms of our Privacy Policy.</p>
          <p>No health information (as defined under HIPAA) is collected without explicit consent.</p>
          <p>Conversations may be stored anonymously for product improvement unless you opt out.</p>
          <p>We do not sell or share your data with third parties for advertising purposes.</p>
          <p><strong>5. User Conduct</strong> You agree to use the App responsibly and not to misuse the services, including but not limited to:</p>
          <ul>
            <li>Attempting to reverse-engineer, copy, or modify the App</li>
            <li>Submitting harmful, abusive, or misleading content</li>
            <li>Interfering with the operation or integrity of the App</li>
          </ul>
          <p><strong>6. Limitation of Liability</strong> To the fullest extent permitted by law, Misophonia Companion and its creators are not liable for any direct, indirect, incidental, or consequential damages resulting from the use—or inability to use—the App. This includes, but is not limited to, psychological distress, loss of data, or misinterpretation of information.</p>
          <p><strong>7. Modifications to the Terms</strong> We reserve the right to modify these Terms at any time. Continued use of the App after changes means you accept the new Terms.</p>
          <p><strong>8. Termination</strong> We may suspend or terminate access to the App at our discretion, without notice, for conduct that violates these Terms.</p>
          <p><strong>9. Governing Law</strong> These Terms are governed by the laws of the state of [Your State], without regard to conflict of laws principles.</p>
          <p><strong>10. Contact</strong> For questions or concerns about these Terms, please email: [Your Contact Email]</p>
        </div>
        <h2>Privacy Policy</h2>
        <div style={{ overflowY:'auto', maxHeight:'30vh', textAlign:'left', width:'100%' }}>
          <p><strong>Effective Date:</strong> [Insert Date]</p>
          <p><strong>1. Introduction</strong> This Privacy Policy describes how Misophonia Companion ('we', 'us', 'our') collects, uses, and protects your personal information when you use our web and mobile application ('App').</p>
          <p><strong>2. Information We Collect</strong></p>
          <p><strong>•</strong> <em>User-Provided Information</em>: When you create an account or interact with the App, you may provide personal information such as your email address and preferences.</p>
          <p><strong>•</strong> <em>Anonymous Usage Data</em>: We may collect anonymized data on app usage, interactions, and conversation content to improve the experience.</p>
          <p><strong>•</strong> <em>Optional Personal Logs</em>: Users may opt in to mood tracking, journaling, or trigger tagging. This data is stored securely and only accessible to the user unless explicitly shared.</p>
          <p><strong>3. How We Use Your Information</strong> To provide and improve our services; to personalize your user experience; for anonymized research and development; to comply with legal obligations if required.</p>
          <p><strong>4. Data Storage and Security</strong> We use secure, encrypted servers and industry-standard protocols to store your data. Sensitive data is never shared with third parties without consent. You may request deletion of your account and data at any time.</p>
          <p><strong>5. Sharing and Disclosure</strong> We do not sell or rent your personal information. We may share anonymized data with research collaborators or analytics partners. We may disclose information if legally compelled (e.g., court order).</p>
          <p><strong>6. Your Rights and Choices</strong> You may update or delete your personal information from your profile. You may opt out of data collection for research purposes. You may contact us for a copy of any personal data we’ve stored.</p>
          <p><strong>7. Children’s Privacy</strong> Our App is not intended for children under 16. We do not knowingly collect data from individuals under this age without parental consent.</p>
          <p><strong>8. Changes to This Policy</strong> We may revise this policy from time to time. Users will be notified of significant changes. Continued use of the App indicates acceptance of the updated policy.</p>
          <p><strong>9. Contact Us</strong> If you have questions or requests related to this Privacy Policy, please contact us at: [Your Contact Email]</p>
        </div>
        <label style={{ display:'block', margin:'1rem 0', textAlign:'center', width:'100%' }}>
          <input type='checkbox' checked={checked} onChange={e => setChecked(e.target.checked)} /> I have read and agree to the Terms and Conditions and Privacy Policy
        </label>
        <button disabled={!checked} onClick={() => { localStorage.setItem('termsAccepted','true'); onAccept(); }} style={{ marginTop:'1.5rem', padding:'0.5rem 1rem' }}>Continue</button>
      </div>
    </div>
  );
}


================================================================================


################################################################################
# File: server/index.js
################################################################################

import express from 'express';
import cors from 'cors';
import dotenv from 'dotenv';
import { OpenAI } from 'openai';
import fetch from 'node-fetch';
import httpProxy from 'http-proxy';

dotenv.config();

// Set AI provider here - "openai" or "gemini"
const AI_PROVIDER = process.env.AI_PROVIDER || "openai";

const app = express();

//app.use(cors());  --> This is the default behavior
app.use(cors({
  origin: [
    '[https://misophonia-guide.netlify.app](https://misophonia-guide.netlify.app)',
    'http://localhost:5173'
  ]
}));
app.use(express.json());

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// Research Assistant endpoint (handles both Gemini and OpenAI)
app.post('/api/gemini', async (req, res) => {
  try {
    const { messages, topic } = req.body;
    
    if (!messages || !Array.isArray(messages)) {
      return res.status(400).json({ error: 'Messages array required.' });
    }
    
    // Using OpenAI
    if (AI_PROVIDER === "openai") {
      // Format messages for OpenAI
      const systemPrompt = topic 
        ? `You are a knowledgeable research assistant focusing on the topic: ${topic}. Provide information about misophonia research.`
        : "You are a knowledgeable research assistant on misophonia research.";
      
      const openaiMessages = [
        { role: 'system', content: systemPrompt },
        ...messages
      ];
      
      const completion = await openai.chat.completions.create({
        model: 'gpt-4o',
        messages: openaiMessages,
        max_tokens: 1024,
        temperature: 0.7
      });
      
      return res.json({
        reply: completion.choices[0]?.message?.content || '',
        structured: null,
        provider: 'openai'
      });
    }
    
    // Using Gemini
    else {
      const apiKey = process.env.GEMINI_API_KEY;
      if (!apiKey) {
        return res.status(500).json({ error: 'GEMINI_API_KEY not set in server/.env.' });
      }
      
      // Compose prompt for Gemini
      let userPrompt = '';
      if (topic && typeof topic === 'string') {
        userPrompt += `Topic: ${topic}\n`;
      }
      userPrompt += messages.map(m => `${m.role === 'user' ? 'User' : 'Assistant'}: ${m.content}`).join('\n');
      
      // Gemini API call
      const geminiRes = await fetch('https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?key=' + apiKey, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          contents: [{ role: 'user', parts: [{ text: userPrompt }] }],
          generationConfig: {
            temperature: 0.7,
            maxOutputTokens: 1024,
          },
          tools: [
            { "function_declarations": [
              { "name": "structured_output", "description": "Return information in a structured JSON format for chat display, with sections, bullet points, and highlights." }
            ]}
          ]
        })
      });
      
      if (!geminiRes.ok) {
        const err = await geminiRes.text();
        return res.status(500).json({ error: 'Error from Gemini API', details: err });
      }
      
      const geminiData = await geminiRes.json();
      // Parse structured output if present
      let structured = null;
      if (geminiData.candidates && geminiData.candidates[0]?.content?.parts) {
        const part = geminiData.candidates[0].content.parts[0];
        if (part.functionCall && part.functionCall.name === 'structured_output') {
          try {
            structured = JSON.parse(part.functionCall.args.json || '{}');
          } catch {
            structured = part.functionCall.args;
          }
        }
      }
      
      return res.json({
        reply: geminiData.candidates?.[0]?.content?.parts?.[0]?.text || '',
        structured,
        provider: 'gemini'
      });
    }
  } catch (error) {
    console.error(error);
    res.status(500).json({ error: `Error from ${AI_PROVIDER === "openai" ? "OpenAI" : "Gemini"} API.` });
  }
});

// Keep existing OpenAI chat endpoint
app.post('/api/chat', async (req, res) => {
  try {
    const { messages } = req.body;
    if (!messages || !Array.isArray(messages)) {
      return res.status(400).json({ error: 'Messages array required.' });
    }
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages,
      max_tokens: 512,
      temperature: 0.7
    });
    const reply = completion.choices[0]?.message?.content || '';
    res.json({ reply });
  } catch (error) {
    console.error(error);
    res.status(500).json({ error: 'Error from OpenAI API.' });
  }
});

// ───────────────────────── RAG proxy ─────────────────────────
const proxy = httpProxy.createProxyServer({ changeOrigin: true });

app.post('/api/rag', (req, res) => {
  proxy.web(req, res, { target: 'http://localhost:8080/search' }, err => {
    console.error(err);
    res.status(502).json({ error: 'RAG service unreachable' });
  });
});

const PORT = process.env.PORT || 3001;
app.listen(PORT, () => {
  console.log(`Server running with ${AI_PROVIDER.toUpperCase()} on port ${PORT}`);
});


================================================================================


################################################################################
# File: scripts/stages/chunk_text.py
################################################################################

# File: scripts/stages/chunk_text.py

#!/usr/bin/env python3
"""
Stage 5 — split the clean body text into **overlapping 768-token windows**
and dump a side-car `<n>_chunks.json`.
"""
from __future__ import annotations
import json, logging, math, pathlib, re
from typing import Dict, List, Sequence, Tuple, Any

# ─── helpers to split into token windows ───────────────────────────
def concat_tokens(sections:Sequence[Dict[str,Any]])->Tuple[List[str],List[int]]:
    tokens,page_map=[],[]
    for s in sections:
        # Be defensive - build text if missing
        if 'text' not in s:
            if 'elements' in s:
                text = '\n'.join(
                    el.get('text', '') for el in s['elements'] 
                    if el.get('text', '') and not el.get('text', '').startswith('self_ref=')
                )
            else:
                text = ""
            log.warning(f"Section missing 'text' field, built from elements: {s.get('section', 'untitled')}")
        else:
            text = s.get("text", "")
            
        words=text.split()
        tokens.extend(words)
        
        # Handle both page_number (from page_secs) and page_start/page_end (from logical_secs)
        if 'page_number' in s:
            # Single page section
            page_map.extend([s['page_number']]*len(words))
        elif 'page_start' in s and 'page_end' in s:
            # Multi-page section - distribute words across pages
            page_start = s['page_start']
            page_end = s['page_end']
            if page_start == page_end:
                # All on same page
                page_map.extend([page_start]*len(words))
            else:
                # Distribute words evenly across pages
                pages_span = page_end - page_start + 1
                words_per_page = max(1, len(words) // pages_span)
                for i, word in enumerate(words):
                    page = min(page_end, page_start + (i // words_per_page))
                    page_map.append(page)
        else:
            # Fallback to page 1
            page_map.extend([1]*len(words))
            log.warning(f"Section has no page info: {s.get('section', 'untitled')}")
            
    return tokens,page_map

def sliding_chunks(tokens:List[str],page_map:List[int],*,window:int,overlap:int)->List[Dict[str,Any]]:
    step=max(1,window-overlap)
    out,i=[],0
    SENT_END_RE=re.compile(r"[.!?]$")
    while i<len(tokens):
        start,end=i,min(len(tokens),i+window)
        while end<len(tokens) and not SENT_END_RE.search(tokens[end-1]) and end-start<window+256:
            end+=1
        out.append({"chunk_index":len(out),"token_start":start,"token_end":end-1,
                    "page_start":page_map[start],"page_end":page_map[end-1],
                    "text":" ".join(tokens[start:end])})
        if end==len(tokens): break
        i+=step
    return out
# ───────────────────────────────────────────────────────────────────

WINDOW  = 768
OVERLAP = int(WINDOW*0.20)           # 154-token (20 %) overlap
log = logging.getLogger(__name__)

def chunk(json_path:pathlib.Path)->List[Dict[str,any]]:
    root=json_path.with_name(json_path.stem+"_chunks.json")
    if root.exists():                              # idempotent
        return json.loads(root.read_text())
    data=json.loads(json_path.read_text())
    if "sections" not in data:          # <- defensive
        raise RuntimeError(f"{json_path} has no 'sections' key – enrichment step lost context")
    toks,pmap=concat_tokens(data["sections"])
    chunks=sliding_chunks(toks,pmap,window=WINDOW,overlap=OVERLAP)
    if chunks:                                    # ← new guard
        root.write_text(json.dumps(chunks,indent=2,ensure_ascii=False))
        log.info("✓ %s chunks → %s", len(chunks), root.name)
    else:
        log.warning("%s – no chunks produced; file skipped", json_path.name)
    return chunks

if __name__ == "__main__":
    import argparse, logging
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    p=argparse.ArgumentParser(); p.add_argument("json",type=pathlib.Path)
    chunk(p.parse_args().json)


================================================================================


################################################################################
# File: scripts/pipeline_modular.py
################################################################################

#!/usr/bin/env python3
"""
One-command orchestrator that chains the six stand-alone stages.

Usage
-----
    python scripts/pipeline_modular.py [PDF | DIR] [--selection sequential|random] [--cap N]
"""
from __future__ import annotations
import argparse, logging, pathlib, random
from collections import Counter
from rich.console import Console
from tqdm import tqdm

from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption
def _make_converter():
    opts = PdfPipelineOptions()
    opts.do_ocr = True
    return DocumentConverter({InputFormat.PDF: PdfFormatOption(pipeline_options=opts)})

# ─── stage modules ──────────────────────────────────────────────────────────
from stages import extract_clean, llm_enrich, chunk_text, db_upsert, embed_vectors

# ─── toggles (flip at will) ─────────────────────────────────────────────────
RUN_EXTRACT    = True
RUN_LLM_ENRICH = True
RUN_CHUNK      = True
RUN_DB         = True      # ← turn off if you only want local JSON
RUN_EMBED      = True      # ← requires OpenAI + Supabase creds
# ────────────────────────────────────────────────────────────────────────────

log = logging.getLogger("pipeline-modular")
Console().rule("[bold cyan]Misophonia PDF → Vector pipeline (modular)")

def main(src:pathlib.Path, selection:str="sequential", cap:int=0) -> None:
    pdfs = [src] if src.is_file() else sorted(src.rglob("*.pdf"))
    if cap:
        pdfs = random.sample(pdfs, cap) if selection == "random" else pdfs[:cap]

    stats = Counter()
    conv = _make_converter()
    for pdf in tqdm(pdfs, desc="Papers"):
        try:
            # ─── Stage 1-2  Extract + Clean ────────────────────────────────
            json_doc = extract_clean.run_one(pdf, conv=conv) if RUN_EXTRACT else \
                       extract_clean.json_path_for(pdf)
            # ─── Stage 4  LLM metadata enrich ─────────────────────────────
            if RUN_LLM_ENRICH:
                llm_enrich.enrich(json_doc)
            # ─── Stage 5  Chunk ------------------------------------------------
            chunks = chunk_text.chunk(json_doc) if RUN_CHUNK else None
            # ─── Stage 6  DB upsert + (optional) Stage 7 embed  ───────────────
            if RUN_DB:
                db_upsert.upsert(json_doc, chunks, do_embed=RUN_EMBED)
            stats["ok"] += 1
        except Exception as exc:          # noqa: BLE001
            log.exception("✖ %s failed → %s", pdf.name, exc)
            stats["fail"] += 1

    Console().rule("[green]Finished")
    log.info("Processed=%s   Failed=%s", stats["ok"], stats["fail"])


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO,
                        format="%(asctime)s — %(levelname)s — %(message)s")
    p = argparse.ArgumentParser()
    p.add_argument("src", type=pathlib.Path,
                   default=pathlib.Path("documents/research/Global"), nargs="?")
    p.add_argument("--selection", choices=["sequential", "random"],
                   default="sequential")
    p.add_argument("--cap", type=int, default=0)
    args = p.parse_args()
    main(args.src, args.selection, args.cap)


================================================================================


################################################################################
# File: js-requirements.md
################################################################################

################################################################################
################################################################################
# JavaScript Dependencies for Misophonia Research RAG System

## Core Dependencies

```json
{
  "dependencies": {
    "dotenv": "^16.0.3",
    "fast-glob": "^3.3.3",
    "firebase": "^10.0.0",
    "firebase-admin": "^11.8.0",
    "mammoth": "^1.9.0",
    "node-fetch": "^3.3.1",
    "openai": "^4.9.0",
    "pdfjs-dist": "^3.7.107",
    "express": "^4.18.2",
    "cors": "^2.8.5",
    "axios": "^1.4.0",
    "body-parser": "^1.20.2"
  }
}
```

## Development Dependencies

```json
{
  "devDependencies": {
    "@eslint/js": "^9.22.0",
    "@types/react": "^19.0.10",
    "@types/react-dom": "^19.0.4",
    "@vitejs/plugin-react": "^4.3.4",
    "eslint": "^9.22.0",
    "eslint-plugin-react-hooks": "^5.2.0",
    "eslint-plugin-react-refresh": "^0.4.19",
    "globals": "^16.0.0",
    "netlify-cli": "^13.2.2",
    "vite": "^6.3.1",
    "vite-plugin-pwa": "^1.0.0"
  }
}
```

## Firebase-specific Dependencies

```json
{
  "dependencies": {
    "firebase/app": "included in firebase package",
    "firebase/firestore": "included in firebase package",
    "firebase/functions": "included in firebase package",
    "firebase-admin/app": "included in firebase-admin package",
    "firebase-admin/firestore": "included in firebase-admin package",
    "firebase-functions": "^4.3.0"
  }
}
```

## Installation Instructions

1. These dependencies are already defined in your project's `package.json` file.
2. To install all dependencies, run:
   ```
   npm install
   ```
3. For Firebase Cloud Functions, navigate to the functions directory and run:
   ```
   cd functions
   npm install
   ```

## Notes

- The Firebase client and admin SDKs are separate packages with different use cases:
  - `firebase` is for client-side applications
  - `firebase-admin` is for server-side applications and has elevated privileges

- For the RAG system, the key dependencies are:
  - `openai`: For generating embeddings and AI responses
  - `firebase-admin`: For accessing Firestore vector database
  - `pdfjs-dist`: For processing PDF documents
  - `express`: For the web server interface


================================================================================


################################################################################
# File: vite.config.js
################################################################################

import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'
import { VitePWA } from 'vite-plugin-pwa'

// https://vite.dev/config/
export default defineConfig({
  plugins: [
    react(),
    VitePWA({
      registerType: 'autoUpdate',
      workbox: {
        runtimeCaching: [
          {
            urlPattern: /^\/api\/(chat|gemini|rag).*$/,
            handler: 'NetworkOnly',
          },
        ],
      },
      manifest: {
        name: 'Misophonia Companion',
        short_name: 'Companion',
        description: 'A therapeutic and research PWA for misophonia.',
        start_url: '.',
        display: 'standalone',
        background_color: '#f8f6ff',
        theme_color: '#b2d8d8',
        icons: [
          {
            src: 'icon-192.png',
            sizes: '192x192',
            type: 'image/png'
          },
          {
            src: 'icon-512.png',
            sizes: '512x512',
            type: 'image/png'
          }
        ]
      }
    })
  ],
  server: {
    proxy: {
      '/api': {
        target: 'http://localhost:3001',
        changeOrigin: true,
      }
    }
  },
  build: {
    rollupOptions: {
      output: {
        manualChunks: undefined,
      },
    },
  },
  define: {
    __APP_BUILD_TIME__: JSON.stringify(Date.now()),
  },
})


================================================================================


################################################################################
# File: src/index.css
################################################################################

:root {
  font-family: system-ui, Avenir, Helvetica, Arial, sans-serif;
  line-height: 1.5;
  font-weight: 400;

  color-scheme: light dark;
  color: rgba(255, 255, 255, 0.87);
  background-color: #242424;

  font-synthesis: none;
  text-rendering: optimizeLegibility;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

a {
  font-weight: 500;
  color: #646cff;
  text-decoration: inherit;
}
a:hover {
  color: #535bf2;
}

body {
  margin: 0;
  display: flex;
  min-width: 320px;
  min-height: 100vh;
}

h1 {
  font-size: 3.2em;
  line-height: 1.1;
}

button {
  border-radius: 8px;
  border: 1px solid transparent;
  padding: 0.6em 1.2em;
  font-size: 1em;
  font-weight: 500;
  font-family: inherit;
  background-color: #1a1a1a;
  cursor: pointer;
  transition: border-color 0.25s;
}
button:hover {
  border-color: #646cff;
}
button:focus,
button:focus-visible {
  outline: 4px auto -webkit-focus-ring-color;
}

@media (prefers-color-scheme: light) {
  :root {
    color: #213547;
    background-color: #ffffff;
  }
  a:hover {
    color: #747bff;
  }
  button {
    background-color: #f9f9f9;
  }
}


================================================================================


################################################################################
# File: public/manifest.json
################################################################################

################################################################################
# File: public/manifest.json
################################################################################
{
  "name": "Misophonia Companion",
  "short_name": "Companion",
  "description": "A therapeutic and research PWA for misophonia.",
  "start_url": ".",
  "display": "standalone",
  "background_color": "#f8f6ff",
  "theme_color": "#b2d8d8",
  "icons": [
    {
      "src": "icon-192.png",
      "sizes": "192x192",
      "type": "image/png"
    },
    {
      "src": "icon-512.png",
      "sizes": "512x512",
      "type": "image/png"
    }
  ]
}


================================================================================


################################################################################
# File: index.html
################################################################################

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Vite + React</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>


================================================================================


################################################################################
# File: src/main.jsx
################################################################################

import { StrictMode } from 'react'
import { createRoot } from 'react-dom/client'
import './index.css'
import App from './App.jsx'

createRoot(document.getElementById('root')).render(
  <StrictMode>
    <App />
  </StrictMode>,
)


================================================================================


# Concatenated Project Code - Part 3 of 3
# Generated: 2025-05-09 11:36:56
# Root Directory: /Users/gianmariatroiani/Documents/misophonia-companion-v3
================================================================================

################################################################################
# File: src/App.jsx
################################################################################

import './App.css'
import React, { useState, useEffect, useRef } from 'react';
import botAvatar from './assets/bot-avatar.png';
import userAvatar from './assets/user-avatar.png';
import TermsModal from './TermsModal';
import RagAssistant from './RagAssistant.jsx';


function NavBar({ setSection, section }) {
  return (
    <nav className="navbar">
      <button className={section === 'home' ? 'active' : ''} onClick={() => setSection('home')}>
        <span role="img" aria-label="home" style={{marginRight: 6}}>üè†</span> Home
      </button>
      <button className={section === 'chatbot' ? 'active' : ''} onClick={() => setSection('chatbot')}>
        <span role="img" aria-label="chat" style={{marginRight: 6}}>üí¨</span> Let's Talk
      </button>
      <button className={section === 'tools' ? 'active' : ''} onClick={() => setSection('tools')}>
        <span role="img" aria-label="tools" style={{marginRight: 6}}>üß∞</span> Therapeutic Tools
      </button>
      <button className={section === 'research' ? 'active' : ''} onClick={() => setSection('research')}>
        <span role="img" aria-label="research" style={{marginRight: 6}}>üî¨</span> Research Assistant
      </button>
    </nav>
  );
}

const AFFIRMATIONS = [
  "You are safe here.",
  "It's okay to take a break.",
  "Your feelings are valid.",
  "Breathe in calm, breathe out stress.",
  "You are not alone."
];
const SOUNDS = [
  { label: 'Rain', src: 'https://cdn.pixabay.com/audio/2022/07/26/audio_124bfae45e.mp3' },
  { label: 'Forest', src: 'https://cdn.pixabay.com/audio/2022/03/15/audio_115b9d7bfa.mp3' },
  { label: 'White Noise', src: 'https://cdn.pixabay.com/audio/2022/03/15/audio_115b9d7bfa.mp3' }
];

function AffirmationBanner() {
  const [idx] = useState(() => Math.floor(Math.random() * AFFIRMATIONS.length));
  return (
    <div style={{
      background: 'linear-gradient(90deg, #e0e7ef 60%, #f8f6ff 100%)',
      color: '#4b6073',
      borderRadius: '16px',
      margin: '0 0 1.1rem 0',
      padding: '0.7rem 1.2rem',
      fontSize: '1.12rem',
      fontWeight: 500,
      textAlign: 'center',
      boxShadow: '0 1px 6px 0 rgba(31, 38, 135, 0.04)',
      letterSpacing: '0.01em',
      opacity: 0.97
    }}>
      {AFFIRMATIONS[idx]}
    </div>
  );
}

function SoundscapePlayer() {
  const [playing, setPlaying] = useState(false);
  const [muted, setMuted] = useState(false);
  const [soundIdx, setSoundIdx] = useState(0);
  const audioRef = useRef(null);

  const handlePlayPause = () => {
    setPlaying(p => !p);
  };
  const handleMute = () => {
    setMuted(m => !m);
  };
  const handleSoundChange = (e) => {
    setSoundIdx(Number(e.target.value));
    setPlaying(false);
    setTimeout(() => setPlaying(true), 50);
  };

  useEffect(() => {
    if (!audioRef.current) return;
    audioRef.current.muted = muted;
    if (playing) {
      audioRef.current.play();
    } else {
      audioRef.current.pause();
    }
  }, [playing, muted, soundIdx]);

  return (
    <div style={{
      display: 'flex', alignItems: 'center', gap: '0.7rem',
      background: 'rgba(255,255,255,0.88)',
      borderRadius: '13px',
      padding: '0.25rem 0.8rem',
      marginBottom: '1.1rem',
      boxShadow: '0 1px 4px 0 rgba(31, 38, 135, 0.03)',
      fontSize: '1.01rem',
      maxWidth: 320
    }}>
      <span style={{color: '#b2d8d8', fontWeight: 700, fontSize: '1.1rem'}}>Soundscape:</span>
      <select value={soundIdx} onChange={handleSoundChange} style={{borderRadius: 7, border: '1px solid #e0e7ef', background: '#f8f6ff', color: '#4b6073', padding: '0.2rem 0.5rem'}}>
        {SOUNDS.map((s, i) => <option value={i} key={s.label}>{s.label}</option>)}
      </select>
      <button onClick={handlePlayPause} style={{border: 'none', background: 'none', cursor: 'pointer', color: playing ? '#81b0b0' : '#aaa', fontWeight: 700, fontSize: '1.05rem'}}>{playing ? 'Pause' : 'Play'}</button>
      <button onClick={handleMute} style={{border: 'none', background: 'none', cursor: 'pointer', color: muted ? '#aaa' : '#b2d8d8', fontWeight: 700, fontSize: '1.05rem'}}>{muted ? 'Unmute' : 'Mute'}</button>
      <audio ref={audioRef} src={SOUNDS[soundIdx].src} loop preload="auto" style={{display: 'none'}} />
    </div>
  );
}

function App() {
  const [section, setSection] = useState('home');
  const [termsAccepted, setTermsAccepted] = useState(localStorage.getItem('termsAccepted') === 'true');
  if (!termsAccepted) return <TermsModal onAccept={() => setTermsAccepted(true)} />;

  return (
    <>
      <div className="animated-bg">
        <div className="bubble bubble1"></div>
        <div className="bubble bubble2"></div>
        <div className="bubble bubble3"></div>
        <div className="bubble bubble4"></div>
        <div className="bubble bubble5"></div>
        <div className="bubble bubble6"></div>
        <div className="bubble bubble7"></div>
        <div className="bubble bubble8"></div>
      </div>

      {/* MAIN CONTENT BOX */}
      <div className="container">
        <AffirmationBanner />
        <SoundscapePlayer />
        <NavBar setSection={setSection} section={section} />
        {section === 'home' && (
          <div className="card">
            <main>
              <h1 className="title">Welcome to Misophonia Companion</h1>
              <p className="subtitle">A soothing space to manage triggers, support healing, and explore research‚Äîbuilt for both sufferers and professionals.</p>
            </main>
          </div>
        )}
        {section === 'chatbot' && <Chatbot />}
        {section === 'tools' && (
          <div className="card">
            <main>
              <h2>Therapeutic Tools</h2>
              <p>Coming soon: Sound therapy, coping strategies, and relaxation exercises.</p>
            </main>
          </div>
        )}
        {section === 'research' && <RagAssistant />}
      </div>

      {/* NEW: footer lives here, outside .container */}
      <div className="disclaimer-footer">
        Misophonia Companion is not a clinical tool or a substitute for
        professional psychological or medical treatment. It does not provide
        diagnosis, therapy, or crisis intervention. If you are experiencing a
        mental-health emergency, please contact a licensed provider or
        emergency services immediately.
      </div>
    </>
  );
}


function Chatbot() {
  const [messages, setMessages] = useState([
    { sender: 'bot', text: 'Hello! I am your Misophonia Companion. How can I support you today?' }
  ]);
  const [input, setInput] = useState('');
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);

  async function handleSend(e) {
    e.preventDefault();
    if (!input.trim()) return;
    const userMsg = { sender: 'user', text: input };
    setMessages((msgs) => [...msgs, userMsg]);
    setLoading(true);
    setError(null);
    setInput('');
    try {
      const res = await fetch('/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          messages: [
            { role: 'system', content: 'You are a calm, supportive misophonia companion and research assistant.' },
            ...messages.map(m => ({ role: m.sender === 'user' ? 'user' : 'assistant', content: m.text })),
            { role: 'user', content: input }
          ]
        })
      });
      if (!res.ok) throw new Error('API error');
      const data = await res.json();
      setMessages((msgs) => [...msgs, { sender: 'bot', text: data.reply }]);
    } catch (err) {
      setMessages((msgs) => [...msgs, { sender: 'bot', text: 'Sorry, I could not connect to the assistant. Make sure the backend server and your API key are set up.' }]);
      setError('API error');
    } finally {
      setLoading(false);
    }
  }

  return (
    <main>
      <h2>Let's Talk</h2>
      <div className="chatbot-box">
        <div className="chat-messages">
          {messages.map((msg, idx) => (
            <div key={idx} className={msg.sender === 'bot' ? 'msg bot' : 'msg user'} style={{display: 'flex', alignItems: 'flex-end', justifyContent: msg.sender === 'user' ? 'flex-end' : 'flex-start'}}>
              {msg.sender === 'bot' && (
                <img
                  src={botAvatar}
                  alt="Bot"
                  className="chat-avatar"
                  onError={e => { e.target.onerror = null; e.target.src = 'https://ui-avatars.com/api/?name=Bot&background=b2d8d8&color=fff&rounded=true&size=64'; }}
                />
              )}
              <span className="bubble-content">{msg.text}</span>
              {msg.sender === 'user' && (
                <img
                  src={userAvatar}
                  alt="You"
                  className="chat-avatar"
                  onError={e => { e.target.onerror = null; e.target.src = 'https://ui-avatars.com/api/?name=You&background=ffdac1&color=3a3a3a&rounded=true&size=64'; }}
                />
              )}
            </div>
          ))}
          {loading && <div className="msg bot" style={{display: 'flex', alignItems: 'flex-end'}}><img src={botAvatar} alt="Bot" className="chat-avatar" /><span className="bubble-content">Thinking‚Ä¶</span></div>}
        </div>
        <form className="chat-input-row" onSubmit={handleSend}>
          <input
            type="text"
            value={input}
            onChange={e => setInput(e.target.value)}
            placeholder="Type your message..."
            className="chat-input"
            autoFocus
            aria-label="Type your message"
            disabled={loading}
          />
          <button type="submit" className="chat-send" disabled={loading || !input.trim()}>Send</button>
        </form>
        {error && <div style={{ color: '#b22222', marginTop: '0.5rem' }}>
          Make sure your backend server is running and your OpenAI API key is set in <code>server/.env</code>.
        </div>}
      </div>
    </main>
  );
}



// GeminiChatbot: Gemini 2.5 Pro chat with topics and structured output
function GeminiChatbot() {
  const TOPICS = [
    { label: 'Neuroscience', value: 'Neuroscience' },
    { label: 'Genetics', value: 'Genetics' },
    { label: 'Therapy', value: 'Therapy' },
    { label: 'Advocacy', value: 'Advocacy' },
    { label: 'News', value: 'Latest News' },
    { label: 'Free Text', value: '' }
  ];
  const [messages, setMessages] = useState([
    { sender: 'bot', text: 'Hi! I am your Gemini-powered Research Assistant. Select a topic or ask anything about misophonia research.' }
  ]);
  const [input, setInput] = useState('');
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);
  const [selectedTopic, setSelectedTopic] = useState(null);
  const [mode, setMode] = useState('gemini'); // future: allow OpenAI fallback

  async function handleGeminiSend(e, topicOverride) {
    if (e) e.preventDefault();
    const topicVal = topicOverride !== undefined ? topicOverride : selectedTopic;
    const userText = topicVal && topicVal !== '' && topicVal !== 'Free Text' ? topicVal : input;
    if (!userText.trim()) return;
    const userMsg = { sender: 'user', text: userText };
    setMessages((msgs) => [...msgs, userMsg]);
    setLoading(true);
    setError(null);
    setInput('');
    setSelectedTopic(null);
    try {
      const res = await fetch('/api/gemini', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          messages: messages.map(m => ({ role: m.sender === 'user' ? 'user' : 'assistant', content: m.text })).concat([{ role: 'user', content: userText }]),
          topic: topicVal && topicVal !== 'Free Text' ? topicVal : undefined
        })
      });
      if (!res.ok) throw new Error('API error');
      const data = await res.json();
      setMessages((msgs) => [...msgs, { sender: 'bot', text: data.reply, structured: data.structured }]);
    } catch (err) {
      setMessages((msgs) => [...msgs, { sender: 'bot', text: 'Sorry, Gemini API error. Check your backend and API key.' }]);
      setError('API error');
    } finally {
      setLoading(false);
    }
  }

  function renderStructured(structured) {
    if (!structured) return null;
    // Example structure: { sections: [{ title, bullets, highlights, ... }], ... }
    return (
      <div className="gemini-structured">
        {structured.sections && structured.sections.map((sec, i) => (
          <div key={i} className="g-section">
            {sec.title && <div className="g-title">{sec.title}</div>}
            {sec.highlights && Array.isArray(sec.highlights) && (
              <ul className="g-highlights">{sec.highlights.map((h, j) => <li key={j} className="g-highlight">{h}</li>)}</ul>
            )}
            {sec.bullets && Array.isArray(sec.bullets) && (
              <ul className="g-bullets">{sec.bullets.map((b, j) => <li key={j}>{b}</li>)}</ul>
            )}
            {sec.text && <div className="g-text">{sec.text}</div>}
          </div>
        ))}
        {structured.extra && <div className="g-extra">{structured.extra}</div>}
      </div>
    );
  }

  return (
    <div className="gemini-chatbot">
      <div className="gemini-toggle-row">
        <button
          className={mode === 'gemini' ? 'toggle-active' : ''}
          onClick={() => setMode('gemini')}
        >Gemini 2.5 Pro</button>
        {/* <button
          className={mode === 'openai' ? 'toggle-active' : ''}
          onClick={() => setMode('openai')}
        >OpenAI</button> */}
      </div>
      <div className="gemini-topic-row">
        {TOPICS.map(t => (
          <button
            key={t.label}
            className={selectedTopic === t.value ? 'topic-btn selected' : 'topic-btn'}
            onClick={() => {
              setSelectedTopic(t.value);
              if (t.value && t.value !== 'Free Text') handleGeminiSend(null, t.value);
            }}
            disabled={loading}
          >{t.label}</button>
        ))}
      </div>
      <div className="chat-messages">
        {messages.map((msg, idx) => (
          <div key={idx} className={msg.sender === 'bot' ? 'msg bot' : 'msg user'} style={{display: 'flex', alignItems: 'flex-end', justifyContent: msg.sender === 'user' ? 'flex-end' : 'flex-start'}}>
            {msg.sender === 'bot' && (
              <img
                src={botAvatar}
                alt="Bot"
                className="chat-avatar"
                onError={e => { e.target.onerror = null; e.target.src = 'https://ui-avatars.com/api/?name=Bot&background=b2d8d8&color=fff&rounded=true&size=64'; }}
              />
            )}
            <span className="bubble-content">
              {msg.structured ? renderStructured(msg.structured) : msg.text}
            </span>
            {msg.sender === 'user' && (
              <img
                src={userAvatar}
                alt="You"
                className="chat-avatar"
                onError={e => { e.target.onerror = null; e.target.src = 'https://ui-avatars.com/api/?name=You&background=ffdac1&color=3a3a3a&rounded=true&size=64'; }}
              />
            )}
          </div>
        ))}
        {loading && <div className="msg bot" style={{display: 'flex', alignItems: 'flex-end'}}><img src={botAvatar} alt="Bot" className="chat-avatar" /><span className="bubble-content">Thinking‚Ä¶</span></div>}
      </div>
      <form className="chat-input-row" onSubmit={e => handleGeminiSend(e)}>
        <input
          type="text"
          value={input}
          onChange={e => setInput(e.target.value)}
          placeholder="Type your question or pick a topic..."
          className="chat-input"
          autoFocus
          aria-label="Type your message"
          disabled={loading}
        />
        <button type="submit" className="chat-send" disabled={loading || !input.trim()}>Send</button>
      </form>
      {error && <div style={{ color: '#b22222', marginTop: '0.5rem' }}>
        Gemini backend error. Check your server and API key in <code>server/.env</code>.
      </div>}
    </div>
  );
}

export default App


================================================================================


################################################################################
# File: scripts/docling_batch_convert_with_metadata.py
################################################################################

#!/usr/bin/env python3
# scripts/docling_batch_convert_with_metadata.py
"""
Convert every PDF under  documents/research/Global
‚Üí flat TXT (documents/research/txt/) **and** rich JSON
(documents/research/json/) that carries per‚Äëpage text plus
bibliographic metadata (title, authors, year, journal, DOI,
abstract, keywords, research_topics).

The script is idempotent: if both TXT and JSON already
exist it skips the file; if one is missing it creates just that one.
"""

from __future__ import annotations

import argparse
import json
import logging
import os
import pathlib
import re
from datetime import datetime
from typing import Any, Dict, Iterable, List

import PyPDF2
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption
from rich.logging import RichHandler
from tqdm import tqdm

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ optional unstructured fallback ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #

try:
    from unstructured.partition.pdf import partition_pdf

    HAVE_UNSTRUCTURED = True
except Exception:  # pragma: no cover
    HAVE_UNSTRUCTURED = False

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ logging ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #


def init_logging(level: str = "INFO") -> None:
    logging.basicConfig(
        level=getattr(logging, level.upper(), "INFO"),
        format="%(asctime)s ‚Äî %(levelname)s ‚Äî %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        handlers=[RichHandler()],
    )


log = logging.getLogger(__name__)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Docling converter helper ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #


def make_converter() -> DocumentConverter:
    pdf_opts = PdfPipelineOptions()
    pdf_opts.do_ocr = True  # scan‚Äësafe
    return DocumentConverter(
        format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_opts)}
    )


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ path helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #


def iter_pdfs(root: pathlib.Path) -> Iterable[pathlib.Path]:
    for p in root.rglob("*.pdf"):
        if p.is_file() and not p.name.startswith("."):
            yield p


def txt_path_for(src: pathlib.Path, txt_dir: pathlib.Path) -> pathlib.Path:
    return txt_dir / f"{src.stem}.txt"


def json_path_for(src: pathlib.Path, json_dir: pathlib.Path) -> pathlib.Path:
    return json_dir / f"{src.stem}.json"


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ bibliographic helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #

META_FIELDS = [
    "title",
    "authors",
    "year",
    "journal",
    "doi",
    "abstract",
    "keywords",
    "research_topics",
]


def _authors(val: Any) -> List[str]:
    if val is None:
        return []
    if isinstance(val, list):
        return [str(a).strip() for a in val if a]
    return re.split(r"\s*,\s*|\s+and\s+", str(val).strip())


def extract_bib_from_filename(pdf: pathlib.Path) -> Dict[str, Any]:
    """
    Infer author/year/title from filenames like ‚ÄúSmith¬†2019¬†Some¬†Paper.pdf‚Äù.
    """
    stem = pdf.stem
    m = re.search(r"\b(19|20)\d{2}\b", stem)
    year = int(m.group(0)) if m else None
    if m:
        author = stem[: m.start()].strip()
        title = stem[m.end() :].strip(" -_")
    else:
        parts = stem.split(" ", 1)
        author = parts[0]
        title = parts[1] if len(parts) == 2 else None

    return {"authors": [author] if author else [], "year": year, "title": title}


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ header‚Äëtext regex extraction (journal / DOI ‚Ä¶) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #


DOI_RE = re.compile(r"\b(10\.\d{4,9}/[-._;()/:A-Z0-9]+)", re.I)
JOURNAL_RE = re.compile(
    r"(Journal|Journals|Revista|Proceedings|Annals|Neuroscience|Psychiatry|Psychology|Nature|Science)[^\n]{0,120}",
    re.I,
)
ABSTRACT_RE = re.compile(r"(?<=\bAbstract\b[:\s])(.{50,2000}?)(?:\n[A-Z][^\n]{0,60}\n|\Z)", re.S)
KEYWORDS_RE = re.compile(r"\bKey\s*words?\b[:\s]*(.+)", re.I)


def extract_bib_from_header(header_txt: str) -> Dict[str, Any]:
    """
    Very tolerant regexes on the first pages' raw text.
    """
    meta: Dict[str, Any] = {}

    doi = DOI_RE.search(header_txt)
    if doi:
        meta["doi"] = doi.group(1)

    jour = JOURNAL_RE.search(header_txt)
    if jour:
        meta["journal"] = " ".join(jour.group(0).split())

    abst = ABSTRACT_RE.search(header_txt)
    if abst:
        meta["abstract"] = " ".join(abst.group(1).split())

    kws = KEYWORDS_RE.search(header_txt)
    if kws:
        meta["keywords"] = [k.strip(" ;.,") for k in re.split(r"[;,]", kws.group(1)) if k.strip()]

    # derive research_topics from keywords
    if meta.get("keywords"):
        meta["research_topics"] = meta["keywords"]

    return meta


def merge_metadata(*sources: Dict[str, Any]) -> Dict[str, Any]:
    """
    Deep, ordered merge of metadata sources according to META_FIELDS precedence.
    """
    merged: Dict[str, Any] = {"doc_type": "scientific paper"}
    for field in META_FIELDS:
        merged[field] = None

    for src in sources:  # earlier dicts have lower priority
        for k in META_FIELDS:
            v = src.get(k)
            if v not in (None, "", [], {}):
                merged[k] = v

    # canonical types / defaults
    merged["authors"] = _authors(merged["authors"])
    merged["keywords"] = merged["keywords"] or []
    merged["research_topics"] = merged["research_topics"] or []
    return merged


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ per‚Äëpage text extraction helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #


def sections_from_docling(doc) -> List[Dict[str, Any]]:
    """
    First choice: iterate doc.pages (reliable).  Fallback: split on \f.
    """
    secs: List[Dict[str, Any]] = []
    try:
        pages = getattr(doc, "pages", None)
        if pages:  # new Docling API
            for idx, page in enumerate(pages, 1):
                try:
                    txt = page.export_to_text()
                except Exception:  # pragma: no cover
                    txt = ""
                if txt and txt.strip():
                    pn = getattr(page, "metadata", {}).get("page_number", idx)
                    secs.append(
                        {
                            "section": f"Page {pn}",
                            "page_number": pn,
                            "text": txt.strip(),
                        }
                    )
        if secs:
            return secs
    except Exception as e:  # pragma: no cover
        log.debug("Docling page iteration failed: %s", e)

    # ---- fallback to page_break marker ---- #
    try:
        full = doc.export_to_text(page_break_marker="\f")
        if "\f" in full:
            for idx, txt in enumerate(full.split("\f"), 1):
                t = txt.strip()
                if t:
                    secs.append({"section": f"Page {idx}", "page_number": idx, "text": t})
        elif full.strip():
            secs.append({"section": "Page 1", "page_number": 1, "text": full.strip()})
    except Exception as e:
        log.debug("Docling export_to_text failed: %s", e)
    return secs


def sections_from_unstructured(pdf: pathlib.Path) -> List[Dict[str, Any]]:
    elements = partition_pdf(str(pdf), strategy="hi_res")
    sections: List[Dict[str, Any]] = []
    buf, cur = "", None
    for el in elements:
        page = getattr(el.metadata, "page_number", None)
        if page is None:
            continue
        if cur is None:
            cur = page
        if page != cur:
            sections.append(
                {"section": f"Page {cur}", "page_number": cur, "text": buf.strip()}
            )
            buf, cur = "", page
        buf += " " + str(el)
    if buf.strip():
        sections.append(
            {"section": f"Page {cur}", "page_number": cur, "text": buf.strip()}
        )
    return sections


def sections_from_pypdf(pdf: pathlib.Path) -> List[Dict[str, Any]]:
    secs: List[Dict[str, Any]] = []
    with open(pdf, "rb") as f:
        for i, page in enumerate(PyPDF2.PdfReader(f).pages, 1):
            try:
                txt = page.extract_text() or ""
            except Exception:
                txt = ""
            secs.append({"section": f"Page {i}", "page_number": i, "text": txt.strip()})
    return secs


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ single‚Äëfile processing pipeline ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #


def process_one(
    conv: DocumentConverter,
    pdf: pathlib.Path,
    txt_dir: pathlib.Path,
    json_dir: pathlib.Path,
    overwrite: bool,
) -> bool:
    txt_path = txt_path_for(pdf, txt_dir)
    json_path = json_path_for(pdf, json_dir)

    has_txt, has_json = txt_path.exists(), json_path.exists()
    if has_txt and has_json and not overwrite:
        log.debug("‚úì Skip %s (TXT+JSON present)", pdf.name)
        return False

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Docling first ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
    doc_meta: Dict[str, Any] = {}
    sections: List[Dict[str, Any]] = []
    try:
        bundle = conv.convert(str(pdf))
        doc = bundle.document
        meta_obj = getattr(doc, "metadata", None)
        if meta_obj is not None:
            for meth in ("model_dump", "dict", "to_dict"):
                if hasattr(meta_obj, meth):
                    doc_meta = getattr(meta_obj, meth)()
                    break
        sections = sections_from_docling(doc)
    except Exception as e:
        log.debug("Docling failed on %s (%s) ‚Äì trying fallbacks", pdf.name, e)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ fallbacks if Docling text inadequate ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
    if not sections or all(len(s["text"]) < 40 for s in sections):
        if HAVE_UNSTRUCTURED:
            try:
                sections = sections_from_unstructured(pdf)
            except Exception as e:
                log.debug("unstructured failed on %s (%s)", pdf.name, e)
        if not sections:
            sections = sections_from_pypdf(pdf)

    if not sections:
        raise RuntimeError("no text extracted")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ compile metadata ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
    first_pages_text = " ".join(s["text"] for s in sections[:2])[:12000]
    header_meta = extract_bib_from_header(first_pages_text)
    filename_meta = extract_bib_from_filename(pdf)
    meta = merge_metadata(doc_meta, filename_meta, header_meta)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ assemble JSON payload ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
    payload = {
        **meta,
        "created_at": datetime.utcnow().isoformat() + "Z",
        "source_pdf": str(pdf),
        "sections": sections,
    }

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ write outputs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
    if overwrite or not has_json:
        json_path.parent.mkdir(parents=True, exist_ok=True)
        json_path.write_text(
            json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8"
        )

    if overwrite or not has_txt:
        txt_path.parent.mkdir(parents=True, exist_ok=True)
        txt_path.write_text("\n\n".join(s["text"] for s in sections), encoding="utf-8")

    return True


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ batch driver ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #


def run(
    src_root: pathlib.Path,
    txt_dir: pathlib.Path,
    json_dir: pathlib.Path,
    overwrite: bool,
    limit: int | None,
) -> None:
    conv = make_converter()
    pdfs = list(iter_pdfs(src_root))
    if limit:
        pdfs = pdfs[:limit]

    processed = skipped = failed = 0
    log.info("Starting ‚Äì %s PDFs (src=%s)", len(pdfs), src_root)

    with tqdm(total=len(pdfs), unit="file", desc="Processing") as bar:
        for pdf in pdfs:
            bar.set_postfix_str(pdf.name)
            try:
                changed = process_one(conv, pdf, txt_dir, json_dir, overwrite)
                if changed:
                    processed += 1
                else:
                    skipped += 1
            except Exception as e:
                failed += 1
                log.error("‚ÄºÔ∏è  %s failed: %s", pdf.name, e)
            bar.update()

    log.info(
        "Done. processed=%s  skipped=%s  failed=%s  (TXT ‚Üí %s , JSON ‚Üí %s)",
        processed,
        skipped,
        failed,
        txt_dir,
        json_dir,
    )


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CLI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #


def cli() -> None:
    p = argparse.ArgumentParser(
        description="Convert research PDFs to TXT and JSON with metadata"
    )
    p.add_argument("--src", default="documents/research/Global", help="PDF folder")
    p.add_argument("--txt-dir", default="documents/research/txt", help="TXT output dir")
    p.add_argument("--json-dir", default="documents/research/json", help="JSON output dir")
    p.add_argument("--overwrite", action="store_true", help="Force re‚Äëcreate outputs")
    p.add_argument(
        "--log",
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="Console log level",
    )
    p.add_argument("--max", type=int, default=0, help="Process at most N PDFs")
    args = p.parse_args()

    init_logging(args.log)
    run(
        pathlib.Path(args.src),
        pathlib.Path(args.txt_dir),
        pathlib.Path(args.json_dir),
        args.overwrite,
        None if args.max == 0 else args.max,
    )


if __name__ == "__main__":
    cli()


================================================================================


################################################################################
# File: src/App.css
################################################################################

/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
/* GLOBAL RESET                                 */
html, body {
  margin: 0 !important;      /* kill the 8 px UA margin */
  padding: 0;
  height: 100%;              /* body fills the viewport vertically */
}

/* optional ‚Äì keeps sizing predictable everywhere */
*, *::before, *::after { box-sizing: border-box; }
/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */

/* ----- BODY --------------------------------------------- */
body {
  min-height: 100vh;
  margin: 0;
  font-family: 'Nunito', 'Segoe UI', 'Helvetica Neue', Arial, 'Liberation Sans', sans-serif;
  color: #3a3a3a;
  background: linear-gradient(120deg, #f8f6ff 0%, #b2d8d8 100%);
  overflow-x: hidden;
  display: flex;
  flex-direction: column;
  align-items: center;
}

.animated-bg {
  position: fixed;
  z-index: 0;
  width: 100vw;
  height: 100vh;
  top: 0; left: 0;
  pointer-events: none;
  overflow: hidden;
}

.bubble {
  position: absolute;
  border-radius: 50%;
  opacity: 0.3;
  animation: float 18s infinite linear;
  background: #b2d8d8;
}
.bubble1 { width: 120px; height: 120px; left: 10vw; top: 80vh; animation-delay: 0s; background: #b2d8d8; }
.bubble2 { width: 80px; height: 80px; left: 70vw; top: 70vh; animation-delay: 4s; background: #f8f6ff; }
.bubble3 { width: 90px; height: 90px; left: 50vw; top: 90vh; animation-delay: 8s; background: #e0e7ef; }
.bubble4 { width: 60px; height: 60px; left: 20vw; top: 85vh; animation-delay: 2s; background: #f7cac9; }
.bubble5 { width: 100px; height: 100px; left: 80vw; top: 95vh; animation-delay: 6s; background: #b5ead7; }
.bubble6 { width: 70px; height: 70px; left: 30vw; top: 92vh; animation-delay: 10s; background: #ffdac1; }
.bubble7 { width: 110px; height: 110px; left: 60vw; top: 85vh; animation-delay: 12s; background: #c7ceea; }
.bubble8 { width: 50px; height: 50px; left: 40vw; top: 98vh; animation-delay: 14s; background: #f6dfeb; }
@keyframes float {
  0% { transform: translateY(0); }
  100% { transform: translateY(-90vh); }
}

/* ----- LAYOUT WRAPPER ----------------------------------- */
.container {
  max-width: 860px;
  margin: 0 auto;
  padding: 0 1rem;
  width: 100%;

  display: flex;
  flex-direction: column;
  align-items: center;

  z-index: 1;
  position: relative;
  min-width: 340px;
  background: none;
  box-shadow: none;
}

/* ----- CARD --------------------------------------------- */
.card {
  background: rgba(255,255,255,0.96);
  border-radius: 22px;
  box-shadow: 0 6px 32px rgba(31,38,135,.13);
  padding: 2rem 1.2rem;
  margin: 2.5rem 0;
  max-width: 520px;
  width: 100%;
  display: flex;
  flex-direction: column;
  align-items: center;
  transition: box-shadow 0.18s;
}

/* mobile tweaks ‚Äì ONE block */
@media (max-width: 700px) {
  .container { 
    max-width: 98vw; 
    padding: 1rem; 
  }
  .card { 
    padding: 1.2rem 0.5rem; 
    border-radius: 16px; 
    max-width: 99vw;
  }
  .navbar {
    flex-direction: column;
    flex-wrap: wrap;
    gap: 0.5rem;
    font-size: 0.97rem;
  }
  .chatbot-box {
    padding: 1rem 0.3rem;
    border-radius: 16px;
  }
  .msg {
    font-size: 0.99rem;
  }
  .chat-avatar {
    width: 30px; height: 30px;
  }
}

/* ----- NAVBAR ------------------------------------------- */
.navbar {
  display: flex;
  justify-content: center;
  gap: 1rem;
  flex-wrap: wrap;
  margin-bottom: 2rem;
}

.navbar button {
  background: #e6e6fa;
  color: #3a3a3a;
  border: none;
  padding: 0.7rem 1.5rem;
  border-radius: 18px;
  font-size: 1rem;
  font-weight: 500;
  cursor: pointer;
  transition: background 0.2s, color 0.2s;
  box-shadow: 0 2px 8px rgba(178, 216, 216, 0.09);
}

.navbar button.active, .navbar button:hover {
  background: #b2d8d8;
  color: #234e52;
}

.title {
  font-size: 2.2rem;
  margin-bottom: 0.5rem;
  color: #234e52;
  font-family: 'Segoe UI', 'Helvetica Neue', Arial, 'Liberation Sans', sans-serif;
}

.subtitle {
  font-size: 1.2rem;
  color: #6d7b8d;
  margin-bottom: 2rem;
}

main {
  padding: 1.5rem 0;
}

h2 {
  color: #234e52;
  margin-bottom: 1rem;
}

/* Chatbot styles (always apply) */
.chatbot-box {
  background: linear-gradient(135deg, rgba(248,246,255,0.93) 60%, rgba(178,216,216,0.10) 100%);
  border-radius: 26px;
  box-shadow: 0 4px 28px 0 rgba(31, 38, 135, 0.11), 0 1.5px 5px 0 rgba(178,216,216,0.08);
  padding: 2.2rem 1.2rem 1.5rem 1.2rem;
  margin: 0.5rem 0 0.5rem 0;
  min-height: 350px;
  width: 100%;
  display: flex;
  flex-direction: column;
  border: 1.5px solid #e0e7ef;
  backdrop-filter: blur(3px);
  position: relative;
  transition: box-shadow 0.2s;
}

.chat-messages {
  flex: 1;
  overflow-y: auto;
  margin-bottom: 1.1rem;
  display: flex;
  flex-direction: column;
  gap: 0.7rem;
  padding-bottom: 0.5rem;
  border-radius: 18px 18px 8px 8px;
  animation: fadeIn 0.7s;
}
@keyframes fadeIn {
  from { opacity: 0; transform: translateY(16px); }
  to { opacity: 1; transform: translateY(0); }
}

.msg {
  max-width: 80%;
  display: flex;
  align-items: flex-end;
  gap: 0.6rem;
  font-size: 1.08rem;
  word-break: break-word;
  box-shadow: 0 1px 6px 0 rgba(31, 38, 135, 0.03);
  margin-bottom: 0.1rem;
}

.msg.user {
  align-self: flex-end;
  flex-direction: row-reverse;
}

.msg.bot {
  align-self: flex-start;
}

.bubble-content {
  padding: 0.7rem 1.1rem;
  border-radius: 16px;
  background: linear-gradient(90deg, #b2d8d8 60%, #e0e7ef 100%);
  color: #2a3d3d;
  font-size: 1.08rem;
  box-shadow: 0 1px 6px 0 rgba(31, 38, 135, 0.03);
}

.msg.bot .bubble-content {
  background: linear-gradient(90deg, #f8f6ff 60%, #e9f7f7 100%);
  color: #5b3a6b;
}

.chat-avatar {
  width: 38px;
  height: 38px;
  border-radius: 50%;
  margin: 0 0.2rem;
  box-shadow: 0 2px 8px 0 rgba(31, 38, 135, 0.05);
  background: #fff;
  object-fit: cover;
}

.chat-input-row {
  display: flex;
  gap: 0.6rem;
  border-top: 1.5px solid #e0e7ef;
  padding-top: 1rem;
  margin-top: 0.3rem;
  align-items: center;
}

.chat-input {
  flex: 1;
  height: 48px;
  padding: 0 1rem;
  border-radius: 12px;
  border: 1.5px solid #c6e2e2;
  font-size: 1.08rem;
  outline: none;
  background: #f4fafd;
  color: #2a3d3d;
  box-sizing: border-box;
  transition: border-color 0.18s;
}

.chat-input:focus {
  border-color: #b2d8d8;
}

.chat-send {
  height: 48px;
  min-width: 90px;
  padding: 0 1.2rem;
  border-radius: 12px;
  border: none;
  background: #b2d8d8;
  color: #fff;
  font-weight: bold;
  font-size: 1.08rem;
  cursor: pointer;
  transition: background 0.2s, box-shadow 0.18s;
  box-shadow: 0 1.5px 6px 0 rgba(31, 38, 135, 0.07);
  display: flex;
  align-items: center;
  justify-content: center;
}

.chat-send:hover:not(:disabled) {
  background: #81b0b0;
  box-shadow: 0 4px 16px 0 rgba(31, 38, 135, 0.13);
}

.chat-send:disabled {
  background: #f4fafd;
  color: #b2d8d8;
  cursor: not-allowed;
  border: 1.5px solid #e0e7ef;
  box-shadow: none;
}

/* More forceful styling for avatar labels */
.chat-avatar, [class*="avatar"], [class*="circle"] {
  font-size: 10px !important;
  width: 45px !important;
  height: 45px !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
}

/* ----- RESEARCH-ASSISTANT POP-UP ------------------------ */
.ra-wrapper {
  position: fixed;            /* stick to viewport */
  bottom: 84px;               /* enough to clear the floating disclaimer */
  left: 50%;
  transform: translateX(-50%);
  width: min(1100px, 96vw);    /* A little wider on desktop, still fully responsive on phones */
  max-height: 70vh;           /* room for long answers w/ scroll */
  overflow-y: auto;
  border-radius: 18px;
  background: #ffffff;
  box-shadow: 0 8px 22px rgba(0,0,0,.07);
  padding: 2rem 2.25rem;
}

/* ‚îÄ‚îÄ‚îÄ research-assistant sticky search bar ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
.rag-form{
  position: sticky;
  bottom: 0;                  /* hugs the card's bottom edge            */
  z-index: 2;
  padding: 1rem 0 0.5rem;     /* little breathing room                  */
  background: rgba(255,255,255,.96);   /* white backdrop while it floats */
  backdrop-filter: blur(3px);
}

.ra-input-row {
  display: flex;
  gap: 0.6rem;
  margin-bottom: 1.25rem;
}

.ra-input {
  flex: 1 1 auto;
  font-size: 1.05rem;
  padding: 0.65rem 0.9rem;
  border: 2px solid #d0e4e4;
  border-radius: 10px;
}

.ra-btn {
  background: #669c99;
  color: #fff;
  border: 0;
  padding: 0 1.25rem;
  border-radius: 10px;
  font-weight: 600;
  cursor: pointer;
}

/* ----- DISCLAIMER (single source of truth) --------------- */
.disclaimer-footer {
  position: fixed;
  bottom: 0;            /* 0 means literally the last pixel */
  left: 0;
  width: 100%;
  font-size: 0.75rem;
  color: #666;
  text-align: center;
  padding: 0;       /* adjust or set to 0 for no inner gap */
  background: transparent;      /* or rgba(255,255,255,.85) if you want a strip */
  z-index: 30;          /* higher than bubbles / page cards */
}

/* ‚îÄ‚îÄ‚îÄ RAG assistant layout tweak ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
.rag-card {
  max-height: 75vh;               /* card never taller than viewport */
  overflow-y: auto;               /* inner scroll, NOT page scroll   */
  display: flex;
  flex-direction: column;         /* children stack top‚Üíbottom       */
}

/* scrolling area for answer + sources */
.rag-body {
  flex: 1 1 auto;                 /* fill remaining height           */
  overflow-y: auto;
}

/* search row always visible inside the scrolling card */
.rag-input-row {
  flex-shrink: 0;                 /* never shrinks                   */
  position: sticky;
  bottom: 0;                      /* sticks to card's bottom         */
  background: #fff;               /* white strip over scrolled text  */
  padding-top: 1rem;              /* add gap so it doesn't hug text  */
}

/* ‚îÄ‚îÄ‚îÄ RAG card: same width as before, inner scroll, sticky input ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
.ra-card {
  width: 100%;                 /* full width of the central container    */
  max-width: 1000px;           /* equals the old Bootstrap template      */
  margin: 0 auto;              /* keep it centred                        */

  /* original "card" visual style (copied, not changed) */
  background: rgba(255,255,255,0.96);
  border-radius: 22px;
  box-shadow: 0 6px 32px rgba(31,38,135,.13);
  padding: 2rem 1.2rem;

  /* NEW: make the inside scroll but freeze the input row */
  display: flex;
  flex-direction: column;
  max-height: 75vh;            /* never taller than the viewport         */
  overflow-y: auto;
}

/* scrolling part (answer + citations) */
.ra-body {
  flex: 1 1 auto;
  overflow-y: auto;
}

/* sticky search row ‚Äì always visible */
.ra-input-row {
  flex-shrink: 0;
  position: sticky;
  bottom: 0;
  background: #fff;            /* white strip so text doesn't peek under */
  padding-top: 1rem;           /* gap between content & bar              */
}

/* RAG research card & footer separation ---------------- */
.ra-card, .rag-card {
  /* just ~1 em gap ‚Äì enough to clear the footer without a chasm */
  margin-bottom: 1.3rem;
}

/* keep the search bar fixed just above the footer text (‚âà24 px) */
.ra-input-row {
  position: sticky;
  bottom: 1.5rem;
}


================================================================================


################################################################################
# File: scripts/optimized_batch_process_gian_v1.py
################################################################################

# File: scripts/optimized_batch_process_gian_v1.py

#!/usr/bin/env python3
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
"""
Batch‚Äëupload pre‚Äëparsed research papers into Supabase
using a fixed‚Äëtoken sliding window (768‚ÄØtokens, 20‚ÄØ% overlap).

INPUT  directories
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
documents/research/json   one JSON per paper (metadata + per‚Äëpage text)
documents/research/txt    flat .txt version (kept for future re‚Äëchunking)

DB schema expected
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  research_documents  (one row per paper)
  research_chunks     (token_window strategy)

Chunking parameters
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  ‚Ä¢ window      768 tokens
  ‚Ä¢ overlap     154 tokens   (20‚ÄØ%)
  ‚Ä¢ step        614 tokens   (window ‚àí overlap)

Sentence‚Äëfriendly rule
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
After *window* tokens are gathered we keep adding UNTIL the next token
ends with `.`, `!`, or `?` ‚Äîor the file ends, or we exceed
window‚ÄØ+‚ÄØ256 tokens.  This avoids mid‚Äësentence splits whenever possible.
"""
from __future__ import annotations

import argparse
import json
import os
import random
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Sequence

from dotenv import load_dotenv
from supabase import create_client
from tqdm import tqdm

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ env / constants ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #

load_dotenv()

REPO_ROOT      = Path(__file__).resolve().parent.parent
JSON_DIR       = REPO_ROOT / "documents" / "research" / "json"
TXT_DIR        = REPO_ROOT / "documents" / "research" / "txt"   # kept but unused here

SUPABASE_URL   = os.getenv("SUPABASE_URL")
SUPABASE_KEY   = os.getenv("SUPABASE_SERVICE_ROLE_KEY")

DEFAULT_WINDOW   = 768
DEFAULT_OVERLAP  = int(DEFAULT_WINDOW * 0.20)     # 154
DEFAULT_STEP     = DEFAULT_WINDOW - DEFAULT_OVERLAP
SENT_END_RE      = re.compile(r"[.!?]$")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ NUL‚Äëbyte scrubber ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #

def scrub_nuls(obj: Any) -> Any:
    """Recursively remove ASCII‚ÄëNUL characters from any str inside *obj*."""
    if isinstance(obj, str):
        return obj.replace("\x00", "")
    if isinstance(obj, list):
        return [scrub_nuls(x) for x in obj]
    if isinstance(obj, dict):
        return {k: scrub_nuls(v) for k, v in obj.items()}
    return obj

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #

def discover_json_files() -> List[Path]:
    return sorted(JSON_DIR.rglob("*.json"))

def load_paper(json_path: Path) -> Dict[str, Any]:
    return json.loads(json_path.read_text(encoding="utf‚Äë8"))

def concat_sections(sections: Sequence[Dict[str, Any]]) -> tuple[list[str], list[int]]:
    """
    Return (tokens, token_to_page)  ‚Ü¶  token_to_page[i] = 1‚Äëbased page number.
    """
    tokens: list[str] = []
    page_map: list[int] = []
    for sec in sections:
        page = sec.get("page_number") or 1
        sec_tokens = sec.get("text", "").split()
        tokens.extend(sec_tokens)
        page_map.extend([page] * len(sec_tokens))
    return tokens, page_map

def sliding_window_chunks(
    tokens: List[str],
    page_map: List[int],
    *,
    window: int = DEFAULT_WINDOW,
    overlap: int = DEFAULT_OVERLAP,
) -> List[Dict[str, Any]]:
    """Fixed‚Äëtoken windows with overlap, but avoid cutting sentences."""
    step = max(1, window - overlap)
    chunks: list[dict[str, Any]] = []
    i = 0
    safety_extra = 256

    while i < len(tokens):
        start = i
        end   = min(len(tokens), start + window)

        # extend to sentence boundary if possible
        while (
            end < len(tokens)
            and not SENT_END_RE.search(tokens[end - 1])
            and (end - start) < window + safety_extra
        ):
            end += 1

        text_slice  = " ".join(tokens[start:end])
        page_start  = page_map[start]
        page_end    = page_map[end - 1]
        chunk_idx   = len(chunks)

        chunks.append(
            {
                "chunk_index":  chunk_idx,
                "token_start":  start,
                "token_end":    end - 1,
                "page_start":   page_start,
                "page_end":     page_end,
                "text":         scrub_nuls(text_slice),
            }
        )
        if end == len(tokens):
            break
        i += step

    return chunks

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Supabase I/O ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #

def get_processed_pdfs(sb) -> set[str]:
    """
    Return the set of source_pdf paths already present in research_documents.
    """
    try:
        rows = sb.table("research_documents").select("source_pdf").execute().data or []
        return {row["source_pdf"] for row in rows}
    except Exception as e:
        print(f"[get_processed_pdfs] {e}")
        return set()

BIB_KEYS = {
    "doc_type", "title", "authors", "year",
    "journal", "doi", "abstract", "keywords",
    "research_topics", "source_pdf",
}

def insert_document(sb, raw_meta: Dict[str, Any]) -> str:
    """
    Insert into research_documents; returns UUID.  Re‚Äëuse row if DOI matches.
    """
    raw_meta = scrub_nuls(raw_meta)
    doi = raw_meta.get("doi")

    # ‚îÄ‚îÄ 1.  reuse if DOI already there
    if doi:
        q = (
            sb.table("research_documents")
            .select("id")
            .eq("doi", doi)
            .limit(1)
            .execute()
        )
        if q.data:
            return q.data[0]["id"]

    # ‚îÄ‚îÄ 2.  trim to bibliographic subset
    meta_small = {k: raw_meta.get(k) for k in BIB_KEYS}

    payload = {
        **meta_small,
        # defaults / housekeeping
        "doc_type":   meta_small.get("doc_type", "scientific paper"),
        "authors":    meta_small.get("authors") or [],
        "keywords":   meta_small.get("keywords") or [],
        "research_topics": meta_small.get("research_topics") or [],
        "metadata":   meta_small,                 # stored as jsonb
        "created_at": datetime.utcnow().isoformat(),
    }
    res = sb.table("research_documents").insert(payload).execute()
    if getattr(res, "error", None):
        raise RuntimeError(res.error)
    return res.data[0]["id"]

def insert_chunks(sb, doc_id: str, json_path: Path, chunks: Sequence[Dict[str, Any]]):
    max_batch = 500
    rows = [
        {
            "document_id":       doc_id,
            "chunk_index":       ch["chunk_index"],
            "token_start":       ch["token_start"],
            "token_end":         ch["token_end"],
            "page_start":        ch["page_start"],
            "page_end":          ch["page_end"],
            "text":              ch["text"],
            "metadata":          json.loads(json.dumps({}).replace('\u0000', '')),  # extra per‚Äëchunk data (now clean)
            "chunking_strategy": "token_window",
            "source_file":       json_path.as_posix(),  # ‚Üê renamed column
            "created_at":        datetime.utcnow().isoformat(),
        }
        for ch in chunks
    ]
    for i in range(0, len(rows), max_batch):
        sb.table("research_chunks").insert(rows[i : i + max_batch]).execute()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ main process ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #

def process_one_paper(
    sb,
    json_path: Path,
    *,
    window: int,
    overlap: int,
) -> tuple[bool, str]:
    try:
        paper = load_paper(json_path)
        sections = paper.get("sections") or []
        if not sections:
            return False, "No sections/text found."

        tokens, page_map = concat_sections(sections)
        if not tokens:
            return False, "Empty token list."

        chunks = sliding_window_chunks(tokens, page_map, window=window, overlap=overlap)
        doc_id = insert_document(sb, paper)
        insert_chunks(sb, doc_id, json_path, chunks)
        return True, f"{len(chunks)} chunks"
    except Exception as e:
        return False, str(e)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CLI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #

def main() -> None:
    p = argparse.ArgumentParser(description="Upload 768‚Äëtoken window chunks to Supabase")
    p.add_argument("--batch-size", type=int, default=20, help="papers per run (0 = all)")
    p.add_argument("--window",     type=int, default=DEFAULT_WINDOW, help="tokens per chunk")
    p.add_argument("--overlap",    type=int, default=DEFAULT_OVERLAP, help="token overlap")
    p.add_argument("--selection",  choices=["sequential", "random"], default="sequential")
    args = p.parse_args()

    if not (SUPABASE_URL and SUPABASE_KEY):
        sys.exit("‚ùå  Missing SUPABASE_URL / SUPABASE_SERVICE_ROLE_KEY")

    sb = create_client(SUPABASE_URL, SUPABASE_KEY)

    all_json = discover_json_files()
    processed_pdfs = get_processed_pdfs(sb)
    remaining = [
        p for p in all_json
        if scrub_nuls(load_paper(p)).get("source_pdf") not in processed_pdfs
    ]

    if args.selection == "random" and args.batch_size and len(remaining) > args.batch_size:
        remaining = random.sample(remaining, args.batch_size)
    elif args.batch_size:
        remaining = remaining[: args.batch_size]

    if not remaining:
        print("‚ú®  Nothing new to process.")
        return

    step = args.window - args.overlap
    print(f"Uploading {len(remaining)} papers ‚Ä¶  (window={args.window}, overlap={args.overlap}, step={step})")
    ok = fail = 0
    for jp in tqdm(remaining, desc="Papers"):
        success, msg = process_one_paper(
            sb,
            jp,
            window=args.window,
            overlap=args.overlap,
        )
        if success:
            ok += 1
        else:
            fail += 1
            print(f"  ! {jp.name} ‚Üí {msg}")

    print(f"\nDone ‚úî   success={ok}   failed={fail}")

if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: netlify/functions/rag-search.js
################################################################################

// File: netlify/functions/rag-search.js

// server/index.js ‚Äì Express¬†gateway for Misophonia¬†Companion
// -----------------------------------------------------------------------------
// Responsibilities:
// 1. Acts as a same‚Äëorigin API gateway for the React SPA while developing¬†/ hosting
//    on Netlify.
// 2. Proxies RAG¬†v8 Flask searches at¬†`/api/rag` so the browser avoids CORS.
// 3. Keeps the existing Chatbot (/api/chat) and Gemini Research Assistant (/api/gemini)
//    endpoints untouched.
// -----------------------------------------------------------------------------
import express from 'express';
import cors from 'cors';
import dotenv from 'dotenv';
import { OpenAI } from 'openai';
import fetch from 'node-fetch';

dotenv.config();

// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
// Configuration
// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

// Which LLM powers the Research Assistant ("openai" | "gemini") ‚Äì defaults to "openai"
const AI_PROVIDER = process.env.AI_PROVIDER || 'openai';

// Location of the running RAG¬†service (rag_web_app_v8.py).  In dev you usually run
// `python scripts/rag_web_app_v8.py` which listens on http://localhost:8080.
// In production¬†/ Netlify you can expose the Flask app via AWS¬†Lambda, Fly.io, etc.
const RAG_HOST = process.env.RAG_HOST || 'http://localhost:8080';

const app = express();

// Allow the React dev server & Netlify preview to call us
app.use(cors({
  origin: [
    'http://localhost:5173',                // Vite dev server
    'https://misophonia-guide.netlify.app', // Production domain (update as needed)
    /https:\/\/.*--misophonia-guide--.*/   // Netlify deploy‚Äëpreview sub‚Äëdomains
  ]
}));

app.use(express.json({ limit: '1mb' }));

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
// 1. RAG¬†v8 proxy ‚Üí POST¬†/api/rag  ‚á¢  Flask¬†/¬†search
// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
app.post('/api/rag', async (req, res) => {
  try {
    // Forward the exact body (ex. { query: "‚Ä¶", limit: 8 }) to the Flask endpoint
    const ragRes = await fetch(`${RAG_HOST}/search`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify(req.body),
      // 30¬†s is plenty for embeddings + GPT‚Äë4o in most cases
      timeout: 30_000
    });

    // Honour status codes from Flask (200, 4xx, 5xx)
    const payload = await ragRes.text();
    res
      .status(ragRes.status)
      .type('application/json')  // ensure JSON even for passthrough text
      .send(payload);
  } catch (error) {
    console.error('Error proxying /api/rag ‚Üí', error);
    res.status(500).json({ error: 'Error calling RAG¬†service.' });
  }
});

// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
// 2. Gemini / OpenAI hybrid Research Assistant (existing logic)
//    POST¬†/api/gemini
// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
app.post('/api/gemini', async (req, res) => {
  try {
    const { messages, topic } = req.body;

    if (!messages || !Array.isArray(messages)) {
      return res.status(400).json({ error: 'Messages array required.' });
    }

    // ------------------------ OpenAI provider ------------------------------
    if (AI_PROVIDER === 'openai') {
      const systemPrompt = topic
        ? `You are a knowledgeable research assistant focusing on the topic: ${topic}. Provide information about misophonia research.`
        : 'You are a knowledgeable research assistant on misophonia research.';

      const openaiMessages = [
        { role: 'system', content: systemPrompt },
        ...messages
      ];

      const completion = await openai.chat.completions.create({
        model: 'gpt-4o',
        messages: openaiMessages,
        max_tokens: 1024,
        temperature: 0.7
      });

      return res.json({
        reply: completion.choices[0]?.message?.content || '',
        structured: null,
        provider: 'openai'
      });
    }

    // ------------------------ Gemini provider -----------------------------
    const apiKey = process.env.GEMINI_API_KEY;
    if (!apiKey) {
      return res.status(500).json({ error: 'GEMINI_API_KEY not set in server/.env.' });
    }

    let userPrompt = '';
    if (topic && typeof topic === 'string') {
      userPrompt += `Topic: ${topic}\n`;
    }
    userPrompt += messages.map(m => `${m.role === 'user' ? 'User' : 'Assistant'}: ${m.content}`).join('\n');

    const geminiRes = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?key=${apiKey}`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        contents: [{ role: 'user', parts: [{ text: userPrompt }] }],
        generationConfig: {
          temperature: 0.7,
          maxOutputTokens: 1024
        },
        tools: [{
          function_declarations: [{
            name: 'structured_output',
            description: 'Return information in a structured JSON format for chat display, with sections, bullet points, and highlights.'
          }]
        }]
      })
    });

    if (!geminiRes.ok) {
      const err = await geminiRes.text();
      return res.status(500).json({ error: 'Error from Gemini API', details: err });
    }

    const geminiData = await geminiRes.json();
    let structured = null;
    if (geminiData.candidates && geminiData.candidates[0]?.content?.parts) {
      const part = geminiData.candidates[0].content.parts[0];
      if (part.functionCall && part.functionCall.name === 'structured_output') {
        try {
          structured = JSON.parse(part.functionCall.args.json || '{}');
        } catch {
          structured = part.functionCall.args;
        }
      }
    }

    return res.json({
      reply: geminiData.candidates?.[0]?.content?.parts?.[0]?.text || '',
      structured,
      provider: 'gemini'
    });
  } catch (error) {
    console.error(error);
    res.status(500).json({ error: `Error from ${AI_PROVIDER === 'openai' ? 'OpenAI' : 'Gemini'} API.` });
  }
});

// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
// 3. Generic Chatbot (existing logic) ‚Äì POST¬†/api/chat
// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
app.post('/api/chat', async (req, res) => {
  try {
    const { messages } = req.body;
    if (!messages || !Array.isArray(messages)) {
      return res.status(400).json({ error: 'Messages array required.' });
    }
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages,
      max_tokens: 512,
      temperature: 0.7
    });
    const reply = completion.choices[0]?.message?.content || '';
    res.json({ reply });
  } catch (error) {
    console.error(error);
    res.status(500).json({ error: 'Error from OpenAI API.' });
  }
});

// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
// Launch
// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
const PORT = process.env.PORT || 3001;
app.listen(PORT, () => {
  console.log(`Server running on port ${PORT}`);
  console.log(`AI provider   : ${AI_PROVIDER.toUpperCase()}`);
  console.log(`/api/rag proxy : ${RAG_HOST}/search`);
});


================================================================================


################################################################################
# File: vector-db-README.md
################################################################################

################################################################################
################################################################################
# Misophonia Research Vector Database

This project implements a comprehensive vector database for misophonia research documents using Firebase/Firestore with vector search capabilities. The system processes research PDFs, extracts text with context preservation, and generates embeddings for semantic search.

## Project Structure

```
/
‚îú‚îÄ‚îÄ firebase.json         # Firebase configuration
‚îú‚îÄ‚îÄ firestore/            # Firestore rules and indexes
‚îú‚îÄ‚îÄ functions/            # Cloud Functions for Firebase
‚îÇ   ‚îî‚îÄ‚îÄ index.js          # Cloud Functions for embedding generation and semantic search
‚îî‚îÄ‚îÄ scripts/              # Python scripts for document processing
    ‚îú‚îÄ‚îÄ optimized_batch_process.py       # Batch processing with optimized chunking
    ‚îú‚îÄ‚îÄ batch_embedding_generator.py     # Parallel embedding generation
    ‚îú‚îÄ‚îÄ comprehensive_search.py          # Combined semantic and keyword search
    ‚îú‚îÄ‚îÄ direct_semantic_search.py        # Direct semantic search without Cloud Functions
    ‚îú‚îÄ‚îÄ check_processed_chunks.py        # Status monitoring for embeddings
    ‚îî‚îÄ‚îÄ test_raw_chunks.py               # Raw chunk search and statistics
```

## Setup Instructions

### Prerequisites

1. Firebase CLI installed: `npm install -g firebase-tools`
2. Firebase project created (misophonia-companion)
3. Python 3.8+ with pip
4. Node.js 18+

### Firebase Setup

1. Log in to Firebase:
   ```
   firebase login
   ```

2. Deploy Firestore rules and indexes:
   ```
   firebase deploy --only firestore
   ```

3. Install dependencies and deploy Cloud Functions:
   ```
   cd functions
   npm install
   firebase deploy --only functions
   ```

### Document Processing

1. Install Python dependencies:
   ```
   cd scripts
   pip install -r requirements.txt
   ```

2. Generate a Firebase service account key:
   - Go to Firebase console > Project settings > Service accounts
   - Click "Generate new private key"
   - Save the JSON file securely

3. Process PDF documents in batches:
   ```
   cd scripts
   python optimized_batch_process.py --batch-size 10
   ```

4. Generate embeddings for chunks:
   ```
   cd scripts
   python batch_embedding_generator.py --batch-size 100 --workers 5
   ```

5. Monitor processing progress:
   ```
   cd scripts
   python check_processed_chunks.py
   ```

## Vector Search Implementation

This project uses Firestore's vector search capabilities to implement semantic search over research documents:

1. Documents are processed and chunked with context preservation
   - Sentence-aware chunking with 2,000 character size and 300 character overlap
   - Hierarchical metadata preservation (document, section, chunk)
   - Context relationships between adjacent chunks

2. Embeddings are generated for each chunk
   - Cloud Functions trigger on new chunks in research_chunks_raw collection
   - Parallel batch processing for efficient embedding generation
   - OpenAI's text-embedding-ada-002 model (1536 dimensions)

3. Embeddings are stored in Firestore with document metadata
   - Complete document context preserved with each embedding
   - Efficient vector search indexes
   - Context expansion capabilities

4. Multiple search approaches are available
   - Cloud Function-based semantic search (semanticSearch)
   - Direct semantic search without Cloud Functions (direct_semantic_search.py)
   - Raw text search for keyword matching (test_raw_chunks.py)
   - Comprehensive search combining semantic and keyword approaches (comprehensive_search.py)

## Usage

### Cloud Function-based Semantic Search

To perform a semantic search from your application:

```javascript
// Initialize Firebase in your client app
const searchResults = await firebase.functions().httpsCallable('semanticSearch')({ 
  query: 'What are the symptoms of misophonia?',
  limit: 5,
  expandContext: true,
  filters: {
    year: [2010, 2023],  // Optional year range filter
    authors: ['Smith']   // Optional author filter
  }
});

// Process and display results
console.log(searchResults.data);
```

### Direct Semantic Search

For direct semantic search without using Cloud Functions:

```bash
cd scripts
python direct_semantic_search.py --query "What are the symptoms of misophonia?" --threshold 0.7
```

### Comprehensive Search

To use the combined semantic and keyword search capabilities:

```bash
cd scripts
python comprehensive_search.py --query "misophonia treatment approaches" --limit 5
```

### Raw Chunk Search

For simple keyword-based search and database statistics:

```bash
cd scripts
python test_raw_chunks.py --stats
python test_raw_chunks.py --topic "misophonia symptoms"
python test_raw_chunks.py --keyword "treatment"
```

## Current Status

- **Documents Processed**: 162 documents from the research collection
- **Unique Authors**: 142 researchers represented
- **Year Range**: 1978-2025 (comprehensive coverage)
- **Raw Chunks**: 56,680 chunks with optimized chunking
- **Processed Chunks**: 37,426 chunks with embeddings (66.0% progress)
- **Unique Documents with Embeddings**: 128 documents (79.0% of collection)
- **Search Capabilities**: Both semantic and keyword search working effectively with high relevance scores (>0.87)
- **Processing Efficiency**: Successfully scaled to 5000 chunks per batch with 100% success rate

## Next Steps

1. ‚úÖ Achieve 70+ unique documents with embeddings (completed: 128 documents)
2. ‚úÖ Achieve 100+ unique documents with embeddings (completed: 128 documents)
3. Continue embedding generation for remaining chunks to reach 100% coverage
4. Implement a web UI for searching the vector database
5. Add feedback mechanisms to improve search quality
6. Implement regular reindexing for new research documents
7. Add authentication and access controls
8. Optimize Cloud Functions for better performance with large result sets

## Resources

- [Firebase Vector Search Documentation](https://firebase.google.com/docs/firestore/vector-search)
- [Cloud Functions Documentation](https://firebase.google.com/docs/functions)
- [Firestore Documentation](https://firebase.google.com/docs/firestore)
- [OpenAI Embeddings Documentation](https://platform.openai.com/docs/guides/embeddings)
- [Implementation Plan](/documents/development/misophonia-vector-db-implementation-plan.md)
- [Processing Guide](/vector-db-processing-README.md)


================================================================================


################################################################################
# File: netlify/functions/research-assistant.js
################################################################################

import path from 'path';
import fetch from 'node-fetch';
import dotenv from 'dotenv';
import { OpenAI } from 'openai';

dotenv.config({ path: path.resolve(process.cwd(), 'server/.env') });

// Set AI provider here - "openai" or "gemini"
const AI_PROVIDER = process.env.AI_PROVIDER || "openai";

export async function handler(event, context) {
  if (event.httpMethod !== 'POST') {
    return { statusCode: 405, body: 'Method Not Allowed' };
  }
  
  let body;
  try {
    body = JSON.parse(event.body);
  } catch {
    return { statusCode: 400, body: JSON.stringify({ error: 'Invalid JSON' }) };
  }
  
  const { messages, topic } = body;
  
  if (!messages || !Array.isArray(messages)) {
    return { statusCode: 400, body: JSON.stringify({ error: 'Messages array required' }) };
  }
  
  // Using OpenAI
  if (AI_PROVIDER === "openai") {
    const openaiKey = process.env.OPENAI_API_KEY;
    if (!openaiKey) {
      return { statusCode: 500, body: JSON.stringify({ error: 'OPENAI_API_KEY not set' }) };
    }
    
    try {
      const openai = new OpenAI({ apiKey: openaiKey });
      
      // Format messages for OpenAI
      const systemPrompt = topic 
        ? `You are a knowledgeable research assistant focusing on the topic: ${topic}. Provide information about misophonia research.`
        : "You are a knowledgeable research assistant on misophonia research.";
      
      const openaiMessages = [
        { role: 'system', content: systemPrompt },
        ...messages
      ];
      
      const completion = await openai.chat.completions.create({
        model: 'gpt-4o',
        messages: openaiMessages,
        max_tokens: 1024,
        temperature: 0.7
      });
      
      const reply = completion.choices[0]?.message?.content || '';
      return { 
        statusCode: 200, 
        body: JSON.stringify({ 
          reply, 
          structured: null,
          provider: 'openai'
        }) 
      };
    } catch (error) {
      console.error("OpenAI API error:", error);
      return { statusCode: 500, body: JSON.stringify({ error: 'Error from OpenAI API' }) };
    }
  }
  
  // Using Gemini
  else {
    const apiKey = process.env.GEMINI_API_KEY;
    if (!apiKey) {
      return { statusCode: 500, body: JSON.stringify({ error: 'GEMINI_API_KEY not set' }) };
    }
    
    let userPrompt = '';
    if (topic && typeof topic === 'string') {
      userPrompt += `Topic: ${topic}\n`;
    }
    userPrompt += messages.map(m => `${m.role === 'user' ? 'User' : 'Assistant'}: ${m.content}`).join('\n');
    const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?key=${apiKey}`;
    
    try {
      const res = await fetch(apiUrl, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          contents: [{ role: 'user', parts: [{ text: userPrompt }] }],
          generationConfig: { temperature: 0.7, maxOutputTokens: 1024 },
          tools: [
            { function_declarations: [
                { name: 'structured_output', description: 'Return information in a structured JSON format for chat display, with sections, bullet points, and highlights.' }
              ]
            }
          ]
        })
      });
      
      if (!res.ok) {
        const err = await res.text();
        return { statusCode: 500, body: JSON.stringify({ error: 'Error from Gemini API', details: err }) };
      }
      
      const data = await res.json();
      let structured = null;
      if (data.candidates && data.candidates[0]?.content?.parts) {
        const part = data.candidates[0].content.parts[0];
        if (part.functionCall && part.functionCall.name === 'structured_output') {
          try {
            structured = JSON.parse(part.functionCall.args.json || '{}');
          } catch {
            structured = part.functionCall.args;
          }
        }
      }
      const reply = data.candidates?.[0]?.content?.parts?.[0]?.text || '';
      return { 
        statusCode: 200, 
        body: JSON.stringify({ 
          reply, 
          structured,
          provider: 'gemini'
        }) 
      };
    } catch (error) {
      console.error("Gemini API error:", error);
      return { statusCode: 500, body: JSON.stringify({ error: 'Error from Gemini API' }) };
    }
  }
}


================================================================================


################################################################################
# File: README.md
################################################################################

################################################################################
################################################################################

<!-- PROJECT LOGO -->
<p align="center">
  <img src="public/vite.svg" alt="Logo" width="120" height="120">
</p>

<h1 align="center">Misophonia Companion</h1>

<p align="center">
  <b>The modern, AI-powered guide and support tool for those living with misophonia.</b><br>
  <i>Built with React, Vite, Node.js, and OpenAI</i>
  <br><br>
  <a href="https://misophonia-companion.windsurf.build"><img src="https://img.shields.io/badge/Live%20Demo-Online-brightgreen?style=for-the-badge" alt="Live Demo"></a>
  <a href="https://github.com/mannino49/Misophonia-companion-v2"><img src="https://img.shields.io/github/stars/mannino49/Misophonia-companion-v2?style=for-the-badge" alt="GitHub Stars"></a>
</p>

---

## üöÄ Features

- **Conversational AI Chatbot:** Powered by OpenAI, get real-time support and information.
- **Soundscape Player:** Customizable soundscapes to help manage triggers.
- **Modern UI:** Responsive, accessible, and visually appealing interface.
- **Progressive Web App:** Installable and works offline.
- **Secure Backend:** All API keys and secrets are kept on the server, never exposed to the client.

---

## üñ•Ô∏è Tech Stack

<div align="center">
  <img src="https://img.shields.io/badge/React-20232A?style=for-the-badge&logo=react&logoColor=61DAFB" />
  <img src="https://img.shields.io/badge/Vite-646CFF?style=for-the-badge&logo=vite&logoColor=FFD62E" />
  <img src="https://img.shields.io/badge/Node.js-339933?style=for-the-badge&logo=nodedotjs&logoColor=white" />
  <img src="https://img.shields.io/badge/Express-000000?style=for-the-badge&logo=express&logoColor=white" />
  <img src="https://img.shields.io/badge/OpenAI-412991?style=for-the-badge&logo=openai&logoColor=white" />
  <img src="https://img.shields.io/badge/Netlify-00C7B7?style=for-the-badge&logo=netlify&logoColor=white" />
</div>

---

## üì¶ Project Structure

```shell
Misophonia Guide/
‚îú‚îÄ‚îÄ public/                # Static assets (icons, manifest)
‚îú‚îÄ‚îÄ src/                   # React frontend source
‚îÇ   ‚îú‚îÄ‚îÄ App.jsx            # Main app logic
‚îÇ   ‚îú‚îÄ‚îÄ main.jsx           # React entry point
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ server/                # Node.js/Express backend
‚îÇ   ‚îú‚îÄ‚îÄ index.js           # API server entry
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ netlify.toml           # Netlify deployment config
‚îú‚îÄ‚îÄ package.json           # Frontend config
‚îî‚îÄ‚îÄ ...
```

---

## ‚ö° Getting Started

### 1. Clone the repository
```bash
git clone https://github.com/mannino49/Misophonia-companion-v2.git
cd Misophonia-companion-v2
```

### 2. Install dependencies
```bash
npm install
cd server && npm install
```

### 3. Set up environment variables
- Copy `.env.example` to `.env` in the `server/` directory and add your OpenAI API key:
```
OPENAI_API_KEY=your_openai_key_here
```

### 4. Run the backend server
```bash
cd server
npm start
```

### 5. Run the frontend (in a new terminal)
```bash
npm run dev
```

- Frontend: [http://localhost:5173](http://localhost:5173)
- Backend API: [http://localhost:3001](http://localhost:3001)

---

## üåê Deployment

- Deployed on Netlify: [Live Demo](https://misophonia-companion.windsurf.build)
- Backend runs as a separate Node.js server (see `server/`)
- All secrets are stored in environment variables and never exposed to the frontend.

---

## üõ°Ô∏è Security & Best Practices

- **No secrets or API keys are stored in the frontend.**
- **.env files and private keys are gitignored.**
- **Backend validates API key presence and never exposes it to the client.**

---

## ü§ù Contributing

Contributions are welcome! Please open issues or submit pull requests.

---

## üìÑ License

MIT License. See [LICENSE](LICENSE) for details.

---

<p align="center">
  <b>Made with ‚ù§Ô∏è by Mannino49</b>
</p>


================================================================================


################################################################################
# File: netlify/functions/search.js
################################################################################

import { createClient } from '@supabase/supabase-js';
import { Configuration, OpenAIApi } from 'openai';

export async function handler(event, context) {
  if (event.httpMethod !== 'POST') {
    return { statusCode: 405, body: 'Method Not Allowed' };
  }
  
  try {
    const body = JSON.parse(event.body);
    const { query, filters, expandContext, page, pageSize, similarityThreshold } = body;
    
    // Initialize Supabase client
    const supabase = createClient(
      process.env.SUPABASE_URL,
      process.env.SUPABASE_SERVICE_ROLE_KEY
    );
    
    // Initialize OpenAI for embeddings
    const configuration = new Configuration({
      apiKey: process.env.OPENAI_API_KEY,
    });
    const openai = new OpenAIApi(configuration);
    
    // Generate embedding
    const embeddingResponse = await openai.createEmbedding({
      model: "text-embedding-ada-002",
      input: query,
    });
    
    const queryEmbedding = embeddingResponse.data.data[0].embedding;
    
    // Search using the RPC function
    const { data: results, error } = await supabase.rpc(
      'match_documents',
      {
        query_embedding: queryEmbedding,
        match_threshold: similarityThreshold || 0.6,
        match_count: pageSize || 10
      }
    );
    
    if (error) throw error;
    
    return {
      statusCode: 200,
      body: JSON.stringify({
        result: {
          results,
          page: page || 1,
          pageSize: pageSize || 10,
          query
        }
      })
    };
  } catch (error) {
    console.error('Error in search function:', error);
    return {
      statusCode: 500,
      body: JSON.stringify({ error: error.message })
    };
  }
}


================================================================================


################################################################################
# File: package.json
################################################################################

{
  "name": "misophonia-companion",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "vite build",
    "lint": "eslint .",
    "preview": "vite preview",
    "link": "npx netlify link",
    "env:import": "npx netlify env:import server/.env",
    "deploy": "npx netlify deploy --prod --dir=dist",
    "ingest": "node scripts/ingest.js"
  },
  "dependencies": {
    "@supabase/supabase-js": "^2.39.0",
    "dotenv": "^16.0.3",
    "firebase-admin": "^11.8.0",
    "http-proxy": "^1.18.1",
    "node-fetch": "^3.3.1",
    "openai": "^4.9.0",
    "pdfjs-dist": "^3.7.107",
    "react": "^19.0.0",
    "react-dom": "^19.0.0",
    "vite-plugin-pwa": "^1.0.0"
  },
  "devDependencies": {
    "@eslint/js": "^9.22.0",
    "@types/react": "^19.0.10",
    "@types/react-dom": "^19.0.4",
    "@vitejs/plugin-react": "^4.3.4",
    "eslint": "^9.22.0",
    "eslint-plugin-react-hooks": "^5.2.0",
    "eslint-plugin-react-refresh": "^0.4.19",
    "globals": "^16.0.0",
    "netlify-cli": "^13.2.2",
    "vite": "^6.3.1"
  }
}


================================================================================


################################################################################
# File: eslint.config.js
################################################################################

import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'

export default [
  { ignores: ['dist'] },
  {
    files: ['**/*.{js,jsx}'],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
      parserOptions: {
        ecmaVersion: 'latest',
        ecmaFeatures: { jsx: true },
        sourceType: 'module',
      },
    },
    plugins: {
      'react-hooks': reactHooks,
      'react-refresh': reactRefresh,
    },
    rules: {
      ...js.configs.recommended.rules,
      ...reactHooks.configs.recommended.rules,
      'no-unused-vars': ['error', { varsIgnorePattern: '^[A-Z_]' }],
      'react-refresh/only-export-components': [
        'warn',
        { allowConstantExport: true },
      ],
    },
  },
]


================================================================================


################################################################################
# File: netlify.toml
################################################################################

# Netlify configuration for Misophonia Companion
[build]
  command = "npm run build"
  publish = "dist"

[dev]
  command = "npm run dev"
  targetPort = 5173

[functions]
  directory = "netlify/functions"

# Redirect all requests to index.html for SPA routing
[[redirects]]
  from = "/api/*"
  to = "/.netlify/functions/:splat"
  status = 200

[[redirects]]
  from = "/api/gemini"
  to = "/.netlify/functions/research-assistant"
  status = 200

[[redirects]]
  from = "/api/rag"
  to = "/.netlify/functions/rag-search"
  status = 200

[[redirects]]
  from = "/*"
  to = "/index.html"
  status = 200


================================================================================


################################################################################
# File: gemini_test.js
################################################################################

import { GoogleGenAI } from "@google/genai";

const ai = new GoogleGenAI({ apiKey: "GEMINI_API_KEY" });

async function main() {
  const response = await ai.models.generateContent({
    model: "gemini-2.0-flash",
    contents: "Explain how AI works in a few words",
  });
  console.log(response.text);
}

main();


================================================================================


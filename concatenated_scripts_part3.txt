# Concatenated Project Code - Part 3 of 3
# Generated: 2025-05-09 15:45:06
# Root Directory: /Users/gianmariatroiani/Documents/misophonia-companion-v3
================================================================================

################################################################################
# File: src/App.jsx
################################################################################

import './App.css'
import React, { useState, useEffect, useRef } from 'react';
import botAvatar from './assets/bot-avatar.png';
import userAvatar from './assets/user-avatar.png';
import TermsModal from './TermsModal';
import RagAssistant from './RagAssistant.jsx';


function NavBar({ setSection, section }) {
  return (
    <nav className="navbar">
      <button className={section === 'home' ? 'active' : ''} onClick={() => setSection('home')}>
        <span role="img" aria-label="home" style={{marginRight: 6}}>üè†</span> Home
      </button>
      <button className={section === 'chatbot' ? 'active' : ''} onClick={() => setSection('chatbot')}>
        <span role="img" aria-label="chat" style={{marginRight: 6}}>üí¨</span> Let's Talk
      </button>
      <button className={section === 'tools' ? 'active' : ''} onClick={() => setSection('tools')}>
        <span role="img" aria-label="tools" style={{marginRight: 6}}>üß∞</span> Therapeutic Tools
      </button>
      <button className={section === 'research' ? 'active' : ''} onClick={() => setSection('research')}>
        <span role="img" aria-label="research" style={{marginRight: 6}}>üî¨</span> Research Assistant
      </button>
    </nav>
  );
}

const AFFIRMATIONS = [
  "You are safe here.",
  "It's okay to take a break.",
  "Your feelings are valid.",
  "Breathe in calm, breathe out stress.",
  "You are not alone."
];
const SOUNDS = [
  { label: 'Rain', src: 'https://cdn.pixabay.com/audio/2022/07/26/audio_124bfae45e.mp3' },
  { label: 'Forest', src: 'https://cdn.pixabay.com/audio/2022/03/15/audio_115b9d7bfa.mp3' },
  { label: 'White Noise', src: 'https://cdn.pixabay.com/audio/2022/03/15/audio_115b9d7bfa.mp3' }
];

function AffirmationBanner() {
  const [idx] = useState(() => Math.floor(Math.random() * AFFIRMATIONS.length));
  return (
    <div style={{
      background: 'linear-gradient(90deg, #e0e7ef 60%, #f8f6ff 100%)',
      color: '#4b6073',
      borderRadius: '16px',
      margin: '0 0 1.1rem 0',
      padding: '0.7rem 1.2rem',
      fontSize: '1.12rem',
      fontWeight: 500,
      textAlign: 'center',
      boxShadow: '0 1px 6px 0 rgba(31, 38, 135, 0.04)',
      letterSpacing: '0.01em',
      opacity: 0.97
    }}>
      {AFFIRMATIONS[idx]}
    </div>
  );
}

function SoundscapePlayer() {
  const [playing, setPlaying] = useState(false);
  const [muted, setMuted] = useState(false);
  const [soundIdx, setSoundIdx] = useState(0);
  const audioRef = useRef(null);

  const handlePlayPause = () => {
    setPlaying(p => !p);
  };
  const handleMute = () => {
    setMuted(m => !m);
  };
  const handleSoundChange = (e) => {
    setSoundIdx(Number(e.target.value));
    setPlaying(false);
    setTimeout(() => setPlaying(true), 50);
  };

  useEffect(() => {
    if (!audioRef.current) return;
    audioRef.current.muted = muted;
    if (playing) {
      audioRef.current.play();
    } else {
      audioRef.current.pause();
    }
  }, [playing, muted, soundIdx]);

  return (
    <div style={{
      display: 'flex', alignItems: 'center', gap: '0.7rem',
      background: 'rgba(255,255,255,0.88)',
      borderRadius: '13px',
      padding: '0.25rem 0.8rem',
      marginBottom: '1.1rem',
      boxShadow: '0 1px 4px 0 rgba(31, 38, 135, 0.03)',
      fontSize: '1.01rem',
      maxWidth: 320
    }}>
      <span style={{color: '#b2d8d8', fontWeight: 700, fontSize: '1.1rem'}}>Soundscape:</span>
      <select value={soundIdx} onChange={handleSoundChange} style={{borderRadius: 7, border: '1px solid #e0e7ef', background: '#f8f6ff', color: '#4b6073', padding: '0.2rem 0.5rem'}}>
        {SOUNDS.map((s, i) => <option value={i} key={s.label}>{s.label}</option>)}
      </select>
      <button onClick={handlePlayPause} style={{border: 'none', background: 'none', cursor: 'pointer', color: playing ? '#81b0b0' : '#aaa', fontWeight: 700, fontSize: '1.05rem'}}>{playing ? 'Pause' : 'Play'}</button>
      <button onClick={handleMute} style={{border: 'none', background: 'none', cursor: 'pointer', color: muted ? '#aaa' : '#b2d8d8', fontWeight: 700, fontSize: '1.05rem'}}>{muted ? 'Unmute' : 'Mute'}</button>
      <audio ref={audioRef} src={SOUNDS[soundIdx].src} loop preload="auto" style={{display: 'none'}} />
    </div>
  );
}

function App() {
  const [section, setSection] = useState('home');
  const [termsAccepted, setTermsAccepted] = useState(localStorage.getItem('termsAccepted') === 'true');
  if (!termsAccepted) return <TermsModal onAccept={() => setTermsAccepted(true)} />;

  return (
    <>
      <div className="animated-bg">
        <div className="bubble bubble1"></div>
        <div className="bubble bubble2"></div>
        <div className="bubble bubble3"></div>
        <div className="bubble bubble4"></div>
        <div className="bubble bubble5"></div>
        <div className="bubble bubble6"></div>
        <div className="bubble bubble7"></div>
        <div className="bubble bubble8"></div>
      </div>

      {/* MAIN CONTENT BOX */}
      <div className="container">
        <AffirmationBanner />
        <SoundscapePlayer />
        <NavBar setSection={setSection} section={section} />
        {section === 'home' && (
          <div className="card">
            <main>
              <h1 className="title">Welcome to Misophonia Companion</h1>
              <p className="subtitle">A soothing space to manage triggers, support healing, and explore research‚Äîbuilt for both sufferers and professionals.</p>
            </main>
          </div>
        )}
        {section === 'chatbot' && <Chatbot />}
        {section === 'tools' && (
          <div className="card">
            <main>
              <h2>Therapeutic Tools</h2>
              <p>Coming soon: Sound therapy, coping strategies, and relaxation exercises.</p>
            </main>
          </div>
        )}
        {section === 'research' && <RagAssistant />}
      </div>

      {/* NEW: footer lives here, outside .container */}
      <div className="disclaimer-footer">
        Misophonia Companion is not a clinical tool or a substitute for
        professional psychological or medical treatment. It does not provide
        diagnosis, therapy, or crisis intervention. If you are experiencing a
        mental-health emergency, please contact a licensed provider or
        emergency services immediately.
      </div>
    </>
  );
}


function Chatbot() {
  const [messages, setMessages] = useState([
    { sender: 'bot', text: 'Hello! I am your Misophonia Companion. How can I support you today?' }
  ]);
  const [input, setInput] = useState('');
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);

  async function handleSend(e) {
    e.preventDefault();
    if (!input.trim()) return;
    const userMsg = { sender: 'user', text: input };
    setMessages((msgs) => [...msgs, userMsg]);
    setLoading(true);
    setError(null);
    setInput('');
    try {
      const res = await fetch('/api/chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          messages: [
            { role: 'system', content: 'You are a calm, supportive misophonia companion and research assistant.' },
            ...messages.map(m => ({ role: m.sender === 'user' ? 'user' : 'assistant', content: m.text })),
            { role: 'user', content: input }
          ]
        })
      });
      if (!res.ok) throw new Error('API error');
      const data = await res.json();
      setMessages((msgs) => [...msgs, { sender: 'bot', text: data.reply }]);
    } catch (err) {
      setMessages((msgs) => [...msgs, { sender: 'bot', text: 'Sorry, I could not connect to the assistant. Make sure the backend server and your API key are set up.' }]);
      setError('API error');
    } finally {
      setLoading(false);
    }
  }

  return (
    <main>
      <h2>Let's Talk</h2>
      <div className="chatbot-box">
        <div className="chat-messages">
          {messages.map((msg, idx) => (
            <div key={idx} className={msg.sender === 'bot' ? 'msg bot' : 'msg user'} style={{display: 'flex', alignItems: 'flex-end', justifyContent: msg.sender === 'user' ? 'flex-end' : 'flex-start'}}>
              {msg.sender === 'bot' && (
                <img
                  src={botAvatar}
                  alt="Bot"
                  className="chat-avatar"
                  onError={e => { e.target.onerror = null; e.target.src = 'https://ui-avatars.com/api/?name=Bot&background=b2d8d8&color=fff&rounded=true&size=64'; }}
                />
              )}
              <span className="bubble-content">{msg.text}</span>
              {msg.sender === 'user' && (
                <img
                  src={userAvatar}
                  alt="You"
                  className="chat-avatar"
                  onError={e => { e.target.onerror = null; e.target.src = 'https://ui-avatars.com/api/?name=You&background=ffdac1&color=3a3a3a&rounded=true&size=64'; }}
                />
              )}
            </div>
          ))}
          {loading && <div className="msg bot" style={{display: 'flex', alignItems: 'flex-end'}}><img src={botAvatar} alt="Bot" className="chat-avatar" /><span className="bubble-content">Thinking‚Ä¶</span></div>}
        </div>
        <form className="chat-input-row" onSubmit={handleSend}>
          <input
            type="text"
            value={input}
            onChange={e => setInput(e.target.value)}
            placeholder="Type your message..."
            className="chat-input"
            autoFocus
            aria-label="Type your message"
            disabled={loading}
          />
          <button type="submit" className="chat-send" disabled={loading || !input.trim()}>Send</button>
        </form>
        {error && <div style={{ color: '#b22222', marginTop: '0.5rem' }}>
          Make sure your backend server is running and your OpenAI API key is set in <code>server/.env</code>.
        </div>}
      </div>
    </main>
  );
}



// GeminiChatbot: Gemini 2.5 Pro chat with topics and structured output
function GeminiChatbot() {
  const TOPICS = [
    { label: 'Neuroscience', value: 'Neuroscience' },
    { label: 'Genetics', value: 'Genetics' },
    { label: 'Therapy', value: 'Therapy' },
    { label: 'Advocacy', value: 'Advocacy' },
    { label: 'News', value: 'Latest News' },
    { label: 'Free Text', value: '' }
  ];
  const [messages, setMessages] = useState([
    { sender: 'bot', text: 'Hi! I am your Gemini-powered Research Assistant. Select a topic or ask anything about misophonia research.' }
  ]);
  const [input, setInput] = useState('');
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState(null);
  const [selectedTopic, setSelectedTopic] = useState(null);
  const [mode, setMode] = useState('gemini'); // future: allow OpenAI fallback

  async function handleGeminiSend(e, topicOverride) {
    if (e) e.preventDefault();
    const topicVal = topicOverride !== undefined ? topicOverride : selectedTopic;
    const userText = topicVal && topicVal !== '' && topicVal !== 'Free Text' ? topicVal : input;
    if (!userText.trim()) return;
    const userMsg = { sender: 'user', text: userText };
    setMessages((msgs) => [...msgs, userMsg]);
    setLoading(true);
    setError(null);
    setInput('');
    setSelectedTopic(null);
    try {
      const res = await fetch('/api/gemini', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          messages: messages.map(m => ({ role: m.sender === 'user' ? 'user' : 'assistant', content: m.text })).concat([{ role: 'user', content: userText }]),
          topic: topicVal && topicVal !== 'Free Text' ? topicVal : undefined
        })
      });
      if (!res.ok) throw new Error('API error');
      const data = await res.json();
      setMessages((msgs) => [...msgs, { sender: 'bot', text: data.reply, structured: data.structured }]);
    } catch (err) {
      setMessages((msgs) => [...msgs, { sender: 'bot', text: 'Sorry, Gemini API error. Check your backend and API key.' }]);
      setError('API error');
    } finally {
      setLoading(false);
    }
  }

  function renderStructured(structured) {
    if (!structured) return null;
    // Example structure: { sections: [{ title, bullets, highlights, ... }], ... }
    return (
      <div className="gemini-structured">
        {structured.sections && structured.sections.map((sec, i) => (
          <div key={i} className="g-section">
            {sec.title && <div className="g-title">{sec.title}</div>}
            {sec.highlights && Array.isArray(sec.highlights) && (
              <ul className="g-highlights">{sec.highlights.map((h, j) => <li key={j} className="g-highlight">{h}</li>)}</ul>
            )}
            {sec.bullets && Array.isArray(sec.bullets) && (
              <ul className="g-bullets">{sec.bullets.map((b, j) => <li key={j}>{b}</li>)}</ul>
            )}
            {sec.text && <div className="g-text">{sec.text}</div>}
          </div>
        ))}
        {structured.extra && <div className="g-extra">{structured.extra}</div>}
      </div>
    );
  }

  return (
    <div className="gemini-chatbot">
      <div className="gemini-toggle-row">
        <button
          className={mode === 'gemini' ? 'toggle-active' : ''}
          onClick={() => setMode('gemini')}
        >Gemini 2.5 Pro</button>
        {/* <button
          className={mode === 'openai' ? 'toggle-active' : ''}
          onClick={() => setMode('openai')}
        >OpenAI</button> */}
      </div>
      <div className="gemini-topic-row">
        {TOPICS.map(t => (
          <button
            key={t.label}
            className={selectedTopic === t.value ? 'topic-btn selected' : 'topic-btn'}
            onClick={() => {
              setSelectedTopic(t.value);
              if (t.value && t.value !== 'Free Text') handleGeminiSend(null, t.value);
            }}
            disabled={loading}
          >{t.label}</button>
        ))}
      </div>
      <div className="chat-messages">
        {messages.map((msg, idx) => (
          <div key={idx} className={msg.sender === 'bot' ? 'msg bot' : 'msg user'} style={{display: 'flex', alignItems: 'flex-end', justifyContent: msg.sender === 'user' ? 'flex-end' : 'flex-start'}}>
            {msg.sender === 'bot' && (
              <img
                src={botAvatar}
                alt="Bot"
                className="chat-avatar"
                onError={e => { e.target.onerror = null; e.target.src = 'https://ui-avatars.com/api/?name=Bot&background=b2d8d8&color=fff&rounded=true&size=64'; }}
              />
            )}
            <span className="bubble-content">
              {msg.structured ? renderStructured(msg.structured) : msg.text}
            </span>
            {msg.sender === 'user' && (
              <img
                src={userAvatar}
                alt="You"
                className="chat-avatar"
                onError={e => { e.target.onerror = null; e.target.src = 'https://ui-avatars.com/api/?name=You&background=ffdac1&color=3a3a3a&rounded=true&size=64'; }}
              />
            )}
          </div>
        ))}
        {loading && <div className="msg bot" style={{display: 'flex', alignItems: 'flex-end'}}><img src={botAvatar} alt="Bot" className="chat-avatar" /><span className="bubble-content">Thinking‚Ä¶</span></div>}
      </div>
      <form className="chat-input-row" onSubmit={e => handleGeminiSend(e)}>
        <input
          type="text"
          value={input}
          onChange={e => setInput(e.target.value)}
          placeholder="Type your question or pick a topic..."
          className="chat-input"
          autoFocus
          aria-label="Type your message"
          disabled={loading}
        />
        <button type="submit" className="chat-send" disabled={loading || !input.trim()}>Send</button>
      </form>
      {error && <div style={{ color: '#b22222', marginTop: '0.5rem' }}>
        Gemini backend error. Check your server and API key in <code>server/.env</code>.
      </div>}
    </div>
  );
}

export default App


================================================================================


################################################################################
# File: scripts/docling_batch_convert_with_metadata.py
################################################################################

#!/usr/bin/env python3
# scripts/docling_batch_convert_with_metadata.py
"""
Convert every PDF under  documents/research/Global
‚Üí flat TXT (documents/research/txt/) **and** rich JSON
(documents/research/json/) that carries per‚Äëpage text plus
bibliographic metadata (title, authors, year, journal, DOI,
abstract, keywords, research_topics).

The script is idempotent: if both TXT and JSON already
exist it skips the file; if one is missing it creates just that one.
"""

from __future__ import annotations

import argparse
import json
import logging
import os
import pathlib
import re
from datetime import datetime
from typing import Any, Dict, Iterable, List

import PyPDF2
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.document_converter import DocumentConverter, PdfFormatOption
from rich.logging import RichHandler
from tqdm import tqdm

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ optional unstructured fallback ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #

try:
    from unstructured.partition.pdf import partition_pdf

    HAVE_UNSTRUCTURED = True
except Exception:  # pragma: no cover
    HAVE_UNSTRUCTURED = False

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ logging ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #


def init_logging(level: str = "INFO") -> None:
    logging.basicConfig(
        level=getattr(logging, level.upper(), "INFO"),
        format="%(asctime)s ‚Äî %(levelname)s ‚Äî %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
        handlers=[RichHandler()],
    )


log = logging.getLogger(__name__)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Docling converter helper ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #


def make_converter() -> DocumentConverter:
    pdf_opts = PdfPipelineOptions()
    pdf_opts.do_ocr = True  # scan‚Äësafe
    return DocumentConverter(
        format_options={InputFormat.PDF: PdfFormatOption(pipeline_options=pdf_opts)}
    )


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ path helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #


def iter_pdfs(root: pathlib.Path) -> Iterable[pathlib.Path]:
    for p in root.rglob("*.pdf"):
        if p.is_file() and not p.name.startswith("."):
            yield p


def txt_path_for(src: pathlib.Path, txt_dir: pathlib.Path) -> pathlib.Path:
    return txt_dir / f"{src.stem}.txt"


def json_path_for(src: pathlib.Path, json_dir: pathlib.Path) -> pathlib.Path:
    return json_dir / f"{src.stem}.json"


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ bibliographic helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #

META_FIELDS = [
    "title",
    "authors",
    "year",
    "journal",
    "doi",
    "abstract",
    "keywords",
    "research_topics",
]


def _authors(val: Any) -> List[str]:
    if val is None:
        return []
    if isinstance(val, list):
        return [str(a).strip() for a in val if a]
    return re.split(r"\s*,\s*|\s+and\s+", str(val).strip())


def extract_bib_from_filename(pdf: pathlib.Path) -> Dict[str, Any]:
    """
    Infer author/year/title from filenames like ‚ÄúSmith¬†2019¬†Some¬†Paper.pdf‚Äù.
    """
    stem = pdf.stem
    m = re.search(r"\b(19|20)\d{2}\b", stem)
    year = int(m.group(0)) if m else None
    if m:
        author = stem[: m.start()].strip()
        title = stem[m.end() :].strip(" -_")
    else:
        parts = stem.split(" ", 1)
        author = parts[0]
        title = parts[1] if len(parts) == 2 else None

    return {"authors": [author] if author else [], "year": year, "title": title}


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ header‚Äëtext regex extraction (journal / DOI ‚Ä¶) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #


DOI_RE = re.compile(r"\b(10\.\d{4,9}/[-._;()/:A-Z0-9]+)", re.I)
JOURNAL_RE = re.compile(
    r"(Journal|Journals|Revista|Proceedings|Annals|Neuroscience|Psychiatry|Psychology|Nature|Science)[^\n]{0,120}",
    re.I,
)
ABSTRACT_RE = re.compile(r"(?<=\bAbstract\b[:\s])(.{50,2000}?)(?:\n[A-Z][^\n]{0,60}\n|\Z)", re.S)
KEYWORDS_RE = re.compile(r"\bKey\s*words?\b[:\s]*(.+)", re.I)


def extract_bib_from_header(header_txt: str) -> Dict[str, Any]:
    """
    Very tolerant regexes on the first pages' raw text.
    """
    meta: Dict[str, Any] = {}

    doi = DOI_RE.search(header_txt)
    if doi:
        meta["doi"] = doi.group(1)

    jour = JOURNAL_RE.search(header_txt)
    if jour:
        meta["journal"] = " ".join(jour.group(0).split())

    abst = ABSTRACT_RE.search(header_txt)
    if abst:
        meta["abstract"] = " ".join(abst.group(1).split())

    kws = KEYWORDS_RE.search(header_txt)
    if kws:
        meta["keywords"] = [k.strip(" ;.,") for k in re.split(r"[;,]", kws.group(1)) if k.strip()]

    # derive research_topics from keywords
    if meta.get("keywords"):
        meta["research_topics"] = meta["keywords"]

    return meta


def merge_metadata(*sources: Dict[str, Any]) -> Dict[str, Any]:
    """
    Deep, ordered merge of metadata sources according to META_FIELDS precedence.
    """
    merged: Dict[str, Any] = {"doc_type": "scientific paper"}
    for field in META_FIELDS:
        merged[field] = None

    for src in sources:  # earlier dicts have lower priority
        for k in META_FIELDS:
            v = src.get(k)
            if v not in (None, "", [], {}):
                merged[k] = v

    # canonical types / defaults
    merged["authors"] = _authors(merged["authors"])
    merged["keywords"] = merged["keywords"] or []
    merged["research_topics"] = merged["research_topics"] or []
    return merged


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ per‚Äëpage text extraction helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #


def sections_from_docling(doc) -> List[Dict[str, Any]]:
    """
    First choice: iterate doc.pages (reliable).  Fallback: split on \f.
    """
    secs: List[Dict[str, Any]] = []
    try:
        pages = getattr(doc, "pages", None)
        if pages:  # new Docling API
            for idx, page in enumerate(pages, 1):
                try:
                    txt = page.export_to_text()
                except Exception:  # pragma: no cover
                    txt = ""
                if txt and txt.strip():
                    pn = getattr(page, "metadata", {}).get("page_number", idx)
                    secs.append(
                        {
                            "section": f"Page {pn}",
                            "page_number": pn,
                            "text": txt.strip(),
                        }
                    )
        if secs:
            return secs
    except Exception as e:  # pragma: no cover
        log.debug("Docling page iteration failed: %s", e)

    # ---- fallback to page_break marker ---- #
    try:
        full = doc.export_to_text(page_break_marker="\f")
        if "\f" in full:
            for idx, txt in enumerate(full.split("\f"), 1):
                t = txt.strip()
                if t:
                    secs.append({"section": f"Page {idx}", "page_number": idx, "text": t})
        elif full.strip():
            secs.append({"section": "Page 1", "page_number": 1, "text": full.strip()})
    except Exception as e:
        log.debug("Docling export_to_text failed: %s", e)
    return secs


def sections_from_unstructured(pdf: pathlib.Path) -> List[Dict[str, Any]]:
    elements = partition_pdf(str(pdf), strategy="hi_res")
    sections: List[Dict[str, Any]] = []
    buf, cur = "", None
    for el in elements:
        page = getattr(el.metadata, "page_number", None)
        if page is None:
            continue
        if cur is None:
            cur = page
        if page != cur:
            sections.append(
                {"section": f"Page {cur}", "page_number": cur, "text": buf.strip()}
            )
            buf, cur = "", page
        buf += " " + str(el)
    if buf.strip():
        sections.append(
            {"section": f"Page {cur}", "page_number": cur, "text": buf.strip()}
        )
    return sections


def sections_from_pypdf(pdf: pathlib.Path) -> List[Dict[str, Any]]:
    secs: List[Dict[str, Any]] = []
    with open(pdf, "rb") as f:
        for i, page in enumerate(PyPDF2.PdfReader(f).pages, 1):
            try:
                txt = page.extract_text() or ""
            except Exception:
                txt = ""
            secs.append({"section": f"Page {i}", "page_number": i, "text": txt.strip()})
    return secs


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ single‚Äëfile processing pipeline ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #


def process_one(
    conv: DocumentConverter,
    pdf: pathlib.Path,
    txt_dir: pathlib.Path,
    json_dir: pathlib.Path,
    overwrite: bool,
) -> bool:
    txt_path = txt_path_for(pdf, txt_dir)
    json_path = json_path_for(pdf, json_dir)

    has_txt, has_json = txt_path.exists(), json_path.exists()
    if has_txt and has_json and not overwrite:
        log.debug("‚úì Skip %s (TXT+JSON present)", pdf.name)
        return False

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Docling first ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
    doc_meta: Dict[str, Any] = {}
    sections: List[Dict[str, Any]] = []
    try:
        bundle = conv.convert(str(pdf))
        doc = bundle.document
        meta_obj = getattr(doc, "metadata", None)
        if meta_obj is not None:
            for meth in ("model_dump", "dict", "to_dict"):
                if hasattr(meta_obj, meth):
                    doc_meta = getattr(meta_obj, meth)()
                    break
        sections = sections_from_docling(doc)
    except Exception as e:
        log.debug("Docling failed on %s (%s) ‚Äì trying fallbacks", pdf.name, e)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ fallbacks if Docling text inadequate ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
    if not sections or all(len(s["text"]) < 40 for s in sections):
        if HAVE_UNSTRUCTURED:
            try:
                sections = sections_from_unstructured(pdf)
            except Exception as e:
                log.debug("unstructured failed on %s (%s)", pdf.name, e)
        if not sections:
            sections = sections_from_pypdf(pdf)

    if not sections:
        raise RuntimeError("no text extracted")

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ compile metadata ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
    first_pages_text = " ".join(s["text"] for s in sections[:2])[:12000]
    header_meta = extract_bib_from_header(first_pages_text)
    filename_meta = extract_bib_from_filename(pdf)
    meta = merge_metadata(doc_meta, filename_meta, header_meta)

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ assemble JSON payload ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
    payload = {
        **meta,
        "created_at": datetime.utcnow().isoformat() + "Z",
        "source_pdf": str(pdf),
        "sections": sections,
    }

    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ write outputs ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #
    if overwrite or not has_json:
        json_path.parent.mkdir(parents=True, exist_ok=True)
        json_path.write_text(
            json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8"
        )

    if overwrite or not has_txt:
        txt_path.parent.mkdir(parents=True, exist_ok=True)
        txt_path.write_text("\n\n".join(s["text"] for s in sections), encoding="utf-8")

    return True


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ batch driver ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #


def run(
    src_root: pathlib.Path,
    txt_dir: pathlib.Path,
    json_dir: pathlib.Path,
    overwrite: bool,
    limit: int | None,
) -> None:
    conv = make_converter()
    pdfs = list(iter_pdfs(src_root))
    if limit:
        pdfs = pdfs[:limit]

    processed = skipped = failed = 0
    log.info("Starting ‚Äì %s PDFs (src=%s)", len(pdfs), src_root)

    with tqdm(total=len(pdfs), unit="file", desc="Processing") as bar:
        for pdf in pdfs:
            bar.set_postfix_str(pdf.name)
            try:
                changed = process_one(conv, pdf, txt_dir, json_dir, overwrite)
                if changed:
                    processed += 1
                else:
                    skipped += 1
            except Exception as e:
                failed += 1
                log.error("‚ÄºÔ∏è  %s failed: %s", pdf.name, e)
            bar.update()

    log.info(
        "Done. processed=%s  skipped=%s  failed=%s  (TXT ‚Üí %s , JSON ‚Üí %s)",
        processed,
        skipped,
        failed,
        txt_dir,
        json_dir,
    )


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CLI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #


def cli() -> None:
    p = argparse.ArgumentParser(
        description="Convert research PDFs to TXT and JSON with metadata"
    )
    p.add_argument("--src", default="documents/research/Global", help="PDF folder")
    p.add_argument("--txt-dir", default="documents/research/txt", help="TXT output dir")
    p.add_argument("--json-dir", default="documents/research/json", help="JSON output dir")
    p.add_argument("--overwrite", action="store_true", help="Force re‚Äëcreate outputs")
    p.add_argument(
        "--log",
        default="INFO",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        help="Console log level",
    )
    p.add_argument("--max", type=int, default=0, help="Process at most N PDFs")
    args = p.parse_args()

    init_logging(args.log)
    run(
        pathlib.Path(args.src),
        pathlib.Path(args.txt_dir),
        pathlib.Path(args.json_dir),
        args.overwrite,
        None if args.max == 0 else args.max,
    )


if __name__ == "__main__":
    cli()


================================================================================


################################################################################
# File: src/App.css
################################################################################

/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
/* GLOBAL RESET                                 */
html, body {
  margin: 0 !important;      /* kill the 8 px UA margin */
  padding: 0;
  height: 100%;              /* body fills the viewport vertically */
}

/* optional ‚Äì keeps sizing predictable everywhere */
*, *::before, *::after { box-sizing: border-box; }
/* ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */

/* ----- BODY --------------------------------------------- */
body {
  min-height: 100vh;
  margin: 0;
  font-family: 'Nunito', 'Segoe UI', 'Helvetica Neue', Arial, 'Liberation Sans', sans-serif;
  color: #3a3a3a;
  background: linear-gradient(120deg, #f8f6ff 0%, #b2d8d8 100%);
  overflow-x: hidden;
  display: flex;
  flex-direction: column;
  align-items: center;
}

.animated-bg {
  position: fixed;
  z-index: 0;
  width: 100vw;
  height: 100vh;
  top: 0; left: 0;
  pointer-events: none;
  overflow: hidden;
}

.bubble {
  position: absolute;
  border-radius: 50%;
  opacity: 0.3;
  animation: float 18s infinite linear;
  background: #b2d8d8;
}
.bubble1 { width: 120px; height: 120px; left: 10vw; top: 80vh; animation-delay: 0s; background: #b2d8d8; }
.bubble2 { width: 80px; height: 80px; left: 70vw; top: 70vh; animation-delay: 4s; background: #f8f6ff; }
.bubble3 { width: 90px; height: 90px; left: 50vw; top: 90vh; animation-delay: 8s; background: #e0e7ef; }
.bubble4 { width: 60px; height: 60px; left: 20vw; top: 85vh; animation-delay: 2s; background: #f7cac9; }
.bubble5 { width: 100px; height: 100px; left: 80vw; top: 95vh; animation-delay: 6s; background: #b5ead7; }
.bubble6 { width: 70px; height: 70px; left: 30vw; top: 92vh; animation-delay: 10s; background: #ffdac1; }
.bubble7 { width: 110px; height: 110px; left: 60vw; top: 85vh; animation-delay: 12s; background: #c7ceea; }
.bubble8 { width: 50px; height: 50px; left: 40vw; top: 98vh; animation-delay: 14s; background: #f6dfeb; }
@keyframes float {
  0% { transform: translateY(0); }
  100% { transform: translateY(-90vh); }
}

/* ----- LAYOUT WRAPPER ----------------------------------- */
.container {
  max-width: 860px;
  margin: 0 auto;
  padding: 0 1rem;
  width: 100%;

  display: flex;
  flex-direction: column;
  align-items: center;

  z-index: 1;
  position: relative;
  min-width: 340px;
  background: none;
  box-shadow: none;
}

/* ----- CARD --------------------------------------------- */
.card {
  background: rgba(255,255,255,0.96);
  border-radius: 22px;
  box-shadow: 0 6px 32px rgba(31,38,135,.13);
  padding: 2rem 1.2rem;
  margin: 2.5rem 0;
  max-width: 520px;
  width: 100%;
  display: flex;
  flex-direction: column;
  align-items: center;
  transition: box-shadow 0.18s;
}

/* mobile tweaks ‚Äì ONE block */
@media (max-width: 700px) {
  .container { 
    max-width: 98vw; 
    padding: 1rem; 
  }
  .card { 
    padding: 1.2rem 0.5rem; 
    border-radius: 16px; 
    max-width: 99vw;
  }
  .navbar {
    flex-direction: column;
    flex-wrap: wrap;
    gap: 0.5rem;
    font-size: 0.97rem;
  }
  .chatbot-box {
    padding: 1rem 0.3rem;
    border-radius: 16px;
  }
  .msg {
    font-size: 0.99rem;
  }
  .chat-avatar {
    width: 30px; height: 30px;
  }
}

/* ----- NAVBAR ------------------------------------------- */
.navbar {
  display: flex;
  justify-content: center;
  gap: 1rem;
  flex-wrap: wrap;
  margin-bottom: 2rem;
}

.navbar button {
  background: #e6e6fa;
  color: #3a3a3a;
  border: none;
  padding: 0.7rem 1.5rem;
  border-radius: 18px;
  font-size: 1rem;
  font-weight: 500;
  cursor: pointer;
  transition: background 0.2s, color 0.2s;
  box-shadow: 0 2px 8px rgba(178, 216, 216, 0.09);
}

.navbar button.active, .navbar button:hover {
  background: #b2d8d8;
  color: #234e52;
}

.title {
  font-size: 2.2rem;
  margin-bottom: 0.5rem;
  color: #234e52;
  font-family: 'Segoe UI', 'Helvetica Neue', Arial, 'Liberation Sans', sans-serif;
}

.subtitle {
  font-size: 1.2rem;
  color: #6d7b8d;
  margin-bottom: 2rem;
}

main {
  padding: 1.5rem 0;
}

h2 {
  color: #234e52;
  margin-bottom: 1rem;
}

/* Chatbot styles (always apply) */
.chatbot-box {
  background: linear-gradient(135deg, rgba(248,246,255,0.93) 60%, rgba(178,216,216,0.10) 100%);
  border-radius: 26px;
  box-shadow: 0 4px 28px 0 rgba(31, 38, 135, 0.11), 0 1.5px 5px 0 rgba(178,216,216,0.08);
  padding: 2.2rem 1.2rem 1.5rem 1.2rem;
  margin: 0.5rem 0 0.5rem 0;
  min-height: 350px;
  width: 100%;
  display: flex;
  flex-direction: column;
  border: 1.5px solid #e0e7ef;
  backdrop-filter: blur(3px);
  position: relative;
  transition: box-shadow 0.2s;
}

.chat-messages {
  flex: 1;
  overflow-y: auto;
  margin-bottom: 1.1rem;
  display: flex;
  flex-direction: column;
  gap: 0.7rem;
  padding-bottom: 0.5rem;
  border-radius: 18px 18px 8px 8px;
  animation: fadeIn 0.7s;
}
@keyframes fadeIn {
  from { opacity: 0; transform: translateY(16px); }
  to { opacity: 1; transform: translateY(0); }
}

.msg {
  max-width: 80%;
  display: flex;
  align-items: flex-end;
  gap: 0.6rem;
  font-size: 1.08rem;
  word-break: break-word;
  box-shadow: 0 1px 6px 0 rgba(31, 38, 135, 0.03);
  margin-bottom: 0.1rem;
}

.msg.user {
  align-self: flex-end;
  flex-direction: row-reverse;
}

.msg.bot {
  align-self: flex-start;
}

.bubble-content {
  padding: 0.7rem 1.1rem;
  border-radius: 16px;
  background: linear-gradient(90deg, #b2d8d8 60%, #e0e7ef 100%);
  color: #2a3d3d;
  font-size: 1.08rem;
  box-shadow: 0 1px 6px 0 rgba(31, 38, 135, 0.03);
}

.msg.bot .bubble-content {
  background: linear-gradient(90deg, #f8f6ff 60%, #e9f7f7 100%);
  color: #5b3a6b;
}

.chat-avatar {
  width: 38px;
  height: 38px;
  border-radius: 50%;
  margin: 0 0.2rem;
  box-shadow: 0 2px 8px 0 rgba(31, 38, 135, 0.05);
  background: #fff;
  object-fit: cover;
}

.chat-input-row {
  display: flex;
  gap: 0.6rem;
  border-top: 1.5px solid #e0e7ef;
  padding-top: 1rem;
  margin-top: 0.3rem;
  align-items: center;
}

.chat-input {
  flex: 1;
  height: 48px;
  padding: 0 1rem;
  border-radius: 12px;
  border: 1.5px solid #c6e2e2;
  font-size: 1.08rem;
  outline: none;
  background: #f4fafd;
  color: #2a3d3d;
  box-sizing: border-box;
  transition: border-color 0.18s;
}

.chat-input:focus {
  border-color: #b2d8d8;
}

.chat-send {
  height: 48px;
  min-width: 90px;
  padding: 0 1.2rem;
  border-radius: 12px;
  border: none;
  background: #b2d8d8;
  color: #fff;
  font-weight: bold;
  font-size: 1.08rem;
  cursor: pointer;
  transition: background 0.2s, box-shadow 0.18s;
  box-shadow: 0 1.5px 6px 0 rgba(31, 38, 135, 0.07);
  display: flex;
  align-items: center;
  justify-content: center;
}

.chat-send:hover:not(:disabled) {
  background: #81b0b0;
  box-shadow: 0 4px 16px 0 rgba(31, 38, 135, 0.13);
}

.chat-send:disabled {
  background: #f4fafd;
  color: #b2d8d8;
  cursor: not-allowed;
  border: 1.5px solid #e0e7ef;
  box-shadow: none;
}

/* More forceful styling for avatar labels */
.chat-avatar, [class*="avatar"], [class*="circle"] {
  font-size: 10px !important;
  width: 45px !important;
  height: 45px !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
}

/* ----- RESEARCH-ASSISTANT POP-UP ------------------------ */
.ra-wrapper {
  position: fixed;            /* stick to viewport */
  bottom: 84px;               /* enough to clear the floating disclaimer */
  left: 50%;
  transform: translateX(-50%);
  width: min(1100px, 96vw);    /* A little wider on desktop, still fully responsive on phones */
  max-height: 70vh;           /* room for long answers w/ scroll */
  overflow-y: auto;
  border-radius: 18px;
  background: #ffffff;
  box-shadow: 0 8px 22px rgba(0,0,0,.07);
  padding: 2rem 2.25rem;
}

/* ‚îÄ‚îÄ‚îÄ research-assistant sticky search bar ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
.rag-form{
  position: sticky;
  bottom: 0;                  /* hugs the card's bottom edge            */
  z-index: 2;
  padding: 1rem 0 0.5rem;     /* little breathing room                  */
  background: rgba(255,255,255,.96);   /* white backdrop while it floats */
  backdrop-filter: blur(3px);
}

.ra-input-row {
  display: flex;
  gap: 0.6rem;
  margin-bottom: 1.25rem;
}

.ra-input {
  flex: 1 1 auto;
  font-size: 1.05rem;
  padding: 0.65rem 0.9rem;
  border: 2px solid #d0e4e4;
  border-radius: 10px;
}

.ra-btn {
  background: #669c99;
  color: #fff;
  border: 0;
  padding: 0 1.25rem;
  border-radius: 10px;
  font-weight: 600;
  cursor: pointer;
}

/* ----- DISCLAIMER (single source of truth) --------------- */
.disclaimer-footer {
  position: fixed;
  bottom: 0;            /* 0 means literally the last pixel */
  left: 0;
  width: 100%;
  font-size: 0.75rem;
  color: #666;
  text-align: center;
  padding: 0;       /* adjust or set to 0 for no inner gap */
  background: transparent;      /* or rgba(255,255,255,.85) if you want a strip */
  z-index: 30;          /* higher than bubbles / page cards */
}

/* ‚îÄ‚îÄ‚îÄ RAG assistant layout tweak ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
.rag-card {
  max-height: 75vh;               /* card never taller than viewport */
  overflow-y: auto;               /* inner scroll, NOT page scroll   */
  display: flex;
  flex-direction: column;         /* children stack top‚Üíbottom       */
}

/* scrolling area for answer + sources */
.rag-body {
  flex: 1 1 auto;                 /* fill remaining height           */
  overflow-y: auto;
}

/* search row always visible inside the scrolling card */
.rag-input-row {
  flex-shrink: 0;                 /* never shrinks                   */
  position: sticky;
  bottom: 0;                      /* sticks to card's bottom         */
  background: #fff;               /* white strip over scrolled text  */
  padding-top: 1rem;              /* add gap so it doesn't hug text  */
}

/* ‚îÄ‚îÄ‚îÄ RAG card: same width as before, inner scroll, sticky input ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ */
.ra-card {
  width: 100%;                 /* full width of the central container    */
  max-width: 1000px;           /* equals the old Bootstrap template      */
  margin: 0 auto;              /* keep it centred                        */

  /* original "card" visual style (copied, not changed) */
  background: rgba(255,255,255,0.96);
  border-radius: 22px;
  box-shadow: 0 6px 32px rgba(31,38,135,.13);
  padding: 2rem 1.2rem;

  /* NEW: make the inside scroll but freeze the input row */
  display: flex;
  flex-direction: column;
  max-height: 75vh;            /* never taller than the viewport         */
  overflow-y: auto;
}

/* scrolling part (answer + citations) */
.ra-body {
  flex: 1 1 auto;
  overflow-y: auto;
}

/* sticky search row ‚Äì always visible */
.ra-input-row {
  flex-shrink: 0;
  position: sticky;
  bottom: 0;
  background: #fff;            /* white strip so text doesn't peek under */
  padding-top: 1rem;           /* gap between content & bar              */
}

/* RAG research card & footer separation ---------------- */
.ra-card, .rag-card {
  /* just ~1 em gap ‚Äì enough to clear the footer without a chasm */
  margin-bottom: 1.3rem;
}

/* keep the search bar fixed just above the footer text (‚âà24 px) */
.ra-input-row {
  position: sticky;
  bottom: 1.5rem;
}


================================================================================


################################################################################
# File: scripts/process_research_metadata.py
################################################################################

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Research Metadata Processor

This script:
1. Iterates over text files in documents/research/txt
2. For each file:
   - Takes first 3000 words from the text
   - Makes an OpenAI API call with a specific prompt
   - Updates the corresponding JSON file in documents/research/json
   - Records processed files to avoid reprocessing on subsequent runs

Required environment variables:
------------------------------
OPENAI_API_KEY

Usage:
-----
python process_research_metadata.py [--batch-size N] [--force] [--model MODEL]
"""

import argparse
import json
import logging
import os
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Set

from dotenv import load_dotenv
from openai import OpenAI
from tqdm import tqdm

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Configuration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #

load_dotenv()

OPENAI_API_KEY: Optional[str] = os.environ.get("OPENAI_API_KEY")
DEFAULT_MODEL = "gpt-4.1-mini-2025-04-14"

# Paths
SCRIPT_DIR = Path(__file__).resolve().parent
ROOT_DIR = SCRIPT_DIR.parent
TXT_DIR = ROOT_DIR / "documents" / "research" / "txt"
JSON_DIR = ROOT_DIR / "documents" / "research" / "json"
PROCESSED_FILE = ROOT_DIR / "scripts" / "processed_files.json"

# Initialize logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s ‚Äî %(levelname)s ‚Äî %(message)s",
)
log = logging.getLogger(__name__)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #

def load_processed_files() -> Set[str]:
    """Load the set of already processed files."""
    if PROCESSED_FILE.exists():
        with open(PROCESSED_FILE, "r") as f:
            try:
                return set(json.load(f))
            except json.JSONDecodeError:
                log.warning("Error decoding processed files list, starting fresh")
    return set()

def save_processed_files(processed: Set[str]) -> None:
    """Save the set of processed files."""
    with open(PROCESSED_FILE, "w") as f:
        json.dump(list(processed), f, indent=2)

def extract_first_n_words(text: str, n: int = 3000) -> str:
    """Extract first n words from text."""
    words = text.split()
    return " ".join(words[:n])

def get_corresponding_json_path(txt_path: Path) -> Path:
    """Get the path to the corresponding JSON file."""
    return JSON_DIR / f"{txt_path.stem}.json"

def generate_metadata(client: OpenAI, text: str, model: str) -> Dict[str, Any]:
    """Call OpenAI API to generate metadata from text."""
    prompt = f"""
Extract the following metadata from this scientific paper and return exactly one JSON object with keys:
  ‚Ä¢ doc_type (e.g. "scientific paper")
  ‚Ä¢ title
  ‚Ä¢ authors (array of strings)
  ‚Ä¢ year (integer)
  ‚Ä¢ journal (string or null)
  ‚Ä¢ DOI (string or null)
  ‚Ä¢ abstract (string or null)
  ‚Ä¢ keywords (array of strings)
  ‚Ä¢ research_topics (array of strings)

If a field is not present, set it to null or an empty array. Here is the paper's full text:

{text}
"""

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are an assistant that extracts structured metadata from scientific papers."},
                {"role": "user", "content": prompt}
            ]
        )
        
        content = response.choices[0].message.content
        
        # Extract JSON from response (in case there's additional text)
        json_match = re.search(r'({[\s\S]*})', content)
        if json_match:
            content = json_match.group(1)
            
        return json.loads(content)
    except Exception as e:
        log.error(f"Error calling OpenAI API: {e}")
        raise

def update_json_file(json_path: Path, metadata: Dict[str, Any]) -> None:
    """Update JSON file with metadata from API response."""
    try:
        # Create if not exists
        if not json_path.exists():
            json_data = {
                "doc_type": "scientific paper",
                "title": "",
                "authors": [],
                "year": None,
                "journal": None,
                "doi": None,
                "abstract": None,
                "keywords": [],
                "research_topics": [],
                "created_at": datetime.utcnow().isoformat() + "Z",
                "source_pdf": "",
                "sections": []
            }
        else:
            # Read existing JSON file
            with open(json_path, "r", encoding="utf-8") as f:
                json_data = json.load(f)

        # Update with new metadata
        for key, value in metadata.items():
            if key in json_data and value not in (None, [], ""):
                json_data[key] = value

        # Write updated JSON file
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(json_data, f, indent=2, ensure_ascii=False)
            
        return True
    except Exception as e:
        log.error(f"Error updating JSON file {json_path}: {e}")
        return False

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Main Process ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ #

def process_files(client: OpenAI, batch_size: int, force: bool, model: str) -> Dict[str, Any]:
    """Process text files and update JSON files with metadata."""
    results = {
        "processed": 0,
        "skipped": 0,
        "errors": 0,
        "file_results": []
    }
    
    # Get all text files
    txt_files = list(TXT_DIR.glob("*.txt"))
    log.info(f"Found {len(txt_files)} text files in {TXT_DIR}")
    
    # Load set of processed files
    processed_files = set() if force else load_processed_files()
    
    # Filter unprocessed files or limit to batch size
    if not force:
        txt_files = [f for f in txt_files if f.name not in processed_files]
    
    log.info(f"Found {len(txt_files)} unprocessed files")
    if batch_size > 0:
        txt_files = txt_files[:batch_size]
        log.info(f"Processing batch of {len(txt_files)} files")
    
    # Process each file
    for txt_path in tqdm(txt_files, desc="Processing files"):
        # Log the current file being processed
        log.info(f"Processing file: {txt_path.name}")
        
        file_result = {
            "file": txt_path.name,
            "success": False,
            "error": None
        }
        
        try:
            # Read text file
            with open(txt_path, "r", encoding="utf-8") as f:
                text = f.read()
            
            # Extract first 3000 words
            truncated_text = extract_first_n_words(text)
            
            # Generate metadata - log that we're calling the API
            log.info(f"Calling OpenAI API for: {txt_path.name}")
            metadata = generate_metadata(client, truncated_text, model)
            
            # Get corresponding JSON path
            json_path = get_corresponding_json_path(txt_path)
            
            # Update JSON file
            log.info(f"Updating JSON file for: {txt_path.name}")
            if update_json_file(json_path, metadata):
                file_result["success"] = True
                processed_files.add(txt_path.name)
                results["processed"] += 1
                log.info(f"Successfully processed: {txt_path.name}")
            else:
                file_result["error"] = "Failed to update JSON file"
                results["errors"] += 1
                log.error(f"Failed to update JSON for: {txt_path.name}")
                
        except Exception as e:
            file_result["error"] = str(e)
            results["errors"] += 1
            log.error(f"Error processing {txt_path.name}: {e}")
            
        results["file_results"].append(file_result)
    
    # Save processed files
    save_processed_files(processed_files)
    
    return results

def main() -> None:
    """Main entry point."""
    parser = argparse.ArgumentParser(description="Process research files and extract metadata using OpenAI API")
    parser.add_argument("--batch-size", type=int, default=0, help="Number of files to process (0 = all)")
    parser.add_argument("--force", action="store_true", help="Process all files even if previously processed")
    parser.add_argument("--model", type=str, default=DEFAULT_MODEL, help="OpenAI model to use")
    args = parser.parse_args()
    
    # Check for OpenAI API key
    if not OPENAI_API_KEY:
        log.error("OPENAI_API_KEY not set. Please set it in your environment variables.")
        sys.exit(1)
    
    # Initialize OpenAI client
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    # Create directories if they don't exist
    TXT_DIR.mkdir(parents=True, exist_ok=True)
    JSON_DIR.mkdir(parents=True, exist_ok=True)
    
    # Process files
    log.info(f"Starting metadata extraction with model: {args.model}")
    results = process_files(client, args.batch_size, args.force, args.model)
    
    # Output results
    log.info(f"Processing complete: {results['processed']} processed, {results['skipped']} skipped, {results['errors']} errors")
    
    # Save report
    ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    report_path = ROOT_DIR / f"metadata_extraction_report_{ts}.json"
    with open(report_path, "w") as f:
        json.dump(results, f, indent=2)
    log.info(f"Report saved to {report_path}")

if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: scripts/templates/index.html
################################################################################

<!-- 
File: scripts/templates/index.html
 -->

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Misophonia Research RAG Interface</title>
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
  <style>
    body{padding:20px;background:#f8f9fa}
    .container{max-width:1000px;margin:0 auto}
    .header{text-align:center;margin-bottom:30px}
    .search-box{margin-bottom:20px}
    .results-container{margin-top:20px}
    .result-card{margin-bottom:15px;border-radius:8px;box-shadow:0 2px 5px rgba(0,0,0,.1)}
    .result-card .card-header{font-weight:bold;display:flex;justify-content:space-between}
    .loading{text-align:center;padding:20px;display:none}
    .response-container{margin-top:30px;padding:20px;background:#fff;border-radius:8px;box-shadow:0 2px 5px rgba(0,0,0,.1)}
    pre { background:#f8f9fa;border:0; }
  </style>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>Misophonia Research RAG Interface</h1>
      <p class="text-muted">Search across research documents with semantic retrieval</p>
    </div>

    <div class="search-box">
      <div class="input-group mb-3">
        <input type="text" id="search-input" class="form-control form-control-lg"
               placeholder="Ask a question about misophonia‚Ä¶" aria-label="Search query">
        <button class="btn btn-primary" type="button" id="search-button">Search</button>
      </div>
      <div class="form-text">Try questions about treatments, neurological basis, symptoms, or coping strategies</div>
    </div>

    <div class="loading" id="loading">
      <div class="spinner-border text-primary" role="status">
        <span class="visually-hidden">Loading‚Ä¶</span>
      </div>
      <p>Searching research documents and generating response‚Ä¶</p>
    </div>

    <div id="response-area" style="display:none">
      <div class="response-container">
        <h3>Research‚ÄëBased Answer</h3>
        <div id="response-content"></div>
      </div>

      <div class="results-container">
        <h3>Source Documents</h3>
        <div id="results-list"></div>
      </div>
    </div>
  </div>

<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
<script>
document.addEventListener("DOMContentLoaded",()=>{
  const $q      = document.getElementById("search-input");
  const $btn    = document.getElementById("search-button");
  const $load   = document.getElementById("loading");
  const $resp   = document.getElementById("response-area");
  const $respCt = document.getElementById("response-content");
  const $list   = document.getElementById("results-list");

  async function doSearch(){
    const query=$q.value.trim();
    if(!query) return;

    $load.style.display="block";
    $resp.style.display="none";

    try{
      const res=await fetch("/search",{
        method:"POST",
        headers:{"Content-Type":"application/json"},
        body:JSON.stringify({query,limit:5})
      });
      if(!res.ok){
        let msg=`Server error (${res.status})`;
        try{msg=(await res.json()).error||msg;}catch{}
        throw new Error(msg);
      }
      const data=await res.json();

      $respCt.innerHTML=`<p>${String(data.response||"").replace(/\n/g,"<br>")}</p>`;
      $list.innerHTML="";

      (data.results||[]).forEach((r,i)=>{
        const m = r.metadata;
        const card=document.createElement("div");
        card.className="card result-card";
        card.innerHTML=`
          <div class="card-header bg-light d-flex justify-content-between align-items-center"
               data-bs-toggle="collapse" data-bs-target="#chunk-${i}">
            <span>${r.source}</span>
            <span class="badge bg-primary opacity-75">${r.match_type}</span>
          </div>
          <div id="chunk-${i}" class="collapse show">
            <div class="card-body">
              <div class="d-flex justify-content-between align-items-baseline">
                <strong>
                  ${m.journal} (${m.year})
                </strong>
                ${m.doi ? `<a href="https://doi.org/${m.doi}" target="_blank" class="badge bg-primary">DOI</a>` : ''}
              </div>

              <h4 class="card-title">${m.title}</h4>
              <small class="text-muted">${m.section}</small>

              <p class="mb-0">
                <em>${m.authors}</em><br>
                ${m.volume ? `${m.journal} ${m.volume}${m.issue ? \`(\${m.issue})\` : ''}:` : ''}
                ${m.page_range}
              </p>

              <div class="mb-2 small text-muted">Similarity: ${(r.similarity??0).toFixed(4)}</div>
              <pre class="small text-muted mb-0" style="white-space:pre-wrap;">${r.chunk||""}</pre>
            </div>
          </div>`;
        $list.appendChild(card);
      });

    }catch(err){
      $respCt.innerHTML=`<div class="alert alert-warning">${err.message}</div>`;
      $list.innerHTML="";
    }finally{
      $load.style.display="none";
      $resp.style.display="block";
    }
  }

  $btn.addEventListener("click",doSearch);
  $q.addEventListener("keypress",e=>{if(e.key==="Enter")doSearch();});
});
</script>
</body>
</html>


================================================================================


################################################################################
# File: server/index.js
################################################################################

import express from 'express';
import cors from 'cors';
import dotenv from 'dotenv';
import { OpenAI } from 'openai';
import fetch from 'node-fetch';
import httpProxy from 'http-proxy';

dotenv.config();

// Set AI provider here - "openai" or "gemini"
const AI_PROVIDER = process.env.AI_PROVIDER || "openai";

const app = express();

//app.use(cors());  --> This is the default behavior
app.use(cors({
  origin: [
    '[https://misophonia-guide.netlify.app](https://misophonia-guide.netlify.app)',
    'http://localhost:5173'
  ]
}));
app.use(express.json());

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// Research Assistant endpoint (handles both Gemini and OpenAI)
app.post('/api/gemini', async (req, res) => {
  try {
    const { messages, topic } = req.body;
    
    if (!messages || !Array.isArray(messages)) {
      return res.status(400).json({ error: 'Messages array required.' });
    }
    
    // Using OpenAI
    if (AI_PROVIDER === "openai") {
      // Format messages for OpenAI
      const systemPrompt = topic 
        ? `You are a knowledgeable research assistant focusing on the topic: ${topic}. Provide information about misophonia research.`
        : "You are a knowledgeable research assistant on misophonia research.";
      
      const openaiMessages = [
        { role: 'system', content: systemPrompt },
        ...messages
      ];
      
      const completion = await openai.chat.completions.create({
        model: 'gpt-4o',
        messages: openaiMessages,
        max_tokens: 1024,
        temperature: 0.7
      });
      
      return res.json({
        reply: completion.choices[0]?.message?.content || '',
        structured: null,
        provider: 'openai'
      });
    }
    
    // Using Gemini
    else {
      const apiKey = process.env.GEMINI_API_KEY;
      if (!apiKey) {
        return res.status(500).json({ error: 'GEMINI_API_KEY not set in server/.env.' });
      }
      
      // Compose prompt for Gemini
      let userPrompt = '';
      if (topic && typeof topic === 'string') {
        userPrompt += `Topic: ${topic}\n`;
      }
      userPrompt += messages.map(m => `${m.role === 'user' ? 'User' : 'Assistant'}: ${m.content}`).join('\n');
      
      // Gemini API call
      const geminiRes = await fetch('https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?key=' + apiKey, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          contents: [{ role: 'user', parts: [{ text: userPrompt }] }],
          generationConfig: {
            temperature: 0.7,
            maxOutputTokens: 1024,
          },
          tools: [
            { "function_declarations": [
              { "name": "structured_output", "description": "Return information in a structured JSON format for chat display, with sections, bullet points, and highlights." }
            ]}
          ]
        })
      });
      
      if (!geminiRes.ok) {
        const err = await geminiRes.text();
        return res.status(500).json({ error: 'Error from Gemini API', details: err });
      }
      
      const geminiData = await geminiRes.json();
      // Parse structured output if present
      let structured = null;
      if (geminiData.candidates && geminiData.candidates[0]?.content?.parts) {
        const part = geminiData.candidates[0].content.parts[0];
        if (part.functionCall && part.functionCall.name === 'structured_output') {
          try {
            structured = JSON.parse(part.functionCall.args.json || '{}');
          } catch {
            structured = part.functionCall.args;
          }
        }
      }
      
      return res.json({
        reply: geminiData.candidates?.[0]?.content?.parts?.[0]?.text || '',
        structured,
        provider: 'gemini'
      });
    }
  } catch (error) {
    console.error(error);
    res.status(500).json({ error: `Error from ${AI_PROVIDER === "openai" ? "OpenAI" : "Gemini"} API.` });
  }
});

// Keep existing OpenAI chat endpoint
app.post('/api/chat', async (req, res) => {
  try {
    const { messages } = req.body;
    if (!messages || !Array.isArray(messages)) {
      return res.status(400).json({ error: 'Messages array required.' });
    }
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages,
      max_tokens: 512,
      temperature: 0.7
    });
    const reply = completion.choices[0]?.message?.content || '';
    res.json({ reply });
  } catch (error) {
    console.error(error);
    res.status(500).json({ error: 'Error from OpenAI API.' });
  }
});

// ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ RAG proxy ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
const proxy = httpProxy.createProxyServer({ changeOrigin: true });

app.post('/api/rag', (req, res) => {
  proxy.web(req, res, { target: 'http://localhost:8080/search' }, err => {
    console.error(err);
    res.status(502).json({ error: 'RAG service unreachable' });
  });
});

const PORT = process.env.PORT || 3001;
app.listen(PORT, () => {
  console.log(`Server running with ${AI_PROVIDER.toUpperCase()} on port ${PORT}`);
});


================================================================================


################################################################################
# File: scripts/ingest.js
################################################################################

import fs from 'fs';
import path from 'path';
import pdfjsLib from 'pdfjs-dist/legacy/build/pdf.js';
const { getDocument } = pdfjsLib;
import { createClient } from '@supabase/supabase-js';
import dotenv from 'dotenv';

// Load environment variables (make sure FIREBASE_* vars are set in server/.env)
dotenv.config({ path: path.resolve(process.cwd(), 'server/.env') });

// Initialize Supabase
const supabase = createClient(
  process.env.SUPABASE_URL,
  process.env.SUPABASE_SERVICE_ROLE_KEY
);

// Function to chunk text
function chunkText(text, size = 1000, overlap = 200) {
  const chunks = [];
  for (let start = 0; start < text.length; start += size - overlap) {
    const chunk = text.slice(start, start + size);
    chunks.push(chunk.trim());
  }
  return chunks;
}

async function processPdf(filePath) {
  const buffer = fs.readFileSync(filePath);
  const loadingTask = getDocument({ data: buffer });
  const pdfDoc = await loadingTask.promise;
  const numPages = pdfDoc.numPages;
  let text = '';
  for (let i = 1; i <= numPages; i++) {
    const page = await pdfDoc.getPage(i);
    const content = await page.getTextContent();
    const strings = content.items.map(item => item.str);
    text += strings.join(' ') + '\n';
  }
  const chunks = chunkText(text);
  const basename = path.basename(filePath, '.pdf');
  
  // Prepare chunks for insertion
  const chunksToInsert = chunks.map((chunk, idx) => ({
    file: basename,
    chunk_index: idx,
    text: chunk,
    created_at: new Date()
  }));
  
  // Insert in batches of 500
  const BATCH_SIZE = 500;
  for (let i = 0; i < chunksToInsert.length; i += BATCH_SIZE) {
    const batch = chunksToInsert.slice(i, i + BATCH_SIZE);
    const { error } = await supabase
      .from('research_chunks')
      .insert(batch);
    
    if (error) {
      console.error('Error inserting batch:', error);
      throw error;
    }
  }
  
  console.log(`Indexed ${chunks.length} chunks for ${basename}`);
}

async function main() {
  const dir = path.resolve(process.cwd(), 'documents/research/Global');
  const files = fs.readdirSync(dir).filter(f => f.toLowerCase().endsWith('.pdf'));
  for (const file of files) {
    console.log(`Processing ${file}...`);
    await processPdf(path.join(dir, file));
  }
  console.log('Ingestion complete.');
}

main().catch(err => {
  console.error(err);
  process.exit(1);
});


================================================================================


################################################################################
# File: js-requirements.md
################################################################################

################################################################################
################################################################################
# JavaScript Dependencies for Misophonia Research RAG System

## Core Dependencies

```json
{
  "dependencies": {
    "dotenv": "^16.0.3",
    "fast-glob": "^3.3.3",
    "firebase": "^10.0.0",
    "firebase-admin": "^11.8.0",
    "mammoth": "^1.9.0",
    "node-fetch": "^3.3.1",
    "openai": "^4.9.0",
    "pdfjs-dist": "^3.7.107",
    "express": "^4.18.2",
    "cors": "^2.8.5",
    "axios": "^1.4.0",
    "body-parser": "^1.20.2"
  }
}
```

## Development Dependencies

```json
{
  "devDependencies": {
    "@eslint/js": "^9.22.0",
    "@types/react": "^19.0.10",
    "@types/react-dom": "^19.0.4",
    "@vitejs/plugin-react": "^4.3.4",
    "eslint": "^9.22.0",
    "eslint-plugin-react-hooks": "^5.2.0",
    "eslint-plugin-react-refresh": "^0.4.19",
    "globals": "^16.0.0",
    "netlify-cli": "^13.2.2",
    "vite": "^6.3.1",
    "vite-plugin-pwa": "^1.0.0"
  }
}
```

## Firebase-specific Dependencies

```json
{
  "dependencies": {
    "firebase/app": "included in firebase package",
    "firebase/firestore": "included in firebase package",
    "firebase/functions": "included in firebase package",
    "firebase-admin/app": "included in firebase-admin package",
    "firebase-admin/firestore": "included in firebase-admin package",
    "firebase-functions": "^4.3.0"
  }
}
```

## Installation Instructions

1. These dependencies are already defined in your project's `package.json` file.
2. To install all dependencies, run:
   ```
   npm install
   ```
3. For Firebase Cloud Functions, navigate to the functions directory and run:
   ```
   cd functions
   npm install
   ```

## Notes

- The Firebase client and admin SDKs are separate packages with different use cases:
  - `firebase` is for client-side applications
  - `firebase-admin` is for server-side applications and has elevated privileges

- For the RAG system, the key dependencies are:
  - `openai`: For generating embeddings and AI responses
  - `firebase-admin`: For accessing Firestore vector database
  - `pdfjs-dist`: For processing PDF documents
  - `express`: For the web server interface


================================================================================


################################################################################
# File: vite.config.js
################################################################################

import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'
import { VitePWA } from 'vite-plugin-pwa'

// https://vite.dev/config/
export default defineConfig({
  plugins: [
    react(),
    VitePWA({
      registerType: 'autoUpdate',
      workbox: {
        runtimeCaching: [
          {
            urlPattern: /^\/api\/(chat|gemini|rag).*$/,
            handler: 'NetworkOnly',
          },
        ],
      },
      manifest: {
        name: 'Misophonia Companion',
        short_name: 'Companion',
        description: 'A therapeutic and research PWA for misophonia.',
        start_url: '.',
        display: 'standalone',
        background_color: '#f8f6ff',
        theme_color: '#b2d8d8',
        icons: [
          {
            src: 'icon-192.png',
            sizes: '192x192',
            type: 'image/png'
          },
          {
            src: 'icon-512.png',
            sizes: '512x512',
            type: 'image/png'
          }
        ]
      }
    })
  ],
  server: {
    proxy: {
      '/api': {
        target: 'http://localhost:3001',
        changeOrigin: true,
      }
    }
  }
})


================================================================================


################################################################################
# File: netlify.toml
################################################################################

# Netlify configuration for Misophonia Companion
[build]
  command = "npm run build"
  publish = "dist"

[dev]
  command = "npm run dev"
  targetPort = 5173

[functions]
  directory = "netlify/functions"
  external_node_modules = ["openai", "@supabase/supabase-js"]

# Redirect all requests to index.html for SPA routing
[[redirects]]
  from = "/api/*"
  to = "/.netlify/functions/:splat"
  status = 200

[[redirects]]
  from = "/api/gemini"
  to = "/.netlify/functions/research"
  status = 200

[[redirects]]
  from = "/api/rag"
  to = "/.netlify/functions/rag"
  status = 200

[[redirects]]
  from = "/*"
  to = "/index.html"
  status = 200

[functions."rag"]   # file rag.js
  node_bundler = "esbuild"
  deno_import_map = "netlify/import_map.json" # only if you jump to Deno/Edge


================================================================================


################################################################################
# File: index.html
################################################################################

<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Vite + React</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>


================================================================================


################################################################################
# File: netlify/functions/_utils.js
################################################################################

// Shared constants for Netlify functions

// Which LLM powers the Research Assistant ("openai" | "gemini") ‚Äì defaults to "openai"
export const AI_PROVIDER = process.env.AI_PROVIDER || 'openai';

// Location of the running RAG service (rag_web_app_v8.py)
export const RAG_HOST = process.env.RAG_HOST || 'http://localhost:8080';


================================================================================


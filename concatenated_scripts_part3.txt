# Concatenated Project Code - Part 3 of 3
# Generated: 2025-05-22 18:55:46
# Root Directory: /Users/gianmariatroiani/Documents/misophonia-companion-v3
================================================================================

################################################################################
# File: scripts/rag_local_web_app.py
################################################################################

# File: scripts/rag_local_web_app.py

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Mini Flask app that answers misophonia questions with Retrieval‑Augmented
Generation (gpt-4.1-mini-2025-04-14 + Supabase pgvector).

### Patch 2  (2025‑05‑06)
• **Embeddings** now created with **text‑embedding‑ada‑002** (1536‑D).  
• Similarity is re‑computed client‑side with a **plain cosine function** so the
  ranking no longer depends on pgvector's built‑in distance or any RPC
  threshold quirks.

The rest of the grounded‑answer logic (added in Patch 1) is unchanged.
"""
from __future__ import annotations

import logging
import math
import os
import re
from pathlib import Path        # (unused but left in to mirror original)
from typing import Dict, List
import json

from dotenv import load_dotenv
from flask import Flask, jsonify, request, make_response
from openai import OpenAI
from supabase import create_client
from flask_compress import Compress
from flask_cors import CORS

# ────────────────────────────── configuration ───────────────────────────── #

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUPABASE_URL   = os.getenv("SUPABASE_URL")
SUPABASE_KEY   = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
PORT           = int(os.getenv("PORT", 8080))

if not (OPENAI_API_KEY and SUPABASE_URL and SUPABASE_KEY):
    raise SystemExit(
        "❌  Required env vars: OPENAI_API_KEY, SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY"
    )

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s",
)
log = logging.getLogger("rag_app")

sb = create_client(SUPABASE_URL, SUPABASE_KEY)
oa = OpenAI(api_key=OPENAI_API_KEY)

app = Flask(__name__)
CORS(app)
app.config['COMPRESS_ALGORITHM'] = 'gzip'
Compress(app)

# ────────────────────────────── helper functions ────────────────────────── #


def embed(text: str) -> List[float]:
    """
    Return OpenAI embedding vector for *text* using text‑embedding‑ada‑002.

    ada‑002 has 1536 dimensions and is inexpensive yet solid for similarity.
    """
    resp = oa.embeddings.create(
        model="text-embedding-ada-002",
        input=text[:8192],  # safety slice
    )
    return resp.data[0].embedding


def cosine_similarity(a: List[float], b: List[float]) -> float:
    """Plain cosine similarity between two equal‑length vectors."""
    dot = sum(x * y for x, y in zip(a, b))
    na = math.sqrt(sum(x * x for x in a))
    nb = math.sqrt(sum(y * y for y in b))
    return dot / (na * nb + 1e-9)


# Add in-memory embedding cache
_qcache = {}
def embed_cached(text):
    if text in _qcache: return _qcache[text]
    vec = embed(text)
    _qcache[text] = vec
    return vec


# Add regex patterns for bibliography detection
_DOI_RE   = re.compile(r'\b10\.\d{4,9}/[-._;()/:A-Z0-9]+\b', re.I)
_YEAR_RE  = re.compile(r'\b(19|20)\d{2}\b')

def looks_like_refs(text: str) -> bool:
    """
    Return True if this chunk is likely just a bibliography list:
      • more than 12 DOIs, or
      • more than 15 year mentions.
    """
    doi_count  = len(_DOI_RE.findall(text))
    year_count = len(_YEAR_RE.findall(text))
    return doi_count > 12 or year_count > 15


def semantic_search(
    query: str,
    *,
    limit: int = 8,
    threshold: float = 0.0,
) -> List[Dict]:
    """
    Retrieve candidate chunks via the pgvector RPC, then re-rank with an
    **explicit cosine similarity** so the final score is always in
    **[-100 … +100] percent**.

    Why the extra work?
    -------------------
    •  The SQL function returns a raw inner-product that can be > 1.  
       (embeddings are *not* unit-length.)  
    •  By pulling the real 1 536-D vectors and re-computing a cosine we get a
       true, bounded similarity that front-end code can safely show.

    The -100 … +100 range is produced by:  
        pct = clamp(cosine × 100, -100, 100)
    """
    # 1. Embed the query once and keep it cached
    q_vec = embed_cached(query)

    # 2. Fast ANN search in Postgres (over-fetch 4× so we can re-rank)
    rows = (
        sb.rpc(
            "match_research_chunks",
            {
                "query_embedding": q_vec,
                "match_threshold": threshold,
                "match_count": limit * 4,
            },
        )
        .execute()
        .data
    ) or []

    # 3. Filter out bibliography-only chunks
    rows = [r for r in rows if not looks_like_refs(r["text"])]

    if not rows:
        return []

    # 4. Fetch document metadata (title, authors …) in one round-trip
    doc_ids = {r["document_id"] for r in rows}
    meta = {
        d["id"]: d
        for d in (
            sb.table("research_documents")
              .select("id,title,authors,year,journal,doi,abstract,keywords,research_topics,source_pdf")
              .in_("id", list(doc_ids))
              .execute()
              .data
            or []
        )
    }

    # 5. Pull embeddings once and compute **plain cosine** (no scaling)
    chunk_ids = [r["id"] for r in rows]

    emb_rows = (
        sb.table("research_chunks")
          .select("id, embedding")
          .in_("id", chunk_ids)
          .execute()
          .data
    ) or []

    emb_map: Dict[str, List[float]] = {}
    for e in emb_rows:
        raw = e["embedding"]
        if isinstance(raw, list):                    # list[Decimal]
            emb_map[e["id"]] = [float(x) for x in raw]
        elif isinstance(raw, str) and raw.startswith('['):   # TEXT  "[…]"
            emb_map[e["id"]] = [float(x) for x in raw.strip('[]').split(',')]

    for r in rows:
        vec = emb_map.get(r["id"])
        if vec:                                     # we now have the real vector
            cos = cosine_similarity(q_vec, vec)
            r["similarity"] = round(cos * 100, 1)   # –100…+100 % (or 0…100 %)
        else:                                       # fallback if something failed
            dist = float(r.get("similarity", 1.0))  # 0…2 cosine-distance
            r["similarity"] = round((1.0 - dist) * 100, 1)

        r["doc"] = meta.get(r["document_id"], {})

    # 6. Keep the top *limit* rows after proper re-ranking
    ranked = sorted(rows, key=lambda x: x["similarity"], reverse=True)[:limit]
    return ranked


# ──────────────────────── NEW RAG‑PROMPT HELPERS ───────────────────────── #

MAX_PROMPT_CHARS: int = 24_000  # ~6 k tokens @ 4 chars/token heuristic


def trim_chunks(chunks: List[Dict]) -> List[Dict]:
    """
    Fail‑safe guard: ensure concatenated chunk texts remain under the
    MAX_PROMPT_CHARS budget.  Keeps highest‑similarity chunks first.
    """
    sorted_chunks = sorted(chunks, key=lambda c: c.get("similarity", 0), reverse=True)
    output: List[Dict] = []
    total_chars = 0
    for c in sorted_chunks:
        chunk_len = len(c["text"])
        if total_chars + chunk_len > MAX_PROMPT_CHARS:
            break
        output.append(c)
        total_chars += chunk_len
    return output


def build_prompt(question: str, chunks: List[Dict]) -> str:
    """
    Build a structured prompt that asks GPT to:
      • answer in Markdown with short intro + numbered list of key points
      • cite inline like [1], [2] …
      • finish with a Bibliography that includes the *paper title*
    """
    snippet_lines, biblio_lines = [], []
    for i, c in enumerate(chunks, 1):
        snippet_lines.append(
            f"[{i}] \"{c['text'].strip()}\" "
            f"(pp. {c['page_start']}-{c['page_end']})"
        )

        d = c["doc"]
        title   = d.get("title", "Untitled")
        authors = ", ".join(d.get("authors") or ["Unknown"])
        journal = d.get("journal", "Unknown journal")
        year    = d.get("year", "n.d.")
        pages   = f"pp. {c['page_start']}-{c['page_end']}"
        doi_raw = d.get("doi")
        doi_md  = f"[doi:{doi_raw}](https://doi.org/{doi_raw})" if doi_raw else ""

        # Title now comes first ↓↓↓
        biblio_lines.append(
            f"[{i}] *{title}* · {authors} · {journal} ({year}) · {pages} {doi_md}"
        )

    prompt_parts = [
        "You are Misophonia Companion, a highly knowledgeable and empathetic AI assistant built to support clinicians, researchers, and individuals managing misophonia.",
        "You draw on evidence from peer-reviewed literature, clinical guidelines, and behavioral science.",
        "Your responses are clear, thoughtful, and grounded in the provided context.",
        "====",
        "QUESTION:",
        question,
        "====",
        "CONTEXT:",
        *snippet_lines,
        "====",
        "INSTRUCTIONS:",
        "• Write your answer in **Markdown**.",
        "• Begin with a concise summary (2–3 sentences).",
        "• Then elaborate on key points using well-structured paragraphs.",
        "• Provide relevant insights or suggestions (e.g., clinical, behavioral, emotional, or research-related).",
        "• If helpful, use lists, subheadings, or analogies to enhance understanding.",
        "• Use a professional and empathetic tone.",
        "• Cite sources inline like [1], [2] etc.",
        "• After the answer, include a 'BIBLIOGRAPHY:' section that lists each source exactly as provided below.",
        "• If none of the context answers the question, reply: \"I'm sorry, I don't have sufficient information to answer that.\"",
        "====",
        "BEGIN OUTPUT",
        "ANSWER:",
        "",  # where the model writes the main response
        "BIBLIOGRAPHY:",
        *biblio_lines,
    ]


    return '\n'.join(prompt_parts)


def extract_citations(answer: str) -> List[str]:
    """
    Parse numeric citations (e.g., "[1]", "[2]") from the answer text.
    Returns unique citation numbers in ascending order.
    """
    citations = re.findall(r"\[(\d+)\]", answer)
    return sorted(set(citations), key=int)


# ──────────────────────────────── routes ────────────────────────────────── #

@app.post("/search")
def search():
    payload = request.get_json(force=True, silent=True) or {}
    question = (payload.get("query") or "").strip()
    if not question:
        return jsonify({"error": "Missing 'query'"}), 400

    try:
        # Retrieve semantic matches (client‑side cosine re‑ranked)
        raw_matches = semantic_search(question, limit=int(payload.get("limit", 8)))

        if not raw_matches:
            return jsonify(
                {
                    "answer": "I'm sorry, I don't have sufficient information to answer that.",
                    "citations": [],
                    "results": [],
                }
            )

        # ──────────────────── TRIM CHUNKS TO BUDGET ──────────────────── #
        chunks = trim_chunks(raw_matches)

        # ──────────────────── BUILD PROMPT & CALL LLM ─────────────────── #
        prompt = build_prompt(question, chunks)

        completion = oa.chat.completions.create(
            model="gpt-4.1-mini-2025-04-14",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
        )
        answer_text: str = completion.choices[0].message.content.strip()

        # ──────────────────── EXTRACT CITATIONS ──────────────────────── #
        citations = extract_citations(answer_text)

        # Remove embedding vectors before sending back to the browser
        for m in raw_matches:
            m.pop("embedding", None)

        # ──────────────────── RETURN JSON ─────────────────────────────── #
        response = jsonify(
            {
                "answer": answer_text,
                "citations": citations,
                "results": raw_matches,
            }
        )
        response.headers['Connection'] = 'keep-alive'
        return response
    except Exception as exc:  # noqa: BLE001
        log.exception("search failed")
        return jsonify({"error": str(exc)}), 500


@app.get("/stats")
def stats():
    """Tiny ops endpoint—count total chunks."""
    resp = sb.table("research_chunks").select("id", count="exact").execute()
    return jsonify({"total_chunks": resp.count})


# ──────────────────────────────── main ─────────────────────────────────── #

if __name__ == "__main__":
    log.info("Starting Flask on 0.0.0.0:%s …", PORT)
    app.run(host="0.0.0.0", port=PORT, debug=True)


================================================================================


################################################################################
# File: netlify/functions/rag.js
################################################################################

// -----------------------------------------------------------------------------
//  Netlify Function  ·  Retrieval-Augmented Generation for Misophonia Companion
// -----------------------------------------------------------------------------
//  •  Embeds the query with OpenAI ada-002 (1536-D)
//  •  Vector search in Supabase via pgvector RPC
//  •  Client-side cosine re-rank (-100…+100  %)
//  •  Bibliography-only chunks are discarded
//  •  Prompt budget guard (≤ 24 000 chars)
//  •  Llama-3.3-70B via Groq generates Markdown answer + Bibliography section
//  •  GET  /stats   → return total number of indexed chunks
//  •  POST /search  → return { answer, citations, results }
// -----------------------------------------------------------------------------
//  Environment variables required at *function* runtime:
//
//      SUPABASE_URL                 (e.g. https://xyz.supabase.co)
//      SUPABASE_SERVICE_ROLE_KEY    (or anon key if RLS permits the RPC)
//      OPENAI_API_KEY               (to embed + generate)
//      GROQ_API_KEY                 (to generate responses)
// -----------------------------------------------------------------------------
//  This file is a line-for-line port of rag_web_app_v9.py, rewritten in Node so
//  it can run natively as a Netlify Function (no external Flask server needed).
// -----------------------------------------------------------------------------
import 'dotenv/config';
import { createClient } from '@supabase/supabase-js';
import OpenAI from 'openai';
import { Groq } from 'groq-sdk';

// ──────────────────────────── configuration ─────────────────────────────── //
const {
  SUPABASE_URL,
  SUPABASE_SERVICE_ROLE_KEY,
  OPENAI_API_KEY,
  GROQ_API_KEY,
} = process.env;

if (!SUPABASE_URL || !SUPABASE_SERVICE_ROLE_KEY || !OPENAI_API_KEY || !GROQ_API_KEY) {
  throw new Error(
    '❌  Missing env vars: SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY, OPENAI_API_KEY, GROQ_API_KEY'
  );
}

const sb     = createClient(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY);
const openai = new OpenAI({ apiKey: OPENAI_API_KEY });
const groq   = new Groq({ apiKey: GROQ_API_KEY });

// ────────────────────────── embedding & similarity ───────────────────────── //
// In-memory LRU-ish cache (Map) so repeated identical queries in the same
// container instance don't double-embed and waste tokens.
const embCache = new Map();

/** Return ada-002 embedding for *text* (1536-D float array) */
async function embed(text) {
  if (embCache.has(text)) return embCache.get(text);

  const { data } = await openai.embeddings.create({
    model: 'text-embedding-ada-002',
    input: text.slice(0, 8192), // hard limit: ada-002 max input tokens
  });

  const vec = data[0].embedding;
  embCache.set(text, vec);
  return vec;
}

/** Plain cosine similarity (no scaling) */
const cosine = (a, b) => {
  let dot = 0,
    na = 0,
    nb = 0;
  for (let i = 0; i < a.length; i++) {
    dot += a[i] * b[i];
    na  += a[i] * a[i];
    nb  += b[i] * b[i];
  }
  return dot / (Math.sqrt(na) * Math.sqrt(nb) + 1e-9); // avoid /0
};

// ─────────────────────── bibliography chunk detector ─────────────────────── //
const DOI_RE  = /\b10\.\d{4,9}\/[-._;()/:A-Z0-9]+\b/gi;
const YEAR_RE = /\b(19|20)\d{2}\b/g;

/** True if chunk looks like a pure references list (skip). */
const looksLikeRefs = (txt) =>
  (txt.match(DOI_RE)?.length || 0) > 12 ||
  (txt.match(YEAR_RE)?.length || 0) > 15;

// ────────────────────────────── vector search ────────────────────────────── //
/**
 * 1. Embed query
 * 2. pgvector RPC (over-fetch 4× so we can re-rank)
 * 3. Drop bibliography noise
 * 4. Pull true embeddings + doc metadata
 * 5. Client-side cosine re-rank (-100…+100 %)
 * 6. Return top *limit* rows
 */
async function semanticSearch(query, limit = 8, threshold = 0) {
  const qVec = await embed(query);

  const { data: rows = [] } = await sb.rpc('match_research_chunks', {
    query_embedding: qVec,
    match_threshold: threshold,  // 0 → ANN returns highest-IP matches
    match_count: limit * 4,      // over-fetch (will re-rank)
  });

  const filtered = rows.filter(r => !looksLikeRefs(r.text));
  if (!filtered.length) return [];

  // Bulk-fetch embeddings + document rows in 2 round-trips
  const chunkIds = filtered.map(r => r.id);
  const docIds   = [...new Set(filtered.map(r => r.document_id))];

  const [
    { data: embRows = [] },
    { data: docs    = [] },
  ] = await Promise.all([
    sb.from('research_chunks')
      .select('id,embedding')
      .in('id', chunkIds)
      .limit(chunkIds.length),
    sb.from('research_documents')
      .select(
        'id,title,authors,year,journal,doi,abstract,keywords,research_topics,source_pdf'
      )
      .in('id', docIds)
      .limit(docIds.length),
  ]);

  const embMap = new Map(
    embRows.map(e => [
      e.id,
      Array.isArray(e.embedding)
        ? e.embedding.map(Number)                       // numeric column
        : e.embedding.slice(1, -1).split(',').map(Number), // text column "[…]"
    ])
  );
  const docMap = new Map(docs.map(d => [d.id, d]));

  // Compute **true cosine** and attach metadata
  filtered.forEach(r => {
    const vec = embMap.get(r.id);
    r.similarity = vec ? Math.round(cosine(qVec, vec) * 1000) / 10 : 0; // –100…+100
    r.doc        = docMap.get(r.document_id) || {};
  });

  return filtered
    .sort((a, b) => b.similarity - a.similarity)
    .slice(0, limit);
}

// ────────────────────────── prompt-budget helper ─────────────────────────── //
const MAX_PROMPT_CHARS = 19_000; //24_000; // 6 k tokens ≈ GPT-4o safety window

/** Drop low-similarity chunks until sum(text.length) ≤ MAX_PROMPT_CHARS */
function trimChunks(chunks) {
  let acc = 0;
  const out = [];
  for (const c of [...chunks].sort((a, b) => b.similarity - a.similarity)) {
    const len = c.text.length;
    if (acc + len > MAX_PROMPT_CHARS) break;
    out.push(c);
    acc += len;
  }
  return out;
}

// ────────────────────────── GPT-4 prompt builder ─────────────────────────── //
function buildPrompt(question, chunks) {
  const snippetLines = [];
  const biblioLines  = [];

  chunks.forEach((c, idx) => {
    snippetLines.push(
      `[${idx + 1}] "${c.text.trim()}" (pp. ${c.page_start}-${c.page_end})`
    );

    const d       = c.doc;
    const title   = d.title || 'Untitled';
    const authors = Array.isArray(d.authors) ? d.authors.join(', ') : d.authors;
    const journal = d.journal || 'Unknown journal';
    const year    = d.year || 'n.d.';
    const pages   = `pp. ${c.page_start}-${c.page_end}`;
    const doi     = d.doi ? `[doi:${d.doi}](https://doi.org/${d.doi})` : '';

    biblioLines.push(
      `[${idx + 1}] *${title}* · ${authors || 'Unknown'} · ${journal} (${year}) · ${pages} ${doi}`
    );
  });

  const prompt_parts = [
    'You are Misophonia Companion, a highly knowledgeable and empathetic AI assistant built to support clinicians, researchers, and individuals managing misophonia.',
    'You draw on evidence from peer-reviewed literature, clinical guidelines, and behavioral science.',
    'Your responses are clear, thoughtful, and grounded in the provided context.',
    '====',
    'QUESTION:',
    question,
    '====',
    'CONTEXT:',
    ...snippetLines,
    '====',
    'INSTRUCTIONS:',
    '• Write your answer in **Markdown**.',
    '• Begin with a concise summary (2–3 sentences).',
    '• Then elaborate on key points using well-structured paragraphs.',
    '• Provide relevant insights or suggestions (e.g., clinical, behavioral, emotional, or research-related).',
    '• If helpful, use lists, subheadings, or analogies to enhance understanding.',
    '• Use a professional and empathetic tone.',
    '• Cite sources inline like [1], [2] etc.',
    "• After the answer, include a 'BIBLIOGRAPHY:' section that lists each source exactly as provided below.",
    "• If none of the context answers the question, reply: \"I'm sorry, I don't have sufficient information to answer that.\"",
    '====',
    'BEGIN OUTPUT',
    'ANSWER:',
    '',                        // blank line – GPT populates here
    'BIBLIOGRAPHY:',
    ...biblioLines,
  ];

  return prompt_parts.join('\n');
}

// ───────────────────── citation extractor (for UI) ──────────────────────── //
const extractCitations = txt =>
  [...new Set((txt.match(/\[(\d+)]/g) || []).map(m => m.slice(1, -1)))];

// ──────────────────────────── main Lambda handler ───────────────────────── //
export async function handler(event) {
  try {
    // ── tiny ops endpoint: /stats ─────────────────────────────────────── //
    if (event.httpMethod === 'GET' && event.path.endsWith('/stats')) {
      const { count } = await sb
        .from('research_chunks')
        .select('id', { count: 'exact' })
        .limit(1);

      return {
        statusCode: 200,
        headers: { 'content-type': 'application/json' },
        body: JSON.stringify({ total_chunks: count }),
      };
    }

    // ── POST /search  (main RAG flow) ─────────────────────────────────── //
    if (event.httpMethod !== 'POST') {
      return { statusCode: 405, body: 'Method Not Allowed' };
    }

    const body  = JSON.parse(event.body || '{}');
    const query = (body.query || '').trim();
    const limit = body.limit ? Number(body.limit) : 8;

    if (!query) {
      return { statusCode: 400, body: 'Missing "query"' };
    }

    /* 1. Semantic retrieval */
    const rawMatches = await semanticSearch(query, limit);
    if (!rawMatches.length) {
      return {
        statusCode: 200,
        headers: { 'content-type': 'application/json' },
        body: JSON.stringify({
          answer:
            "I'm sorry, I don't have sufficient information to answer that.",
          citations: [],
          results: [],
        }),
      };
    }

    /* 2. Prompt construction */
    const chunks = trimChunks(rawMatches);
    const prompt = buildPrompt(query, chunks);

    /* 3. LLM generation via Groq */
    const chat = await groq.chat.completions.create({
      model: "qwen-qwq-32b", //'llama-3.3-70b-versatile',
      messages: [
        { role: 'system',
          content: `You are a supportive misophonia companion.
RULES:
• Do not expose chain-of-thought or meta reasoning.
• Do not emit <think> tags.` },
        { role: 'user', content: prompt }
      ],
      temperature: 0,
      max_tokens: 2800, //4096, =2048/2=1024*3=
      stream: false
      // stream: true,
    });

    let answerText = chat.choices[0].message.content.trim();

    /* ─ Post-filters ─────────────────────────────────────── */
    // 1. Remove <think> … </think> blocks (multiline, greedy)
    answerText = answerText.replace(/<think>[\s\S]*?<\/think>/gi, '');
    // 2. Strip any pre-answer rambling up to the first Markdown heading / paragraph
    answerText = answerText.replace(/^[\s\S]*?\n\s*?(?=#+ |\S)/, '');

    const citations = extractCitations(answerText);

    /* 4. Remove heavy embedding vectors before returning to client */
    rawMatches.forEach(m => delete m.embedding);

    return {
      statusCode: 200,
      headers: { 'content-type': 'application/json' },
      body: JSON.stringify({ answer: answerText, citations, results: rawMatches }),
    };
  } catch (err) {
    // Any failure (OpenAI, Supabase, JSON parse) → 500
    console.error('RAG λ failed →', err);
    return {
      statusCode: 500,
      headers: { 'content-type': 'application/json' },
      body: JSON.stringify({ error: String(err) }),
    };
  }
}


================================================================================


################################################################################
# File: scripts/stages/embed_vectors.py
################################################################################

# File: scripts/stages/embed_vectors.py

#!/usr/bin/env python3
"""
Stage 7 — Fill NULL embeddings in research_chunks (token-safe batch generator)
"""
from __future__ import annotations
# ... full resilient script body copied verbatim ...
#  ───────────────────────── configuration ─────────────────────────
import argparse, json, logging, os, sys, time
from datetime import datetime
from typing import Any, Dict, List
from dotenv import load_dotenv
from openai import OpenAI
from supabase import create_client
from tqdm import tqdm
load_dotenv()
SUPABASE_URL  = os.getenv("SUPABASE_URL")
SUPABASE_KEY  = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
OPENAI_API_KEY= os.getenv("OPENAI_API_KEY")
EMBEDDING_MODEL="text-embedding-ada-002"
MODEL_TOKEN_LIMIT=8192
TOKEN_GUARD=200
MAX_TOTAL_TOKENS=MODEL_TOKEN_LIMIT-TOKEN_GUARD
DEFAULT_BATCH_ROWS=5000
DEFAULT_COMMIT_ROWS=100
MAX_RETRIES=5
RETRY_DELAY=2
openai_client=OpenAI(api_key=OPENAI_API_KEY)
logging.basicConfig(level=logging.INFO,
    format="%(asctime)s — %(levelname)s — %(message)s")
log=logging.getLogger(__name__)
# ---------------- Supabase helpers / fetch rows / token helpers -------------
def init_supabase():
    if not SUPABASE_URL or not SUPABASE_KEY:
        log.error("SUPABASE creds missing"); sys.exit(1)
    return create_client(SUPABASE_URL,SUPABASE_KEY)
def count_processed_chunks(sb)->int:
    res=sb.table("research_chunks").select("id",count="exact").not_.is_("embedding","null").execute()
    return res.count or 0
def fetch_unprocessed_chunks(sb,*,limit:int,offset:int=0)->List[Dict[str,Any]]:
    first,last=offset,offset+limit-1
    res=sb.table("research_chunks").select("id,text,token_start,token_end")\
        .eq("chunking_strategy","token_window").is_("embedding","null")\
        .range(first,last).execute()
    return res.data or []
def generate_embedding_batch(texts:List[str])->List[List[float]]:
    attempt=0
    while attempt<MAX_RETRIES:
        try:
            resp=openai_client.embeddings.create(model=EMBEDDING_MODEL,input=texts)
            return [d.embedding for d in resp.data]
        except Exception as exc:
            attempt+=1; log.warning("Embedding batch %s/%s failed: %s",attempt,MAX_RETRIES,exc)
            time.sleep(RETRY_DELAY)
    raise RuntimeError("OpenAI embedding batch failed after retries")
def chunk_tokens(row:Dict[str,Any])->int:
    try:
        t=int(row["token_end"])-int(row["token_start"])+1
        if 0<t<=16384: return t
    except: pass
    approx=int(len((row.get("text") or "").split())*0.75)+1
    return min(max(1,approx),MODEL_TOKEN_LIMIT)
def safe_slices(rows:List[Dict[str,Any]],max_rows:int)->List[List[Dict[str,Any]]]:
    slices,current,cur_tokens=[],[],0
    for r in rows:
        txt=(r.get("text") or "").replace("\x00","")
        if not txt.strip(): continue
        t=chunk_tokens(r)
        if t>MAX_TOTAL_TOKENS:
            log.warning("Chunk %s is too long – skipping.",r["id"]); continue
        need_new=len(current)>=max_rows or cur_tokens+t>MAX_TOTAL_TOKENS
        if need_new and current:
            slices.append(current); current=[]; cur_tokens=0
        current.append({"id":r["id"],"text":txt}); cur_tokens+=t
    if current: slices.append(current)
    return slices
def embed_slice(sb,slice_rows:List[Dict[str,Any]])->int:
    embeds=generate_embedding_batch([r["text"] for r in slice_rows])
    ok=0
    for row,emb in zip(slice_rows,embeds):
        attempt=0
        while attempt<MAX_RETRIES:
            res=sb.table("research_chunks").update({"embedding":emb}).eq("id",row["id"]).execute()
            if getattr(res,"error",None):
                attempt+=1; log.warning("Update %s failed (%s/%s): %s",row["id"],attempt,MAX_RETRIES,res.error)
                time.sleep(RETRY_DELAY)
            else: ok+=1; break
    return ok
# ------------------------------ main ----------------------------------------
def main()->None:
    # make the module-level constants writable in this function *before* we read them
    global EMBEDDING_MODEL, MODEL_TOKEN_LIMIT, MAX_TOTAL_TOKENS
    
    ap=argparse.ArgumentParser(description="Embed research_chunks where embedding IS NULL (token-safe)")
    ap.add_argument("--batch-size",type=int,default=DEFAULT_BATCH_ROWS)
    ap.add_argument("--commit",type=int,default=DEFAULT_COMMIT_ROWS)
    ap.add_argument("--skip",type=int,default=0)
    ap.add_argument("--model",default=EMBEDDING_MODEL)
    ap.add_argument("--model-limit",type=int,default=MODEL_TOKEN_LIMIT)
    args=ap.parse_args()
    EMBEDDING_MODEL=args.model; MODEL_TOKEN_LIMIT=args.model_limit
    MAX_TOTAL_TOKENS=MODEL_TOKEN_LIMIT-TOKEN_GUARD
    if not OPENAI_API_KEY: log.error("OPENAI_API_KEY missing"); sys.exit(1)
    sb=init_supabase()
    log.info("Rows already embedded: %s",count_processed_chunks(sb))
    loop,total=0,0
    while True:
        loop+=1
        rows=fetch_unprocessed_chunks(sb,limit=args.batch_size,offset=args.skip)
        if not rows: log.info("✨  Done — no more rows."); break
        log.info("Loop %s — fetched %s rows.",loop,len(rows))
        slices=safe_slices(rows,args.commit)
        log.info("Created %s token-safe OpenAI requests.",len(slices))
        for sl in tqdm(slices,desc=f"Embedding loop {loop}",unit="batch"):
            try: total+=embed_slice(sb,sl)
            except Exception as exc: log.error("Slice failed: %s",exc)
        log.info("Loop %s complete – total embedded so far: %s",loop,total)
    ts=datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    report={"timestamp":ts,"batch_size":args.batch_size,"commit":args.commit,
            "skip":args.skip,"total_embedded":total,
            "total_with_embeddings":count_processed_chunks(sb)}
    fname=f"supabase_embedding_report_{ts}.json"
    with open(fname,"w") as fp: json.dump(report,fp,indent=2)
    log.info("Report saved to %s",fname)
if __name__=="__main__": main()


================================================================================


################################################################################
# File: scripts/test_vector_search.py
################################################################################

#!/usr/bin/env python3
################################################################################
################################################################################
"""
Vector‑search smoke‑test (Supabase edition)
==========================================

• Firebase/Firestore has been removed — every data call now goes through
  Supabase's PostgREST API.

• The script calls a SQL helper function that must exist on your database:
    public.search_research_chunks(query_text TEXT,
                                  match_count INT DEFAULT 10,
                                  similarity_threshold REAL DEFAULT 0.6)
  which should:
    1. embed the incoming `query_text`
    2. invoke your `match_documents` similarity function
    3. return the top‑`match_count` rows as
       (id UUID, text TEXT, metadata JSONB, similarity REAL)

  See README / earlier instructions for a ready‑made implementation.

Environment variables required
------------------------------
SUPABASE_URL                 – e.g. https://xxxx.supabase.co
SUPABASE_SERVICE_ROLE_KEY    – or an anon key if RLS permits the RPC
OPENAI_API_KEY               – only if your SQL helper embeds via an HTTP call
"""

from __future__ import annotations

import os
import sys
import time
from typing import Any, Dict, List

from dotenv import load_dotenv
from supabase import create_client
from openai import OpenAI

# ──────────────────────── configuration & sanity checks ───────────────────── #

load_dotenv()

# ---------- Supabase config ----------
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY")   # or anon if RLS permits
sb = create_client(SUPABASE_URL, SUPABASE_KEY)

if not SUPABASE_URL or not SUPABASE_KEY:
    sys.exit(
        "❌  SUPABASE_URL / SUPABASE_SERVICE_ROLE_KEY env vars are missing.\n"
        "    export them and rerun."
    )

# Sample questions to probe the index
SAMPLE_QUERIES: List[str] = [
    "What are the symptoms of misophonia?",
    "How prevalent is misophonia in university students?",
    "What is the relationship between misophonia and hyperacusis?",
    "What treatments are effective for misophonia?",
    "How does misophonia affect quality of life?",
]

# Add this after other configuration
openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def embed(text: str) -> List[float]:
    """Return OpenAI ada‑002 embedding (1536‑dim list of floats)."""
    resp = openai_client.embeddings.create(
        model="text-embedding-ada-002",
        input=text,
    )
    return resp.data[0].embedding

# ─────────────────────────── helper functions ─────────────────────────────── #


def perform_vector_search(query_vec, top_k=5, thresh=0.6):
    """
    Call the SQL RPC we just created.
    `query_vec` is a list[float] length 1536 coming from OpenAI.
    """
    try:
        resp = sb.rpc(
            "search_research_chunks",
            {
                "query_embedding": query_vec,
                "match_count": top_k,
                "similarity_threshold": thresh,
            },
        ).execute()
        if getattr(resp, "error", None):
            raise RuntimeError(resp.error)
        return resp.data or []
    except Exception as e:
        print(f"   ⚠  RPC failed: {e}")
        return []


def print_results(rows: List[Dict[str, Any]]) -> None:
    """
    Nicely format the search results.
    """
    if not rows:
        print("   (no matches)\n")
        return

    for idx, row in enumerate(rows, 1):
        meta = row.get("metadata", {}) or {}
        title = meta.get("title", "Unknown title")
        year = meta.get("year", "????")
        author = meta.get("primary_author", "Unknown author")
        sim = row.get("similarity", 0.0)

        snippet = (row.get("text", "") or "").replace("\n", " ")[:280] + "…"

        print(f"\nResult {idx}  •  sim={sim:.3f}")
        print(f"  {title} — {author} ({year})")
        print(f"  {snippet}")


# ────────────────────────────────── main ──────────────────────────────────── #


def main() -> None:
    print("\n🔍  Supabase vector search smoke‑test\n" + "—" * 60)
    for i, q in enumerate(SAMPLE_QUERIES, 1):
        print(f'\nQuery {i + 1}/{len(SAMPLE_QUERIES)}: "{q}"')


        # 1. get the vector
        print("Generating embedding...")
        query_vec = embed(q)  # Python list[float]

        # 2. PostgREST / Postgres expects a *string* like: [0.1,0.2,…]
        vec_literal = "[" + ",".join(f"{x:.6f}" for x in query_vec) + "]"

        # 3. call the RPC
        print("Performing vector search...")
        resp = sb.rpc(
            "search_research_chunks",
            {
                "query_embedding": vec_literal,  # <- NOT the raw text
                "match_count": 5,
                "similarity_threshold": 0.6,
            },
        ).execute()
        
        if getattr(resp, "error", None):
            print(f"   ⚠  RPC failed: {resp.error}\n")
            continue
            
        results = resp.data or []
        print_results(results)

        if i < len(SAMPLE_QUERIES):
            print("\nPausing 2 s before the next query …")
            time.sleep(2)

    print("\n✔  Done")


if __name__ == "__main__":
    main()


================================================================================


################################################################################
# File: scripts/stages/db_upsert.py
################################################################################

# File: scripts/stages/db_upsert.py

#!/usr/bin/env python3
"""
Stage 6 — create / update `research_documents` + `research_chunks` rows.
If `do_embed=True` we immediately hand-off to Stage 7 for embeddings.
"""
from __future__ import annotations
import json, logging, os, pathlib, sys
from datetime import datetime
from typing import Any, Dict, List, Sequence

from dotenv import load_dotenv
from supabase import create_client
from tqdm import tqdm

load_dotenv()
SUPABASE_URL  = os.getenv("SUPABASE_URL")
SUPABASE_KEY  = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
OPENAI_API_KEY= os.getenv("OPENAI_API_KEY")

META_FIELDS = ["doc_type","title","authors","year","journal","doi","abstract",
               "keywords","research_topics","peer_reviewed","open_access",
               "license","open_access_status"]

log = logging.getLogger(__name__)

# ─── Supabase & sanitiser helpers ──────────────────────────────────
def scrub_nuls(obj:Any)->Any:
    if isinstance(obj,str):  return obj.replace("\x00","")
    if isinstance(obj,list): return [scrub_nuls(x) for x in obj]
    if isinstance(obj,dict): return {k:scrub_nuls(v) for k,v in obj.items()}
    return obj

def init_supabase():
    if not (SUPABASE_URL and SUPABASE_KEY):
        sys.exit("⛔  SUPABASE env vars missing")
    return create_client(SUPABASE_URL,SUPABASE_KEY)

def upsert_document(sb,meta:Dict[str,Any])->str:
    meta = scrub_nuls(meta)
    doi = meta.get("doi")
    if doi:
        existing = (
            sb.table("research_documents")
            .select("id")
            .eq("doi", doi)
            .limit(1)
            .execute()
            .data
        )
        if existing:
            doc_id = existing[0]["id"]
            sb.table("research_documents").update(meta).eq("id", doc_id).execute()
            return doc_id
    res = sb.table("research_documents").insert(meta).execute()
    if hasattr(res, "error") and res.error:
        raise RuntimeError(f"Document insert failed: {res.error}")
    return res.data[0]["id"]

def insert_chunks(sb,doc_id:str,chunks:Sequence[Dict[str,Any]],src_json:pathlib.Path):
    ts = datetime.utcnow().isoformat()
    inserted_ids = []
    
    rows = [
        {
            "document_id": doc_id,
            "chunk_index": ch["chunk_index"],
            "token_start": ch["token_start"],
            "token_end": ch["token_end"],
            "page_start": ch["page_start"],
            "page_end": ch["page_end"],
            "text": scrub_nuls(ch["text"]),
            "metadata": scrub_nuls(ch.get("metadata", {})),
            "chunking_strategy": "token_window",
            "source_file": str(src_json),
            "created_at": ts,
        }
        for ch in chunks
    ]
    
    for i in range(0, len(rows), 500):
        batch = rows[i : i + 500]
        try:
            res = sb.table("research_chunks").insert(batch).execute()
            if hasattr(res, "error") and res.error:
                log.error("Supabase insert failed → %s", res.error)
                raise RuntimeError(f"Chunk insert failed: {res.error}")
            # Collect the inserted IDs
            inserted_ids.extend([r["id"] for r in res.data])
        except Exception as e:
            log.error(f"Failed to insert chunk batch {i//500 + 1}: {e}")
            raise
    
    return inserted_ids

def upsert(json_doc:pathlib.Path, chunks:List[Dict[str,Any]]|None,
           *, do_embed:bool=False)->None:
    sb=init_supabase()

    data=json.loads(json_doc.read_text())
    row={k:data.get(k) for k in META_FIELDS}|{"source_pdf":data.get("source_pdf", str(json_doc))}
    
    doc_id=upsert_document(sb, row)

    if not chunks:
        log.info("No chunks to insert for %s", json_doc.stem)
        return

    inserted_ids = insert_chunks(sb, doc_id, chunks, json_doc)
    log.info("↑ %s chunks inserted for %s", len(chunks), json_doc.stem)

    if do_embed and inserted_ids:
        # Import here to avoid circular dependency
        from stages import embed_vectors
        # Pass the IDs of chunks we just inserted
        log.info("Embedding %s chunks...", len(inserted_ids))
        # embed_vectors expects to fetch chunks by ID, so we just trigger it
        embed_vectors.main()  # This will find all chunks with NULL embeddings

if __name__ == "__main__":
    import argparse
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    p=argparse.ArgumentParser()
    p.add_argument("json",type=pathlib.Path)
    p.add_argument("--chunks",type=pathlib.Path)
    p.add_argument("--embed",action="store_true")
    args=p.parse_args()
    
    chunks = None
    if args.chunks and args.chunks.exists():
        chunks = json.loads(args.chunks.read_text())
    
    upsert(args.json, chunks, do_embed=args.embed)


================================================================================


################################################################################
# File: scripts/stages/llm_enrich.py
################################################################################

# File: scripts/stages/llm_enrich.py

#!/usr/bin/env python3
"""
Stage 4 — read an existing JSON, make **one** GPT-4 call that fills every
bibliographic field, and merge it back.
"""
from __future__ import annotations
import json, logging, pathlib, re, os
from textwrap import dedent
from typing import Any, Dict, List

# ─── minimal shared helpers ────────────────────────────────────────
def _authors(val:Any)->List[str]:
    if val is None: return []
    if isinstance(val,list): return [str(a).strip() for a in val if a]
    return re.split(r"\s*,\s*|\s+and\s+", str(val).strip())

META_FIELDS_CORE = [
    "doc_type","title","authors","year","journal","doi","abstract",
    "keywords","research_topics",
]
EXTRA_MD_FIELDS = ["peer_reviewed","open_access","license","open_access_status"]
META_FIELDS     = META_FIELDS_CORE+EXTRA_MD_FIELDS
_DEF_META_TEMPLATE = {**{k:None for k in META_FIELDS_CORE},
                      "doc_type":"scientific paper",
                      "authors":[], "keywords":[], "research_topics":[],
                      "peer_reviewed":None,"open_access":None,
                      "license":None,"open_access_status":None}

def merge_meta(*sources:Dict[str,Any])->Dict[str,Any]:
    merged=_DEF_META_TEMPLATE.copy()
    for src in sources:
        for k in META_FIELDS:
            v=src.get(k)
            if v not in (None,"",[],{}): merged[k]=v
    merged["authors"]=_authors(merged["authors"])
    merged["keywords"]=merged["keywords"] or []
    merged["research_topics"]=merged["research_topics"] or []
    return merged
# ───────────────────────────────────────────────────────────────────

from dotenv import load_dotenv
from openai import OpenAI
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL          = "gpt-4.1-mini-2025-04-14"
log            = logging.getLogger(__name__)

def _first_words(txt:str,n:int=3000)->str: return " ".join(txt.split()[:n])

def _gpt(text:str)->Dict[str,Any]:
    if not OPENAI_API_KEY: return {}
    cli=OpenAI(api_key=OPENAI_API_KEY)
    prompt=dedent(f"""
        Extract all metadata fields ({', '.join(META_FIELDS)}) from the paper
        below and return ONE JSON object.  Text:  {text}
    """)
    rsp=cli.chat.completions.create(model=MODEL,temperature=0,
            messages=[{"role":"system","content":"metadata extractor"},
                      {"role":"user","content":prompt}])
    raw=rsp.choices[0].message.content
    m=re.search(r"{[\s\S]*}",raw)
    return json.loads(m.group(0) if m else "{}")

# ─── public function ───────────────────────────────────────────────────────
def enrich(json_path:pathlib.Path)->None:
    data=json.loads(json_path.read_text())
    # reconstruct the body text from the new structure
    full=" ".join(
        el.get("text","") for sec in data["sections"] for el in sec.get("elements",[])
    )
    new_meta=_gpt(_first_words(full))
    # ✅ merge – keep everything that was already there
    data.update(new_meta)                 # <- adds/overwrites meta fields
    json_path.write_text(json.dumps(data, indent=2, ensure_ascii=False), 'utf-8')
    log.info("✓ metadata enriched → %s", json_path.name)

if __name__ == "__main__":
    import argparse, logging
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    p=argparse.ArgumentParser(); p.add_argument("json",type=pathlib.Path)
    enrich(p.parse_args().json)


================================================================================


################################################################################
# File: verify_pdf_processing.py
################################################################################

# File: verify_pdf_processing.py

#!/usr/bin/env python

import json
import pathlib
import sys
from scripts.old.pipeline_integrated import _make_converter, extract_pdf, concat_tokens, TXT_DIR, JSON_DIR

def verify_processing():
    """Verify that sections now contain the text field and tokens are generated properly."""
    if len(sys.argv) < 2:
        print("Usage: python verify_pdf_processing.py <path/to/sample.pdf>")
        return 1
    
    sample = pathlib.Path(sys.argv[1])
    if not sample.exists() or sample.suffix.lower() != '.pdf':
        print(f"Error: {sample} is not a valid PDF file")
        return 1
    
    print(f"Processing {sample}...")
    conv = _make_converter()
    
    # Extract PDF
    try:
        json_path = extract_pdf(sample, TXT_DIR, JSON_DIR, conv, overwrite=True)
        print(f"Successfully extracted to {json_path}")
    except Exception as e:
        print(f"Extraction error: {e}")
        return 1
    
    # Load JSON and verify text field
    obj = json.loads(json_path.read_text())
    
    # Check sections have text field
    section_count = len(obj["sections"])
    sections_with_text = sum(1 for s in obj["sections"] if "text" in s)
    
    print(f"\nFound {section_count} sections, {sections_with_text} with text field")
    
    # Check token generation
    tokens, page_map = concat_tokens(obj["sections"])
    print(f"Generated {len(tokens)} tokens from sections\n")
    
    # Print first few sections for inspection
    for i, section in enumerate(obj["sections"][:3]):
        print(f"Section {i+1}: {section.get('section', '(unnamed)')}")
        print(f"  Page: {section.get('page_start')}-{section.get('page_end')}")
        text = section.get('text', '')
        print(f"  Text length: {len(text)} chars")
        print(f"  Text preview: {text[:100]}...\n")
    
    if len(tokens) == 0:
        print("ERROR: No tokens generated - fix might not be working!")
        return 1
        
    print("SUCCESS: Text fields are present and tokens are generated!")
    return 0

if __name__ == "__main__":
    sys.exit(verify_processing())


================================================================================


################################################################################
# File: netlify/functions/search.js
################################################################################

import { createClient } from '@supabase/supabase-js';
import { Configuration, OpenAIApi } from 'openai';

export async function handler(event, context) {
  if (event.httpMethod !== 'POST') {
    return { statusCode: 405, body: 'Method Not Allowed' };
  }
  
  try {
    const body = JSON.parse(event.body);
    const { query, filters, expandContext, page, pageSize, similarityThreshold } = body;
    
    // Initialize Supabase client
    const supabase = createClient(
      process.env.SUPABASE_URL,
      process.env.SUPABASE_SERVICE_ROLE_KEY
    );
    
    // Initialize OpenAI for embeddings
    const configuration = new Configuration({
      apiKey: process.env.OPENAI_API_KEY,
    });
    const openai = new OpenAIApi(configuration);
    
    // Generate embedding
    const embeddingResponse = await openai.createEmbedding({
      model: "text-embedding-ada-002",
      input: query,
    });
    
    const queryEmbedding = embeddingResponse.data.data[0].embedding;
    
    // Search using the RPC function
    const { data: results, error } = await supabase.rpc(
      'match_documents',
      {
        query_embedding: queryEmbedding,
        match_threshold: similarityThreshold || 0.6,
        match_count: pageSize || 10
      }
    );
    
    if (error) throw error;
    
    return {
      statusCode: 200,
      body: JSON.stringify({
        result: {
          results,
          page: page || 1,
          pageSize: pageSize || 10,
          query
        }
      })
    };
  } catch (error) {
    console.error('Error in search function:', error);
    return {
      statusCode: 500,
      body: JSON.stringify({ error: error.message })
    };
  }
}


================================================================================


################################################################################
# File: eslint.config.js
################################################################################

import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'

export default [
  { ignores: ['dist'] },
  {
    files: ['**/*.{js,jsx}'],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
      parserOptions: {
        ecmaVersion: 'latest',
        ecmaFeatures: { jsx: true },
        sourceType: 'module',
      },
    },
    plugins: {
      'react-hooks': reactHooks,
      'react-refresh': reactRefresh,
    },
    rules: {
      ...js.configs.recommended.rules,
      ...reactHooks.configs.recommended.rules,
      'no-unused-vars': ['error', { varsIgnorePattern: '^[A-Z_]' }],
      'react-refresh/only-export-components': [
        'warn',
        { allowConstantExport: true },
      ],
    },
  },
]


================================================================================


################################################################################
# File: requirements.txt
################################################################################

################################################################################
################################################################################
# Python dependencies for Misophonia Research RAG System

# Core dependencies
firebase-admin>=6.2.0
openai>=1.0.0
numpy>=1.24.0
python-dotenv>=1.0.0
flask>=2.0.0
requests>=2.28.0

# PDF processing
PyPDF2>=3.0.0
unstructured>=0.10.0
pdfminer.six>=20221105

# Utilities
tqdm>=4.65.0
tabulate>=0.9.0
colorama>=0.4.6
argparse>=1.4.0

# Firebase/Google Cloud
google-cloud-firestore>=2.11.0

# For concurrent processing
concurrent-log-handler>=0.9.20

# Supabase integration
supabase~=2.0.0


================================================================================


################################################################################
# File: scripts/stages/__init__.py
################################################################################

# File: scripts/stages/__init__.py

"""Stage helpers live here so `pipeline_integrated` can still be the single-file
reference implementation while every stage can be invoked on its own."""

"""
Namespace package so the stage modules can be imported with
    from stages import <module>
"""
__all__ = [
    "common",
    "extract_clean",
    "llm_enrich",
    "chunk_text",
    "db_upsert",
    "embed_vectors",
]


================================================================================

